<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-02-13T22:25:47+00:00</updated><id>/</id><title type="html">Joseph Hamilton - Solution Oriented Data Scientist</title><subtitle>A website by and for Joseph Hamilton. A place to explore projects and pursuits, to chase curiosities and catch dreams.
</subtitle><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><entry><title type="html">Project McNulty - Choices</title><link href="/project/mcnulty/2018/02/13/Project-McNulty-MVP/" rel="alternate" type="text/html" title="Project McNulty - Choices" /><published>2018-02-13T00:00:00+00:00</published><updated>2018-02-13T00:00:00+00:00</updated><id>/project/mcnulty/2018/02/13/Project-McNulty-MVP</id><content type="html" xml:base="/project/mcnulty/2018/02/13/Project-McNulty-MVP/">&lt;p&gt;Project McNulty has passed MVP.&lt;/p&gt;

&lt;p&gt;Working on Project McNulty has been interesting.  I’ve suffered a bit due to days not being in person for
lectures.  In general, the project has been more straightforward.  Nonetheless, all of our work seems
to require a lot of time - many very late nights.  There is almost always some aspect of what you’re working
on that is new.  So any time saved because you already know one thing is counter-balanced with something
that you haven’t yet mastered.&lt;/p&gt;

&lt;p&gt;SQL and database stuff isn’t new to me.  But PostgreSQL is.  I’ve known OF Postgres.  But I’ve mainly used
MySQL in the past.  And Postgres isn’t even an option for databases in the SQL track on HackerRank… which
I’ve almost completely finished.  So getting used to weird Postgres quirks is one thing.  Yet another is
learning SQLAlchemy and more important for this particular project Socrata and SODA.  Because of the structure
already inherent in the dataset I was using, I wanted to create and use a function that would use SQLAlchemy
to create the table strcture for Postgres automatically from the Socrata metadata.  What I ended up with
certainly isn’t &lt;em&gt;complete&lt;/em&gt;.  But it would be very straightforward to pick it up and tailor it for any
future project using another Socrata-based dataset.&lt;/p&gt;

&lt;p&gt;Punching the data in was easy once I had the table set up.  After a few tests with very small batches, I
pushed in the entire thing.  It took about 100 seconds for 6.5 million rows.  I still want to go back and
create a function to “top off” my data.  The Chicago Data Portal updates almost daily.  I should be able to
use a SODA query to pull in recent records and upsert to punch it into my local store.&lt;/p&gt;

&lt;p&gt;Next… once I started to &lt;strong&gt;use&lt;/strong&gt; the data, I encountered some common database sorts of problems.  Queries
were slow.  So I punched in a few indices.  That helped.&lt;/p&gt;

&lt;p&gt;After that I have a number of issues related to size.  First, I can exhaust memory.  Any number of things
are holding onto or leaking memory.  Rebooting the VM helps.  But overall, I’ve set the VM at 10G.  I
&lt;em&gt;could&lt;/em&gt; update the laptop to 32G and give the VM 25G.  But that’s expensive.  Alternatively, I can set
up swap files.  The best for speed is the SSD.  But I don’t want to eat into that space and upgrading the
SSD wouldn’t just be costly - it’d be painful and risky.  The HD has plenty of space but it’s slow.  So
as an alternative I ordered a slim USB3 stick.  Using it for swap will likely kill it.  Oh well.  I’ll be
free from “out-of-memory” issues.&lt;/p&gt;

&lt;p&gt;Memory issues are just one part of the size problem.  Time is another.  But today we covered spare matrices
in lecture.  I have several large categoricals.  When I blow those out to dummy variables I get huge things
that take a lot of time.  I &lt;strong&gt;NEED&lt;/strong&gt; to employ sparse matrices here for this project.&lt;/p&gt;

&lt;p&gt;But beyond what efforts I will undertake to setup swap for a large amount of virtual memory and tricks
to speed things up, now that I’ve gotten a couple pretty pictures based on prototypes for the MVP, I
also just need to play more with much smaller datasets.  For the MVP, I just queried for all of the data
for 2017.  Even that was a quarter million records.  For model evaluation, I do need something that captures
a fair representation of the categoricals.  But nothing so big.&lt;/p&gt;

&lt;p&gt;I didn’t bother with interpretability or descriptions of the modeling… at all for the MVP.  I just described
the dataset, tossed in a geo picture I created via Tableau and showed a couple of learning curves.&lt;/p&gt;

&lt;p&gt;It was good enough for MVP.&lt;/p&gt;

&lt;p&gt;But… what was &lt;strong&gt;VERY INTERESTING&lt;/strong&gt; for the Project McNulty MVP was that we did not present our MVPs.  Project
Three is actually a solo project.  But we were grouped into teams.  Our teams are like “project buddies” where
we each go to our team for assistance and help before others or the instructors.  The real fun here, however,
was that someone else on our team presented our MVP on our behalf.  This meant we had to communicate with
each other well enough that we could give the teammate’s presentation.  The way this was explained early on
was that we’d have to understand each other’s code.  But that wasn’t truly necessary.  It &lt;strong&gt;WAS&lt;/strong&gt; if that
team presented via the work in a Juptyer notebook which several did.  A slide presentation of any kind
negated that need.  And for the final presentation we’re back to giving our own talk.&lt;/p&gt;

&lt;p&gt;So… now we &lt;em&gt;could&lt;/em&gt; have simply acted as if we were presenting the talk on our teammate’s behalf… you know…
which we were doing.  But no… the first person that went up started by acting as if they were the other
person.  And it just got more hilarious from there.  Everyone followed that trend of pretending to be the
other person.  Then people started trying to do a good job &lt;strong&gt;imitating&lt;/strong&gt; the other person.&lt;/p&gt;

&lt;p&gt;It got utterly comical at that point.  Now, for some things the instructor’s questions were fielded by the
actual author.  Alex got into it with David because David questioned whether he was really using “Accuracy”
as the metric to compare models.  Alex retorted that he was only using Accuracy because the things he had
ready to go to create charts used Accuracy.  I exclaimed “ditto” because I knew I had the very same issue.
But… when it was my turn, Amy (who was presented on my behalf) and I started speaking together.  So,
I deferred to let “Joseph” respond.  And Amy was hilarious since she just bluffed her way through saying,
in essence, that for the most part all of these metrics trend together and any one of them would do.&lt;/p&gt;

&lt;p&gt;This was silly, but incredibly funny.  On a more serious note, this highlighted something I’d missed not
being here for a couple days.  I scoured the material again to confirm “logloss” wasn’t covered.  It seems
it was something David covered from material on his own blog.  I need to go read that… especially since
it is probably important for my project.  Not only was it not in the material.  It wasn’t in the challenge sets and we haven’t
yet covered it in a Pair Problem.  But after the David-Amy exchange, David all but insinuated it
may well be in an upcoming Pair Problem.&lt;/p&gt;

&lt;p&gt;So… while the others were going, Amy was clearly getting into the spirit of all this and she asked me
to imitate myself so she could work from that.  I told here that was silly.  Imitate myself?  Preposterous.
In any case, she did a fair job imitating me… I guess.  One obvious thing it did was it made her present
loudly.  She’s normally as quiet as a mouse and gets even quieter as she continues to speak.  I imagine I’m
at the other end… just a loud presenter.  Well… it get her more where she needs to be.  I don’t think I
did anywhere near as good a job channeling Michael.  And although Michael may have somewhat started with
an Amy imitation, I don’t think he kept it up long.&lt;/p&gt;

&lt;p&gt;Even more hilarity followed as Adam imitated Derrick.  But the really memorable thing was a comment Adam
made when Derrick got done presenting for Tiffany and Tiffany went up to present for Adam.  Adam whined
that now Tiffany finally knew what it felt like to follow Tiffany.  We all lost it.  Several students
have, in the past, overtly requested from the instructors that they be able to present before Tiffany.&lt;/p&gt;

&lt;p&gt;Why?  Tiffany’s past work experience has drilled into her the use of Keynote and Tableau in a way that
is very fine.  She’s mentioned a previous boss was very finicky on the use of Keynote.  Things had to
look “good”.  It’s not that we cannot do so… given time… lots of time…  It’s that no matter what
we do, it is upstaged by Tiffany who seems not to even be trying.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project McNulty has passed MVP.</summary></entry><entry><title type="html">Snow Strikes</title><link href="/metis/2018/02/10/Snow-Strikes/" rel="alternate" type="text/html" title="Snow Strikes" /><published>2018-02-10T00:00:00+00:00</published><updated>2018-02-10T00:00:00+00:00</updated><id>/metis/2018/02/10/Snow-Strikes</id><content type="html" xml:base="/metis/2018/02/10/Snow-Strikes/">&lt;p&gt;La Nina winters have returned…&lt;/p&gt;

&lt;p&gt;I dunno about you, but with regards to &lt;a href=&quot;https://en.wikipedia.org/wiki/El_Ni%C3%B1o%E2%80%93Southern_Oscillation&quot;&gt;ENSO&lt;/a&gt;,
I kinda prefer the El Niño years.  The cyclones are “over there” in the Pacific and the winters tend to be dry.&lt;/p&gt;

&lt;p&gt;Between flu seasons and snow season, if you’re someone considering the Metis Bootcamp, I might suggest you
avoid the winter season.  Of course, if you &lt;strong&gt;ARE&lt;/strong&gt; such a person, might I suggest you list me as one who recommended
you to Metis?  Chuckle…&lt;/p&gt;

&lt;p&gt;In any case, I was “remote” Wednesday and Thursday this week due to kids down with the flu.  Then &lt;strong&gt;EVERYONE&lt;/strong&gt; was
remote Friday since the Metis Chicago campus was closed due to the Winter Storm..&lt;/p&gt;

&lt;p&gt;Bizarrely, the Pair Programming schedule aligned with this.  We have an odd number in the cohort.  So as “pairs” go,
there is always a leftover.  That individual can work solo or they can join any other pair of their choice.  I dutifully
got in front of my computer Wednesday morning before 9am to identify my pair partner only to find I had none.  Then
I looked at the assignment for the day…&lt;/p&gt;

&lt;p&gt;OK.  I have to pause a bit and opine a bit about Pair Programming.&lt;/p&gt;

&lt;p&gt;We’re past the initial weirdness where we didn’t know each other and where egos may have come into play.  Sure, some
are more amenable to working with others.  But we’re all in this together.  So, my thoughts of the moment don’t have
to do with that aspect.&lt;/p&gt;

&lt;p&gt;No… it’s the nature of the assignments.&lt;/p&gt;

&lt;p&gt;The actual assignments range pretty widely in their difficulty level, their topic, their scope, etc.  A lot of the
time, they have a “trivial” feel to them.  Of course, what’s trivial to one may be novel to another.  But you get a
sense these are standard teaching exercises or refreshers.  And then one comes out of the blue to spank you hard.&lt;/p&gt;

&lt;p&gt;For example, this week…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Monday - SQL exercises (yawn.. refresher)&lt;/li&gt;
  &lt;li&gt;Tuesday - &lt;strong&gt;standard&lt;/strong&gt; simple recursion examples.  So standard, in fact, that Amy took one look at it and was
entirely ready to &lt;em&gt;teach&lt;/em&gt; the lesson of &lt;a href=&quot;https://en.wikipedia.org/wiki/Memoization&quot;&gt;memoization&lt;/a&gt; because this
particular examples was &lt;strong&gt;THE&lt;/strong&gt; “go to” example for that topic.&lt;/li&gt;
  &lt;li&gt;Wednesday - Program Batch Gradient Descent from Scratch&lt;/li&gt;
  &lt;li&gt;Thursday  - Finish Wednesday’s work&lt;/li&gt;
  &lt;li&gt;Friday - Run through an entire classification exercise with charts and logistic regression of 6 features.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Do ya see a difference there?!?  If not just follow along…&lt;/p&gt;

&lt;p&gt;Being “off” Wednesday and Thursday underscored the difference between reading the material and being in a group
setting where you can rattle off questions immediately to the instructors.  Furthermore there is a clear structure between:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reading about certain techniques&lt;/li&gt;
  &lt;li&gt;Implementing projects using said techniques&lt;/li&gt;
  &lt;li&gt;Running through a complete exercise using said techniques in less than 30 minutes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, I had one &lt;strong&gt;HUGE&lt;/strong&gt; advantage over my peers on Wednesday.  They all had to move on once the 30 minutes were up.
I had no such restriction.  They didn’t fire up Zoom or anything for me.  I was just connected via Slack in a loose
fashion with the curriculum and such available for me to read at my pace.  So the only thing that hindered me from
completing the task was my caretaker duties at home.&lt;/p&gt;

&lt;p&gt;And… on that note… if I can program Batch Gradient Descent from scratch while enduring the fingernails-on-chalkboard
sensation of a teenage daughter whimpering and moaning while fighting off a high fever, I imagine I can endure the
pressure of Code Interviews.  Mind you, if you’re a prospective employer, might I suggest you peruse my work on
HackerRank, GitHub, these projects, etc., and spare me said ordeal.  But I’m better prepared nonetheless.&lt;/p&gt;

&lt;p&gt;In any case, working with Gradient Descent was interesting because you had to determine things were failing because
your code was bad or because your choice of alpha was way off.&lt;/p&gt;

&lt;p&gt;I didn’t even realize the plan was to continue Thursday till I again dutifully checked before 9am to see the assignment.
Apparently, even then, only a couple pairs had working examples.  I’d pat myself on the back except that it’d be an
unfair comparison.  Sure I got it working… but not in two 30 minute sessions.&lt;/p&gt;

&lt;p&gt;Friday was much more fun because the entire class was remote.  This made things interesting because of how everyone
chose to work together.  Several of us had issues with one platform or another.  Nate, my pair partner, immediately
suggested we stick to nothing more than Slack.  This meant no voice.  So, actually, we were both just working the
problem independently, simultaneously while peppering each other with questions or comments.  I was surprised how
far I got actually, but there was a strong sense that I was “behind” on the material due to missing lectures.
Once the entire class joined via Zoom for review, it became clear our choice was sound.  Far too many others wasted
too much of their time fighting with getting “connected”.&lt;/p&gt;

&lt;p&gt;It was also hilarious to see everyone in their “natural” setting.  I must say it seemed several of the students hadn’t
really even gotten out of bed yet!  We got introduced to a number of pets.&lt;/p&gt;

&lt;p&gt;After a couple lectures and a bit of slacking teammates on project work, I had to take the “lunch break” to go out and
shovel snow.  I just barely finished in time to come back in for a Metis-wide quiz.  All four campuses were taking
this quiz at roughly the same time.  It was declared to be a closed-book quiz.  Of course, it’s all on our honor,
especially since we (in Chicago) were all remote.  But the entire idea is to assess ourselves, both on an individual
level and on the levels of each campus and the curriculum/teaching overall.  So why bother cheating?!?&lt;/p&gt;

&lt;p&gt;We had an hour allocated to the quiz.  But I burned through it fairly quickly because for most of it you either knew
it or you didn’t.  There wasn’t much middle ground.  Almost none of it struck me as something I’d never heard of.
It wasn’t that kind of “not knowing”.  It was more like, oh yeah… that topic… that I’d not yet had the chance to
review properly.&lt;/p&gt;

&lt;p&gt;There were still a few things where I sort of understood the general idea but hadn’t yet solidified on &lt;em&gt;which&lt;/em&gt; term
meant &lt;em&gt;what&lt;/em&gt; thing.  So I did slow down and methodically work through my reasoning so at the very least my answers
demonstrated a consistent thinking, even if consistently wrong.&lt;/p&gt;

&lt;p&gt;All in all, I’m having a ball, but I’m “at risk” of slipping behind.  It’s tough.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">La Nina winters have returned…</summary></entry><entry><title type="html">Project McNulty - Choices</title><link href="/project/mcnulty/2018/02/08/Project-McNulty-Choices/" rel="alternate" type="text/html" title="Project McNulty - Choices" /><published>2018-02-08T00:00:00+00:00</published><updated>2018-02-08T00:00:00+00:00</updated><id>/project/mcnulty/2018/02/08/Project-McNulty-Choices</id><content type="html" xml:base="/project/mcnulty/2018/02/08/Project-McNulty-Choices/">&lt;p&gt;Project McNulty - what to do…&lt;/p&gt;

&lt;p&gt;I’ve chosen two scopes.  But I do have to select one for the project.&lt;/p&gt;

&lt;p&gt;I have approached this project with a bit more caution.  I don’t want to be “chasing ghosts”.&lt;/p&gt;

&lt;p&gt;Now, of course, part of this overall experience is to develop the skill of storytelling, even
when there really doesn’t seem to be much of a story to tell.  But, it’s a lot more fun
if there is something there.&lt;/p&gt;

&lt;p&gt;I also wanted to be able  to pivot to the second choice rather seemlessly if needed, as needed.
So, in that vein, I performed a bit of Exploratory Data Analysis (EDA) for both my scope choices.&lt;/p&gt;

&lt;p&gt;I have decided to proceed with the Chicago Crime Scene scope.  However, time permitting, I will
continue to work on the other one.  Where it’s “simple”, this ought to be straightforward.  For
example, it shouldn’t be more work to cram that data into Postgresql once I’m able to do that for
the first scope.&lt;/p&gt;

&lt;p&gt;Here are the EDA Notebooks for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/joseph-r-hamilton/Project_McNulty/blob/master/Exploration/Assessment_Crime.ipynb&quot;&gt;The Chicago Crime Scene&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/joseph-r-hamilton/Project_McNulty/blob/master/Exploration/Assessment_Music.ipynb&quot;&gt;A Music Genre Detector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project McNulty - what to do…</summary></entry><entry><title type="html">Project McNulty Begins</title><link href="/project/mcnulty/2018/02/06/Project-McNulty-Begins/" rel="alternate" type="text/html" title="Project McNulty Begins" /><published>2018-02-06T00:00:00+00:00</published><updated>2018-02-06T00:00:00+00:00</updated><id>/project/mcnulty/2018/02/06/Project-McNulty-Begins</id><content type="html" xml:base="/project/mcnulty/2018/02/06/Project-McNulty-Begins/">&lt;p&gt;Project McNulty begins.&lt;/p&gt;

&lt;p&gt;OK.  Once bitten, twice shy.&lt;/p&gt;

&lt;p&gt;They &lt;strong&gt;TOLD&lt;/strong&gt; us last time to submit two ideas for Project Two and to be able to
pivot to the second one if things didn’t work out with the first.  I am not sure
anyone actually did so.  I cannot even remember what my second idea was.  I know
I gave it very little consideration.&lt;/p&gt;

&lt;p&gt;Now, to be fair, it was hard to assess things.  First of all, half of that project
was scraping.  We couldn’t even look at data until we had it.  Furthermore, a lot
of us wouldn’t have really known how.  But we WERE getting a sense of the lack of
correlation at the MVP stage.  I imagine several of us could have, should have
hopped at that point.  Again though… we didn’t know.  For example, I spent
time scraping &lt;em&gt;more&lt;/em&gt; data.  I had fun.  It as a worthwhile experience.  I proved
in MongoDB.  But only towards the end did I look at the learning curves which
made it really, really clear more data wasn’t going to help me at all.&lt;/p&gt;

&lt;p&gt;So… THIS time I’m treating both ideas seriously and have already pulled down
the initial datasets for both.  As much as is feasible I’m going to do a quick,
hasty deep dive to look for “signal”.  Things like:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Are there worthy correlations between features and target?&lt;/li&gt;
  &lt;li&gt;Is the classification going to be skewed due to imbalanced categoricals?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I plan to have things like that addressed very shortly.  I may be able to add
features later.  But I don’t want to get stuck chasing ghosts this time.&lt;/p&gt;

&lt;p&gt;David called this “fail fast”.  We need to be able to determine quickly when
to “fail” and move on rather than falling trap to a sunk cost fallacy.&lt;/p&gt;

&lt;p&gt;Let’s see how this goes…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project McNulty begins.</summary></entry><entry><title type="html">Bootcamp - Investigation One</title><link href="/metis/2018/02/06/Investigation-One/" rel="alternate" type="text/html" title="Bootcamp - Investigation One" /><published>2018-02-06T00:00:00+00:00</published><updated>2018-02-06T00:00:00+00:00</updated><id>/metis/2018/02/06/Investigation-One</id><content type="html" xml:base="/metis/2018/02/06/Investigation-One/">&lt;p&gt;I gave the presentation for the first of two Investigations today!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/slides/docker/&quot; target=&quot;_blank&quot;&gt;Here it is.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The investigations have started.&lt;/p&gt;

&lt;p&gt;No… it’s not the Spanish Inquisition.&lt;/p&gt;

&lt;p&gt;Every student in the cohort has to give two presentations on “Investigations”
at some time during the Bootcamp.  I believe it was the first or second day
when the instructors pointed us to a schedule on Google Docs.  We had to sign
up for two timeslots.&lt;/p&gt;

&lt;p&gt;I dutifully ignored this.&lt;/p&gt;

&lt;p&gt;I mean how could I even think about researching some random topic in the midst
of all the pressure related to the projects!?!  and the Challenges!!&lt;/p&gt;

&lt;p&gt;Somewhere in the past couple of weeks, I caught a whiff or a conversation between
Alice some of the students.  I didn’t quite catch what prompted the discussion.
But it seemed to be something along the lines of someone commenting about all
the things to do and the struggle of the pressure relative to scheduling and
prioritizing.&lt;/p&gt;

&lt;p&gt;Alice provided this order of prioritization:2018-01-24-Project-Luther-Begins.md&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Projects&lt;/li&gt;
  &lt;li&gt;Blog&lt;/li&gt;
  &lt;li&gt;Challenges&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now… that doesn’t appear to mean just jettison stuff… No, we got a brief
(and supposedly one-time) bit of chastisement from David this morning on
submitting things on time as expected.  And none of that could have pertained
to the Project since we all do that stuff (MVPs, standups, Presentations, etc.).
The only exception so far has been Amy who was out sick on the day for Presentations
on Project Two and she gave her presentation belatedly after I presented my
Investigation.&lt;/p&gt;

&lt;p&gt;So… you can run, but you can’t hide.  David pestered me last week so I finally signed
up for Investigations.  I chose one early date and one late date since I plan on
one “easy” and one “hard” topic.  OK.  Truth be told, you wait to be almost the
last one to sign up, you no longer have choice of slots.&lt;/p&gt;

&lt;p&gt;Docker was on the list of suggested topics, so I jumped on it given my love for
things related to virtualization and my experience playing with containers of
various OSs.&lt;/p&gt;

&lt;p&gt;I stayed with reveal.js and like before I tried to explore some new functionality
of reveal.js while doing so.  I used the “cube” transition which, I discovered,
locks you into that for the full presentation - no per-slide overrides.  It seems
to have to with how both sides of the transition have to work with the format.&lt;/p&gt;

&lt;p&gt;But the real fun was incorporating “shellinabox” which permitted me to include
a terminal &lt;strong&gt;within&lt;/strong&gt; the presentation for the purposes of a live demo.  This
worked well.  It was “cool” to have the terminal be a part of the cube transition.
What did not work as well was what would seem to be simpler - just incorporating
websites in iframes in a slide.  I mean… that’s a precursor or dependency of
the functionality for shellinabox!!  But I just could not get it to work.  I 
did manage to use something so I bounced from the presentation to website and
back rather seamlessly… by using some hack for debugging that’s been left in
reveal.js as an undocumented feature.  So I did keep in the spirit of not
exiting the presentation.  But still… sheesh!&lt;/p&gt;

&lt;p&gt;Another “fun” aspect was that earlier in the day Alice walked the group through installing postgresql
in EC2 instances on AWS.  From there we were supposed to be able to pull the
data into local Jupyter notebooks.  We had all sorts of problems because of
some odd postgresql issues.  It was funny to use as a demo an example of firing
up a local Docker with postgresql and have it all work immediately.  Of course, this
was a bit of apples to oranges since the work to get that to work with Juptyer
would be very similar.&lt;/p&gt;

&lt;p&gt;This was the “easy” investigation.  I have high hopes for the next one to be
much more interesting.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">I gave the presentation for the first of two Investigations today!</summary></entry><entry><title type="html">Illness Strikes</title><link href="/metis/2018/02/06/Illness-Strikes/" rel="alternate" type="text/html" title="Illness Strikes" /><published>2018-02-06T00:00:00+00:00</published><updated>2018-02-06T00:00:00+00:00</updated><id>/metis/2018/02/06/Illness-Strikes</id><content type="html" xml:base="/metis/2018/02/06/Illness-Strikes/">&lt;p&gt;My family has been hit by the flu.&lt;/p&gt;

&lt;p&gt;You can prepare all you want for all kinds of mishaps.  But you cannot
prevent them.&lt;/p&gt;

&lt;p&gt;I’ll not be the first in the cohort to be absent due to illness.  Amy won
that prize.  But I will have to take a couple days absence this week.&lt;/p&gt;

&lt;p&gt;Fortunately (so far) it’s not me.  Oh, I’ve been nursing my own bugs.
But my stuff has been annoying sinusitis.  What did the doctor call it?
I think she said it was “sub-acute sinusitis”.  The “sub” part is easy
to diagnose.  If it’s not “sub-acute” you don’t wait several weeks to
go to the doctor.  In any case, I did go to the doctor after the first
week of the Bootcamp.  That may have been incredibly fortuitous because
I got the flu shot while I was there.  I hope that helps protect me.&lt;/p&gt;

&lt;p&gt;My son showed significant symptoms Sunday morning and my daughter
succumbed late yesterday afternoon..&lt;/p&gt;

&lt;p&gt;Oh yeah… I’d better call that attendance line…  Come to think of it,
I wonder if my wife called it yesterday…&lt;/p&gt;

&lt;p&gt;I alerted the Metis instructors that this week might be dicey.  My wife
shifted some responsibilities off to coworkers and took Monday and Tuesday
off.  Alice suggested later this week might be feasible for me to stay
home and work remotely.&lt;/p&gt;

&lt;p&gt;There may be no “good” time to deck out of this intensive bootcamp.
But this may be better than other times.  The third project is just beginning.
I give my first investigation this afternoon.&lt;/p&gt;

&lt;p&gt;I just hope I don’t the flu myself.  That would be… bad… very bad.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">My family has been hit by the flu.</summary></entry><entry><title type="html">Project Luther - Presentation and Wrapup</title><link href="/project/luther/2018/02/02/Project-Luther-Presentation-and-Wrapup/" rel="alternate" type="text/html" title="Project Luther - Presentation and Wrapup" /><published>2018-02-02T00:00:00+00:00</published><updated>2018-02-02T00:00:00+00:00</updated><id>/project/luther/2018/02/02/Project-Luther-Presentation-and-Wrapup</id><content type="html" xml:base="/project/luther/2018/02/02/Project-Luther-Presentation-and-Wrapup/">&lt;p&gt;Project Luther Presentations were today!&lt;/p&gt;

&lt;p&gt;Project Luther is nearing completion.&lt;/p&gt;

&lt;p&gt;Everyone presented this morning.  (well… almost everyone… Amy has been out sick the last couple days).&lt;/p&gt;

&lt;p&gt;I feel into an odd trap with this project.  This was a two-part project.  The first parts was web-scraping.
This part can be time intensive and indeed rather frustrating as you deal with odd issues.  But nonetheless,
it’s fairly straightfoward.  You set out with a rather specifc task and target: get THAT data.  The second
part was far more murky.  Sure, it may be that linear regression could be viewed as systematic.  You do
X, Y and Z, possibly iteratively.  But this would ignore the primary difficulties.&lt;/p&gt;

&lt;p&gt;This part, the linear regression part, is more akin to the “Science” in “Data Science” than “Data”.  You
are essentially running an experiment.  The pattern you seek may simply not exist.  You may end up with
a null result.&lt;/p&gt;

&lt;p&gt;You do need to do a good job ensuring you’ve done your due diligence in attacking the problem.  And you
need to develop whatever insight you can, even if the main goal of a predictive model isn’t fulfilled.&lt;/p&gt;

&lt;p&gt;As I worked on the presentation, I would occasionally need a chart.  That might require rerunning a
function or model.  And a presentation slide may need an explanation of an issue.  To better explain
something, is even more investigation needed?  Can you tweak the model this way or that to learn more
or see anything differently?  In short, you could end up right back in a never-ended cycle chasing
modeling.&lt;/p&gt;

&lt;p&gt;From just the point-of-view of the presentation itself, I chose to continue with reveal.js.  I did indeed
further develop my skills there.  Since it essentially IS a website, this lets me learn more about CSS
as well.  I may eventually add in some custom javascript for my own desired effects.  So using reveal.js
for the presentations is preparing me for the upcoming focus on things like D3.  For example, on several
slides I employed “flex” for placement on the slides.&lt;/p&gt;

&lt;p&gt;I also made use of the “export to PDF” aspect of reveal (rumor was the instructors wanted a PDF).
  It does work, after a fashion.  But it did
something hilarious with the panoramic background.  I’m not entirely sure, but it seems it used the
vertical offset - as if the PDF pages were constantly going “down” rather than “right”.  Tis a minor
thing.  A more serious thing is that I used the heirarchical organization of reveal to slip in
extra, appendix-like, material into the slide presentation.  I didn’t actually use these extra hidden
slides.  Indeed, they were hidden so well I don’t think anyone realized I had the extras.  But they
aren’t hidden when exporting to PDF.  They’re unfolded and put back into the sequence.  Interesting.&lt;/p&gt;

&lt;p&gt;The feedback I was given on the presentation chided me for focusing too much on WHAT I did and not WHY.
That is, I’d lost view of the Storytelling aspect of the presentation and project.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Luther Presentations were today!</summary></entry><entry><title type="html">Project Luther - Analysis</title><link href="/project/luther/2018/02/01/Project-Luther-Analysis/" rel="alternate" type="text/html" title="Project Luther - Analysis" /><published>2018-02-01T00:00:00+00:00</published><updated>2018-02-01T00:00:00+00:00</updated><id>/project/luther/2018/02/01/Project-Luther-Analysis</id><content type="html" xml:base="/project/luther/2018/02/01/Project-Luther-Analysis/">&lt;p&gt;Project Luther provides plenty of opportunity for playing around with data.&lt;/p&gt;

&lt;p&gt;We have covered quite a lot of material related to linear regression techniques
and issues over the last few days.  Some of this felt like decent review,
especially since I have done this both on HackerRank and via the MOOC series
I was pursuing.  Other parts were completely new to me.  I now have a much
better idea how to describe Ridge and Lasso Regression.  And there was a lot
of depth regarding a variety of issues.&lt;/p&gt;

&lt;p&gt;One key takeaway, for example, was to be very wary of using R^2 for any sort
of model comparison.  Maybe use Adjusted R^2 since it is better for comparing
modeling with different sets of features.  But the idea is that R^2 will always
increase slightly with added features even if those features have more
predictive power.  Even better still is just to focus on
root-means-squared-error.&lt;/p&gt;

&lt;p&gt;I have been digging into sklearn.  There’s a lot of power here.  So much so
that it’s easy to jump into using these module, classes and functions with
precious little degree of understanding.&lt;/p&gt;

&lt;p&gt;There’s something to be said to coding stuff up from scratch to develop
understanding.&lt;/p&gt;

&lt;p&gt;I ended up doing that yesterday.  I was trying to repeat some of what we’d
discussed with my current data and modeling.  I wanted to &lt;strong&gt;SEE&lt;/strong&gt; the model’s
behavior.  Is it “High Bias”, “High Variance”?  It seemed easy enough to
extrapolate from our work in train-test splits or cross-validation to create
what I needed.  I still feel so clumsy with Matplotlib.  But I did it.&lt;/p&gt;

&lt;p&gt;And then David (one of the instructors) came by and said “Oh, you recreated
sklearn’s learning curve.  Sheesh.  Truth be known, if I’d know that was there,
I likely would have just used it!!&lt;/p&gt;

&lt;p&gt;Anyway, here’s what I did:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample_test_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;''' Create a set of train/test samples.
        The goal here is random sets of a certain size.
        There is no intention of these sets being non-overlapping.
        Size is same for both test and train unless the requested
        size is more than half of the N provided.
        Input:
            N    the size of the data
            k    the number or samples desired
            size the size of the samples desired
        Output:
            List of pairs of list of indices
    '''&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;learning_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;est&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Create a plot to visualize the learning of the model.
    Inputs:
        est    the estimator
        X      the features
        y      the target
        points the number of tests
        rep    the number of repetitions at each test size
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test_err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_ind&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_test_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;est&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;est&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;est&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;test_err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This gives a “clumsy” picture like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/slides/Luther/Chart_x01.png&quot; alt=&quot;Learning Curve &quot; /&gt;&lt;/p&gt;

&lt;p&gt;So I then ran off and found the quasi-official sample function to use with the
sklearn learning-curve fucntion.  That ends up looking something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/slides/Luther/Chart_02.png&quot; alt=&quot;Learning Curve &quot; /&gt;&lt;/p&gt;

&lt;p&gt;If it wasn’t humbling enough to switch to the “nice” functions and charts, the
really sobering thing here is what both of these charts showed.  After I’d
had all that fun creating a robust scraping regimen to get tens of thousands of
records, these charts underscored rather powerfully that for some instances,
more data doesn’t really help.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Luther provides plenty of opportunity for playing around with data.</summary></entry><entry><title type="html">Metis - Open House</title><link href="/metis/2018/01/31/Metis-OpenHouse/" rel="alternate" type="text/html" title="Metis - Open House" /><published>2018-01-31T00:00:00+00:00</published><updated>2018-01-31T00:00:00+00:00</updated><id>/metis/2018/01/31/Metis-OpenHouse</id><content type="html" xml:base="/metis/2018/01/31/Metis-OpenHouse/">&lt;p&gt;Tonight, I stayed after the Bootcamp to join the Metis Open House.&lt;/p&gt;

&lt;p&gt;It’s a strange experience to participate in another Open House on the other side of
the Bootcamp process.&lt;/p&gt;

&lt;p&gt;And it’s &lt;strong&gt;IMMENSELY&lt;/strong&gt; surreal to hear Alice, one of the Instructors, describe how
much fun it is for the instructors to push us through high-pressure, fast projects.&lt;/p&gt;

&lt;p&gt;Chuckle…&lt;/p&gt;

&lt;p&gt;But… there’s free food.  And it was good food.  Giordano’s pizza!!&lt;/p&gt;

&lt;p&gt;And it seemed good to stay here and focus on the current project.&lt;/p&gt;

&lt;p&gt;But… this second project has ended up a tad strange.  The instructors really guided
us to segment the work on this project.  So, for me, this meant today was Analysis.
Scraping was supposed to be done.  I actually had set my spiders such that they could
just run in the background.  So while I did analyisis today, the spiders increased my
record count from 50k to 60k.  Tomorrow the focus is on the Presentation.&lt;/p&gt;

&lt;p&gt;For some projects, the analysis could go on forever.  But there’s not much you can
do with bad data.  If there is no signal to capture, you get the pleasure of doing
nothing other than proving that point.  So, to a degree today’s been a bit of winding
down.  I’m focusing on methodology and at least creating good Jupyter Notebooks that
can be useful later.&lt;/p&gt;

&lt;p&gt;About half the cohort stayed for the Open House.  Tiffany even brought her husband along.&lt;/p&gt;

&lt;p&gt;We were informed there would be a couple past alumni presenting their projects.  Then
Nathan told us today that was a miscommunication.  Finally, it seems he found someone
to present, at least here in Chicago.  This was actually the major draw for me.  I
don’t really need to hear the pitch all over again.  But getting more ideas for the
Passon Project will certainly be helpful.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Tonight, I stayed after the Bootcamp to join the Metis Open House.</summary></entry><entry><title type="html">Project Luther Continues</title><link href="/project/luther/2018/01/30/Project-Luther-Continues/" rel="alternate" type="text/html" title="Project Luther Continues" /><published>2018-01-30T00:00:00+00:00</published><updated>2018-01-30T00:00:00+00:00</updated><id>/project/luther/2018/01/30/Project-Luther-Continues</id><content type="html" xml:base="/project/luther/2018/01/30/Project-Luther-Continues/">&lt;p&gt;Project Luther continues.&lt;/p&gt;

&lt;p&gt;We present on Friday.  However, the Instructors have issued the stricture that we must have our analysis complete
by end-of-day Wednesday.  Now… I may take a bit of issue with what “end-of-day” means.  But they’ve specified
Thursday is to be solely for development of presentations.&lt;/p&gt;

&lt;p&gt;For me, this means that Tuesday was/is for &lt;strong&gt;more&lt;/strong&gt; data scraping.  And Wednesday is primarily going to be devoted
to analysis.  Or to put it another way, I cannot do a good job on the presentation if I’m still fighting with the
analysis.  And I cannot do justice to the analysis, if I’m still scrambling over scraping.&lt;/p&gt;

&lt;p&gt;There’s simply not enough time to do all of what we want on these projects.  I had hoped to incorporate some
basic Unit Testing and CI support.  That’s out.  But I &lt;strong&gt;did&lt;/strong&gt; implement the pipeline to MongoDB as a backend.
My fears may have been unwarranted.  But I was leery of issues of concurrent scrapes dumping to a single
text file.  It was likely not really an issue.  I had though the concurrency setting was set to “1”.  But that
was what I had seen as I reviewed the project from last year.  In any case, I’m happy to have incorporated
MongoDB.&lt;/p&gt;

&lt;p&gt;I did implement a scraping approach that could end up pulling all the property tax data from Will County…
if it’s left to run long enough.  It’s still a bit slow for some odd reason.  Maybe, I can tweak it a bit…
who knows.&lt;/p&gt;

&lt;p&gt;But I’ve already shifted to analysis.  The pair-plots seemed to show me that there may yet be some pattern or
signal to capture but it’s being swamped by outliers.  So I started doing some box-plots and histograms to get
a sense for things… And oh boy!  I have outliers.&lt;/p&gt;

&lt;p&gt;I still have work to do to incorporate a good number of my features.  And I may be able to create some
more features based on aggregated data (assuming I can get reasonably close to a complete scrape).  But even
with the initial set of records, it’s clear there are things I could do.  I may end up jettisoning some records
to get rid of outliers.   Or I may switch those features to logarithms.  I’ll have to experiment.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="MongoDB" /><summary type="html">Project Luther continues.</summary></entry></feed>