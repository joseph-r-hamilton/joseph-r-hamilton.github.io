<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-07-13T19:46:21+00:00</updated><id>/</id><title type="html">Joseph Hamilton - Solution Oriented Data Scientist</title><subtitle>A website by and for Joseph Hamilton. A place to explore projects and pursuits, to chase curiosities and catch dreams.
</subtitle><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><entry><title type="html">D3 Mini-Project: Convex Hull</title><link href="/training/2018/07/13/D3-Mini-Project-01/" rel="alternate" type="text/html" title="D3 Mini-Project: Convex Hull" /><published>2018-07-13T00:00:00+00:00</published><updated>2018-07-13T00:00:00+00:00</updated><id>/training/2018/07/13/D3-Mini-Project-01</id><content type="html" xml:base="/training/2018/07/13/D3-Mini-Project-01/">&lt;p&gt;Take a look at a recent mini-project I have been playing with.&lt;/p&gt;

&lt;p&gt;A number of things recently converged to prompt me to create a mini project to sharpen some skills.&lt;/p&gt;

&lt;p&gt;I used D3 during the Data Science bootcamp.  I was the first to take advantage of it.  But during a
recent technical interview, someone asked me how to do a particular thing in Javascript.  My answer
was truthful, yet likely unsatisfactory.  I simply stated I wouldn’t have done that part in Javascript,
but in Python and then feed that into Javascript.  That could be as straightforward as creating an
appropriate Flask-based REST API.  Later upon researching this idea, I’m actually more confident on
this point.  It’s not just that I only know to do it that way.  It may be important to do it that way
for security purposes.&lt;/p&gt;

&lt;p&gt;But that got me thinking that I really do need to become a polyglot and know how to do a variety of
things in different ways using different languages.  Around the same time, I came aross an SQL challenge
on the CodeSignal (formerly CodeFight) site.  The challenge expected you to use a hard-coded specific
function to calculate the area of a &lt;a href=&quot;https://www.geeksforgeeks.org/convex-hull-set-2-graham-scan/&quot;&gt;convex hull&lt;/a&gt;.
During the bootcamp David suggested to us that one way to learn something is to code it by yourself
from scratch.  For some reason I had just recently reviewed the idea or concept that SQL was elevated
to become an imperative language despite starting out strictly as a declarative language.  Furthermore,
not only did SQL have standard functions for this, so too does the Python stack (in scipy).  I wanted
to see how best to do this using numpy and pandas without scipy.&lt;/p&gt;

&lt;p&gt;After getting Convex Hull working in procedural SQL and python, I decided to create a visualization to
demonstrate or show the methodology of the Graham algortihm to determine the Convex Hull given a series
of points.&lt;/p&gt;

&lt;p&gt;This is somewhat of a work-in-progress.  I want to circle back here and polish things up a bit and then
show the way area is calculated after the convex hull has been determined.  But here it is…&lt;/p&gt;

&lt;p&gt;NOTE: I’ve not yet included buttons or a mechanism to restart or pause the demo.  If you didn’t catch
it just refresh the page while you have the chart on screen.&lt;/p&gt;

&lt;iframe class=&quot;fragment fade-in&quot; style=&quot;float:center&quot; width=&quot;800&quot; height=&quot;800&quot; src=&quot;/assets/html/mini_001/c_002.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;TODO: include Python and SQL code&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Javascript" /><summary type="html">Take a look at a recent mini-project I have been playing with.</summary></entry><entry><title type="html">University of Illinois</title><link href="/uofi/2018/07/10/University-of-Illinois/" rel="alternate" type="text/html" title="University of Illinois" /><published>2018-07-10T00:00:00+00:00</published><updated>2018-07-10T00:00:00+00:00</updated><id>/uofi/2018/07/10/University-of-Illinois</id><content type="html" xml:base="/uofi/2018/07/10/University-of-Illinois/">&lt;p&gt;I’ve been accepted into the &lt;a href=&quot;https://cs.illinois.edu/academics/graduate/professional-mcs-program/mcs-data-science-track&quot;&gt;UofI MSDS program&lt;/a&gt;!!&lt;/p&gt;

&lt;p&gt;And… alas… I cannot do it.&lt;/p&gt;

&lt;p&gt;This Masters in Data Science at University of Illinois Urbana-Champaign was without a shadow of a
doubt the best thing I could have done to take advantage of the TAA support.&lt;/p&gt;

&lt;p&gt;However, due to some oddities of time constraints of the TAA program, I had some hard deadlines.
In order to get a TAA program approved you have to be accepted into a program and registered
for classes.  You have to do this &lt;em&gt;before&lt;/em&gt; getting approval from the government.&lt;/p&gt;

&lt;p&gt;The staff at UofI were helpful to a point.  They were relatively quick to direct me to the proper
application process.  However, when I explained to them that I needed to be accepted, enrolled and
registered in approximately two weeks they sort of dropped the issue suggesting this would be
unfair to other applicants.&lt;/p&gt;

&lt;p&gt;I can sort of understand this.  However, there is one aspect of this leads to believe it need not
have been this way…  This particular masters program is &lt;em&gt;entirely&lt;/em&gt; online.  Why in the world
would it have mattered to slip in one more remote student?&lt;/p&gt;

&lt;p&gt;I contacted my career adviser at IDES to discuss options.  I had to fall back to IIT because I
couldn’t get UofI to move quickly enough.  The staff at IIT know how to move quickly to help
people in my situation.  It did not seem likely that we would be able to drop that and seek
another approval.  Furthermore, once you are in a TAA approved program, you may not pay for
course on your own.  I asked if we could get a waiver for this.  No luck.&lt;/p&gt;

&lt;p&gt;So I had to inform UofI that the best I could do at this point is seek a deferral.  I might be
able to purse this starting approximately a year from now.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Training" /><summary type="html">I’ve been accepted into the UofI MSDS program!!</summary></entry><entry><title type="html">IIT First Class</title><link href="/iit/2018/07/06/IIT-First-Class/" rel="alternate" type="text/html" title="IIT First Class" /><published>2018-07-06T00:00:00+00:00</published><updated>2018-07-06T00:00:00+00:00</updated><id>/iit/2018/07/06/IIT-First-Class</id><content type="html" xml:base="/iit/2018/07/06/IIT-First-Class/">&lt;p&gt;My first course at IIT has begun.&lt;/p&gt;

&lt;p&gt;But it is not a class on Recommender Systems.  Oh well…&lt;/p&gt;

&lt;p&gt;I had heard from some peers about the hustle one has to go through when you’ve already got
an IDES/Worknet approved training plan and then the provider cancels the class.  So I wasn’t
all that surprised when I had to endure the same fate.&lt;/p&gt;

&lt;p&gt;Luckily for me the replacement course had exactly the same title and timeframe.  How can
that be?  Well, IIT has an “advanced topics” sort of course in which they focus on a particular
topic in this space: &lt;strong&gt;IIT ITMD 525 - Topics in Data Science and Management&lt;/strong&gt;.  Each semester
there may be several such courses.  In the system, they all have this number and title.
IIT cancelled the Recommender Systems course at the last minute supposedly due to low
enrollment.  But when I bounced around for a replacement, I simply chose the course on
MongoDB.&lt;/p&gt;

&lt;p&gt;In one sense, I was a bit dismayed.  I felt this was somewhat redundant.  I’ve used MongoDB
a number of times in the last few years.  However, I’ve learned recently that there is a
difference between the awareness you get from cursorily using some tool or platform and
the depth of knowledge you get from a deep dive.  I &lt;em&gt;knew&lt;/em&gt; SQL from years of use.  But I
now know SQL much better after having the training in the Metis Science Bootcamp followed
by plowing through the coursework on HackerRank, CodeSignal, DataCamp, etc.  I &lt;em&gt;knew&lt;/em&gt; Python.
But I now know Python much better.  It’s the same sort of thing here with MongoDB.  There
are all sorts of aspects of MongoDB you never need to consider for small projects yet which
are critical for companies using MongoDB.&lt;/p&gt;

&lt;p&gt;Furthermore, after this course, I will prepare for and obtain the
&lt;a href=&quot;https://university.mongodb.com/certification&quot;&gt;MongoDB certification&lt;/a&gt;.
Not sure if I could have done anything similar for Recommender Systems.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Training" /><summary type="html">My first course at IIT has begun.</summary></entry><entry><title type="html">IIT Beginnings</title><link href="/iit/2018/05/25/IIT-Beginnings/" rel="alternate" type="text/html" title="IIT Beginnings" /><published>2018-05-25T00:00:00+00:00</published><updated>2018-05-25T00:00:00+00:00</updated><id>/iit/2018/05/25/IIT-Beginnings</id><content type="html" xml:base="/iit/2018/05/25/IIT-Beginnings/">&lt;p&gt;I am a student once again!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Joseph_Hamilton_Juji_Profile.png&quot; alt=&quot;Juji Profile &quot; /&gt;&lt;/p&gt;

&lt;p&gt;I met an IIT grad last night at a meetup.  I told her I was an IIT student.  She thought I meant an alum.&lt;/p&gt;

&lt;p&gt;Nope.  I’m a bona fide IIT &lt;em&gt;student&lt;/em&gt;.  My first course on Recommender Systems starts in early July.&lt;/p&gt;

&lt;p&gt;It is rather clear to me that I will be in mode of some rather serious self-training for quite a while.
Indeed, in the Data Science arena, constant learning is pretty much required.&lt;/p&gt;

&lt;p&gt;My TRA/TAA program is in an odd state of limbo.  Things are so backwards working with the government.
I was supposed to have multiple options for comparison.  But for each option I needed to have been
accepted, registered and have the complete curriculum mapped out such that the institution could send
the government a letter testifying to such.  Only IIT was nimble enough to make this work for the
time constraints I faced.&lt;/p&gt;

&lt;p&gt;Call out to &lt;a href=&quot;https://appliedtech.iit.edu/people/nuala-power&quot;&gt;Nuala Power&lt;/a&gt;.  This woman knows how
to make things happen quickly.&lt;/p&gt;

&lt;p&gt;Depaul was just too slow to get back to me.  Their program would have been better to help me towards
the Cloudera certs.  But ehh… I’ll do that on my own in time.&lt;/p&gt;

&lt;p&gt;And University of Illinois… chuckle… sheesh those wheels are turning slowly.  But they are turning.
They’ve sent me an email that they received my application… weeks after they received it.  I think
this means it bumped from one stage in the process to another.  If I do get accepted, I’ll have to
determine what to do at that point.  I’ll probably beg for a deferral.  I &lt;strong&gt;will not&lt;/strong&gt; be able to
do it while under the oversight of TRA.  The government is very clear on this point.  If they’re
involved and pay for something they approved, thou shalt not pay for anything else, anywhere else
on your own.&lt;/p&gt;

&lt;p&gt;But, the wrinkle is what happens when I get a job.  I’m getting mixed signals.  But it seems that
I &lt;em&gt;can&lt;/em&gt; drop everything at my whim.  Or I can complete any courses underway.  However, support for
completing the previously approved program may end.  Furthermore, because my first IIT course has
an odd schedule, we’re going to apply for another waiver and then apply for final approval.  This
is what I mean about things being in limbo.  I have not yet actually been approved.&lt;/p&gt;

&lt;p&gt;So… in theory… if I get a job in the very near future this IIT program may simply evaporate.&lt;/p&gt;

&lt;p&gt;Oh well.&lt;/p&gt;

&lt;p&gt;None of that matters so much to IIT.  They’re comfortable with the odd aspects of these governmental
things.  So I’m fully registered with my own new IIT email and such.&lt;/p&gt;

&lt;p&gt;The course instructor sent me a request to help him get to know me.  Silly me.  I thought this meant
send him an email.  No.  He provided a link to a Juji chatbot session.  This was quite a bizarre
experience.  I essentially had an interview with an AI.  At the end of it, &lt;a href=&quot;https://juji.ai/post-chat/5b080a1e-38a3-43c2-b196-8bcb2e5c3305/share&quot;&gt;Juji provided me an
assessment of my strengths&lt;/a&gt;. 
This would be cute on its own.  But it actually dovetails with some of the more rigorous assessments
I’ve done recently.  So much so that it was creepy if it pulled all that from a short interview.&lt;/p&gt;

&lt;p&gt;For example, here’s a snippet from a Birkman Assessment.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Your Usual Style is the proactive, positive, and adaptable behavior you have learned to
use to achieve successful outcomes. These strength behaviors help you work
productively and are often seen as your strengths by others. Strength behaviors are
comfortable and easy for you to use.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Your Usual Style is:&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Friendly, decisive and energetic&lt;/li&gt;
    &lt;li&gt;Socially sensitive&lt;/li&gt;
    &lt;li&gt;Fairly methodical in your approach&lt;/li&gt;
    &lt;li&gt;Fairly assertive without being domineering&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;In describing yourself to others indicate that:&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;You are responsive to a variety of demands&lt;/li&gt;
    &lt;li&gt;You encourage healthy competition&lt;/li&gt;
    &lt;li&gt;You pay attention to detail and follow through&lt;/li&gt;
    &lt;li&gt;You spend enough time on decisions that mistakes are not made due to hasty action&lt;/li&gt;
  &lt;/ul&gt;

&lt;/blockquote&gt;

&lt;p&gt;Hmm…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Training" /><summary type="html">I am a student once again!</summary></entry><entry><title type="html">DataCamp - a Beginning</title><link href="/datacamp/2018/05/11/DataCamp/" rel="alternate" type="text/html" title="DataCamp - a Beginning" /><published>2018-05-11T00:00:00+00:00</published><updated>2018-05-11T00:00:00+00:00</updated><id>/datacamp/2018/05/11/DataCamp</id><content type="html" xml:base="/datacamp/2018/05/11/DataCamp/">&lt;p&gt;I’ve started to pursue lessons via DataCamp.&lt;/p&gt;

&lt;p&gt;Training in all sorts of things related to Data Science and Data Engineering is going to keep me relatively
busy for quite a while.&lt;/p&gt;

&lt;p&gt;I was introduced to HackerRank by Metis.  Metis grads recommended it as a mechanism to prepare for the
Metis Data Science Bootcamp.  I can now echo that.  And Metis used it for some things.&lt;/p&gt;

&lt;p&gt;But now that I’m past the bootcamp, I’m exploring lots of mechanisms and options for self-training.
One thing that caught my eye was DataCamp.  It seems to be rather defined.  It’s not necesarily more
comprehensive than HackerRank.  But it seems more structured in a couple of ways.  First, now that I have
the experience of the bootcamp, I’m much better able to know what I want to pursue, learn or practice.  In
a sense, I know how to “read” things.  As such, I can see the structure of Datacamp’s material.  If I want
to practice algorithm X, I know exactly where to go for it.  HackerRank is more open ended where they give
problems and you can use anything to solve it.  Next, DataCamp has organized their coursework in career
tracks.&lt;/p&gt;

&lt;p&gt;Honestly, I’m not sure I’ll prioritize a single career track.  I plan on completing &lt;em&gt;everything&lt;/em&gt; on their site.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Python" /><category term="R" /><category term="SQL" /><summary type="html">I’ve started to pursue lessons via DataCamp.</summary></entry><entry><title type="html">Project Kojak Closing</title><link href="/project/kojak/2018/04/04/Project-Kojak-Closing/" rel="alternate" type="text/html" title="Project Kojak Closing" /><published>2018-04-04T00:00:00+00:00</published><updated>2018-04-04T00:00:00+00:00</updated><id>/project/kojak/2018/04/04/Project-Kojak-Closing</id><content type="html" xml:base="/project/kojak/2018/04/04/Project-Kojak-Closing/">&lt;p&gt;It’s the night before Career Day, and all through the house…&lt;/p&gt;

&lt;p&gt;Wow.  What a ride!&lt;/p&gt;

&lt;p&gt;I have not been blogging much these past couple of weeks.  I may end up back-creating a post or two
here to describe the overall project.&lt;/p&gt;

&lt;p&gt;At the moment, I’ll just try to describe what happened over the past three weeks.&lt;/p&gt;

&lt;p&gt;I chose to extend Project Four in a loose sense.  The purpose and nature wasn’t really anything like
the previous project.  The main similarity was the source of the data.&lt;/p&gt;

&lt;p&gt;The goal - find trolls on Reddit.&lt;/p&gt;

&lt;p&gt;I wanted to focus on a few aspects of “Big Data” for this project.&lt;/p&gt;

&lt;p&gt;I wrestled with the instructors a bit over this topic because of a core problem that seems to be
intrinsic for a lot of Data Science or analytic type projects.  One must be careful whether one is
chasing a Supervised or Unsupervised problem.  This may seem rather easy.  Either you have labeled
data or you don’t.  But the temptation to believe you can create labels can be rather strong.
No matter how much I looked for some, there wasn’t really any way to get a list of known trolls.
I sort of broke through and won blessing for the topic when I was able to cast it as a study of
Anomaly Detection and possibly clustering to learn more about trolls.&lt;/p&gt;

&lt;p&gt;So, in a sense the focus shifted a bit away from Trolls and more towards Anomaly Detection.&lt;/p&gt;

&lt;p&gt;The first goal was just to prepare the data.  Back in 2015 someone used the Reddit API to create
a historical archive of &lt;strong&gt;all&lt;/strong&gt; the past Reddit comments.  This data bounced around to several
forums or locations.  Kaggle hosted one month’s worth of data for a challenge.  Someone put this
up in AWS in a S3 bucket.  And, I already knew from Project Four, a file hosting service was
keeping these archives going with about a one-month lag from current.  So, I wanted to create
a second S3 bucket to upload the past couple of years.&lt;/p&gt;

&lt;p&gt;This wasn’t much more trouble than just firing up EC2 instances and some scripting to download,
uncompress and save.  I had to do this in a way that didn’t depend on my connection.  It took a
while.  Of course, I &lt;strong&gt;could&lt;/strong&gt; have punched in many instances and done this in parallel.  But
I didn’t feel I needed to rush through this part because I needed to do some research in parallel.&lt;/p&gt;

&lt;p&gt;I grabbed a number of papers and such to learn what folk have done with similar projects on
Reddit data or anomaly detection algorithms, etc.  And I started to learn Spark and Dask.  I had
lots of places to start:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;David’s Spark and Zepellin Lecture&lt;/li&gt;
  &lt;li&gt;Mike’s Isolation Forest Investigation&lt;/li&gt;
  &lt;li&gt;Tiffany’s Zepellin, GraphX and Graphframes Investigation&lt;/li&gt;
  &lt;li&gt;Alex’s Dask Investigation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SOME of these weren’t quite ready when I needed them.  I started reading up on Dask before Alex
gave his Investigation presentation.  But I was able to grab his Jupyter notebooks after his
presentation to repeate them personnaly.  Same for GraphX with Tiffany.&lt;/p&gt;

&lt;p&gt;David asked me at one point if I’d settled on Dask or Spark.  I hadn’t yet.  But I think it was
right about that time that something dawned on me that should have been pretty clear from the
beginning.  The Reddit archives are comment data only.  But I’m interested not in finding
Troll &lt;strong&gt;comments&lt;/strong&gt;.  I wanted to find Troll &lt;strong&gt;users&lt;/strong&gt;.  Some of the papers I’d reviewed had
maintained a focus on the per-comment data.  This wasn’t my goal at all.  I think I’d imagined
that I would be using Feature Engineering in order to add a few extra features to the existing
features in the data set.  There was a small eureka moment when it became very clear to me that
I wasn’t adding features - I was creating them… all of them.  That is, I had per-comment data but for my
modeling I needed per-user data.&lt;/p&gt;

&lt;p&gt;It then seemed important not to choose between Spark and Dask but to use both of them for their
respective advantages.  One paper I’d reviewed described using graph-based features.  It seemed I
could do this with GraphX/Graphframes.  So, that’d push me towards Spark.  But Dask’s rather native
tie-in with Python would facilitate use of SKLearn.  At that point, I chose to use both.
All I needed to do next was choose how to store data in a way that would work well for both.
I settled in on Parquet.&lt;/p&gt;

&lt;p&gt;Time was the ever-present pressing problem.  There was never really enough of it.  In one sense, I
was well ahead of schedule having my data pulled into the S3 bucket before the informal deadline of
“Data Collected”.  But… this was somewhat of an illusion since my modeling would only be based on
the result of the feature engineering.  And I had several things I had to learn simultaneously:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Spark&lt;/li&gt;
  &lt;li&gt;Zeppelin&lt;/li&gt;
  &lt;li&gt;EMR clusters&lt;/li&gt;
  &lt;li&gt;GraphX/Graphframes&lt;/li&gt;
  &lt;li&gt;Dask&lt;/li&gt;
  &lt;li&gt;Dask clusters (which eventually meant Kubernetes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The other interesting aspect of this project was that we formed small accountability groups.
Within these we orderd in such a way that everyone was responsible for another.  In my group,
I was responsbile for Tiffany who was responsible for Chris who was responsible for me.&lt;/p&gt;

&lt;p&gt;So, I had to pay attention to Tiffany’s status as she progressed through her project.  When I heard
she was having issues due to getting kicked out of her SSH connection to her EC2 instance, I arranged
a quick tutorial session where I showed her how to take advantage of redirects, background jobs and
nohup to ensure her data scraping jobs persisted regardless of the connection.  This was essentially
what I had done with my EC2 instances for extracting and storing data in the S3 bucket.&lt;/p&gt;

&lt;p&gt;Then later on when I was working through the Zeppelin notebook she created for her GraphX investigation,
I ran into the same roadblock she’d found with regards to getting Graphframes.  There’s something a tad
messed up at the moment with regards to installing it for use with Zeppelin.  I found a way to get it
workign and quickly shared it with her.  She promised to help me with Tableau later on… but alas…
there’s really not that much to visualize with Anomaly Detection.&lt;/p&gt;

&lt;p&gt;There came a point where we had to end the modeling and start working on a presentation.  Tiffany, Chris
and I met to discuss strategies.  And then later David and Alice met with our team.  As they’d told us
they would, they didn’t ask each of us to describe our approach for our project.  Instead we had to describe
the project and presentation strategy for the person we were responsible for.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">It’s the night before Career Day, and all through the house…</summary></entry><entry><title type="html">Status Update</title><link href="/metis/2018/03/25/Status-Update/" rel="alternate" type="text/html" title="Status Update" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/metis/2018/03/25/Status-Update</id><content type="html" xml:base="/metis/2018/03/25/Status-Update/">&lt;p&gt;Whew!  The first month post-bootcamp was busy!&lt;/p&gt;

&lt;p&gt;Woah.  I got away from blogging once things got really busy towards the end of bootcamp.&lt;/p&gt;

&lt;p&gt;I have some unfinished draft posts and I may go in and backcreate some posts I’ve been meaning
to write.  So at some point in the future, it may not seem so.  But this has been a six week hiatus.&lt;/p&gt;

&lt;p&gt;Because I may go into more depth with some of those posts on what happened in those last couple of weeks
of the bootcamp, I’m only going to summarize that here.  In essence, things got busy.  The last project
was very challenging in several ways.  But, for the most part, I actually adhered to the idea of segmenting
things per the guidance of the instructors… moreso than any of the other projects.  For some queer reason,
however, I had the idea it’d be &lt;em&gt;easier&lt;/em&gt; once I halted work on the project and focused solely on the
presentation itself.  I needed a bit of free time because I had to start shifting my focus to some
post-bootcamp stuff.  But this wasn’t the case at all.  In any case, we made it.&lt;/p&gt;

&lt;p&gt;All of us had the same need to recover from the bootcamp… while aggressively working our contacts from
the Career Day, etc.  But I had one additional area to pursue - lining up more training.  And this proved
to be nerve-wracking.&lt;/p&gt;

&lt;p&gt;What’s up here?&lt;/p&gt;

&lt;p&gt;The nature of the Reduction-in-Force that affected me makes me eligible for some specific governmental
benefits.  There are actually a couple levels here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.illinoisworknet.com/WIOA&quot;&gt;WIOA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.doleta.gov/tradeact/&quot;&gt;TAA&lt;/a&gt;/&lt;a href=&quot;https://www.doleta.gov/tradeact/2014_amend_att1.cfm&quot;&gt;TRA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A hasty, rough summary of these and how they apply to many like me would be to say that the former
is to help people in industries which have been designated in decline.  But the latter is for those
whose jobs had been moved to another country.&lt;/p&gt;

&lt;p&gt;Pursuing the first is easy because pretty much all you need to do is document your previous employer.
It’s pretty clear whether that is or is not covered.  But how to you prove your job went overseas?
The trick here is… &lt;em&gt;you&lt;/em&gt; don’t… your &lt;em&gt;former company&lt;/em&gt; has to.  This is just one part of the
bureaucratic difficulties I’ve been enduring.  I was very certain my job was shipped out.  Why?
Because I attempted to chase it.  I had already assisted with some of the training of the group in
Mexico that was taking over the work.  It took a bit of legwork but I found the hiring manager who
was going to oversee the work as they group that team.  They responded promptly when I sent my resume.
It was a brief conversation:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You’re perfect for this role.  You’ve been doing what we’ll be doing.
But the job is in Mexico City.  No telecommuting.  No relocation support.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And… though I didn’t ask, I bet salary would be adjust to Mexico City norms.  So… yeah.  I knew
definitively where my job went.  But… it took between three to four months for the paperwork to progress
from my former company to the appropriate governmental agencies to me.  This was a problem because with
TAA/TRA there are some specific deadlines/timeframes involved.  Specifically for my situation, I was
supposed to be actively in training before six months after I first filed for unemployment.&lt;/p&gt;

&lt;p&gt;I was able to foresee this bit of nonsense because of the people at the placement agency.  Indeed,
I never would have learned about any of these benefits were it not for these people.  But they helped me
to see the timeframes I should expect.  This is why I jumped into the bootcamp when I did.&lt;/p&gt;

&lt;p&gt;Nonetheless, I found myself with the challenge of determining &lt;strong&gt;what&lt;/strong&gt; training to pursue.  There is a bit
of an oddity in that you have to have been accepted and enrolled with the &lt;em&gt;complete&lt;/em&gt; training plan documented
before the government will approve and issue the grant to pay for it.  This puts a bit of a burden on the
training agency.  Some know how to roll with this.  Some do not.  I’d already been informed the most likely
options for Data Science or Big Data were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Illinois Institute of Technology&lt;/li&gt;
  &lt;li&gt;Depaul University&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I had to agree.  But, as I compiled supporting documentation for the government, I also researched bona
fide Masters of Science in Data Science programs.  Most of these are well outside the approved price range.
However, I found one that seemed &lt;strong&gt;GREAT&lt;/strong&gt; - University of Illinois.  Their program is entirely online via
Coursera.  Sadly, though, they proved to be far too slow.  I could not get their support to work fast enough
to meet governmental deadlines.&lt;/p&gt;

&lt;p&gt;Depaul has great programs.  But… the problem is that they’ve packaged up individual programs.  It didn’t
seem likely that I could do several of them.  And… sadly… though they do have some experience with these 
governmental programs, they also couldn’t work fast enough.&lt;/p&gt;

&lt;p&gt;IIT?  As everyone had told me, the woman overseeing the professional stuff knows &lt;em&gt;very well&lt;/em&gt; how to work
with this stuff.  The trouble I had was that their coursework seemed too low level.  This was balanced
by how flexible they are.  I am now essentially pursuing a custom plan.  For example, the first course
I’ll take there is on Recommender Systems.  Sure, that was covered in the bootcamp.  But this will be a
chance to go into greater depth and get some practice.&lt;/p&gt;

&lt;p&gt;I am now, once again, a college student.  I year from now, I’ll have a professional certificate from IIT.
And who knows… though I’ve not yet heard back from Illinois, they may accept me too.  I’d have to string
them along, likely starting later.  In any case, it’s clear enough to me that I’ll be doing lots of training
continually in this field, even if I land a job in the near term.&lt;/p&gt;

&lt;p&gt;Now that this stuff is for the most part resolved, I can shift my focus back to other things… like this blog.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Blog" /><summary type="html">Whew! The first month post-bootcamp was busy!</summary></entry><entry><title type="html">Dask and Spark</title><link href="/project/kojak/2018/03/25/Dask-on-AWS/" rel="alternate" type="text/html" title="Dask and Spark" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/project/kojak/2018/03/25/Dask-on-AWS</id><content type="html" xml:base="/project/kojak/2018/03/25/Dask-on-AWS/">&lt;p&gt;Spark and Dask are somewhat overlapping in what they do.  But they are also somewhat complementary.&lt;/p&gt;

&lt;p&gt;To be terribly honest, for a lot of production level things it seems the “Big” of Big Data really has to be
pretty signficant or upon analysis just building a large SQL database will serve just fine.&lt;/p&gt;

&lt;p&gt;For my current project, there are things I want to take advantage of at both ends of a pipeline.  For the
initial feature engineering I may benefit from using Graphframes on Spark.  There may be algorithms
avaialble in Dask (eg. SpectralClustering) that aren’t yet available in Spark.&lt;/p&gt;

&lt;p&gt;Dask is relatively new.  And AWS keeps providing alternatives to doing things.  So this post may
get outdated realtively quickly.  But, at the moment, this is what an individual would experience:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dask is far easier to install than Spark on a single, local machine.&lt;/li&gt;
  &lt;li&gt;Spark is much more straightforward to get working in a cluster on AWS.&lt;/li&gt;
  &lt;li&gt;Dask is vastly easier to learn for anyone used to Python and Pandas.&lt;/li&gt;
  &lt;li&gt;Spark is great for anyone already adept at SQL.&lt;/li&gt;
  &lt;li&gt;Dask is likely more capable of providing someone &lt;em&gt;immediate&lt;/em&gt; relief if they’re running into
memory issues trying to pull all their data into a large Pandas dataframe.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, again, this is &lt;strong&gt;at the moment&lt;/strong&gt;.  If I had to roll my own Hadoop setup to lay Spark atop of, I
very likely would be singing a different tune.&lt;/p&gt;

&lt;p&gt;So what’s the issue with Dask?&lt;/p&gt;

&lt;p&gt;First, let’s try to separate out things that pertain to any effort of taking advantage of a cluster.
Not every problem can be parallelized.  And many things that can require a fair amount of care to
change the general approach.  You cannot always just take your algorithms and functions and throw
them to a parallel virtual machine, cluster or whatnot.  This becomes evident even on a single
machine.  There are several sklearn models that incorpriate an “njobs” parameter to manage
parallel processing across CPUs/cores.  For the most part with Dask, if you can use this on a
single machine, you can immediately use it on a cluster.  You may still end up with issues where
the data isn’t where you need it to be.&lt;/p&gt;

&lt;p&gt;No, that’s not really different.  You have to manage or at least be aware of that issue on Dask,
Spark or whatever.&lt;/p&gt;

&lt;p&gt;What’s different at the moment is that for a Dask cluster, you have to set up the cluster.  For
Spark with EMR this is qausi-magically managed for you.&lt;/p&gt;

&lt;p&gt;And the more important thing.  Dask is bleeding edge.  The various software solutions to manage
this cluster creation have come and gone rapidly in the relatively recent past.&lt;/p&gt;

&lt;p&gt;But I have a feeling that things have somewhat settled on the suggested solution.  So, I’ll describe
that here.  It may help to lay out one guiding principle to guide your thinking.  If you try to
look at this from the perspective of Dask or your python code using Dask as being “in control” and
setting up the cluster as needed, you need to flip your thinking.  Get comfortable with the idea
that you need a system to manage a cluster to stick Dask (along with anything else) into.&lt;/p&gt;

&lt;p&gt;So.. what is that system?&lt;/p&gt;

&lt;p&gt;At the moment it’s Kubernetes for the cluster and Helm for the “sticking Dask in “ function.&lt;/p&gt;

&lt;p&gt;This keeps things modular.  It really doesn’t matter whether your Kubernetes cluster is on Google,
Amazon, local network or whatever.  Once you have it, you let Helm do the driving.&lt;/p&gt;

&lt;p&gt;For AWS, this is how it breaks down:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark is on EMR.&lt;/li&gt;
  &lt;li&gt;If you need to boostrap stuff to build in required functionality…
    &lt;ul&gt;
      &lt;li&gt;You’ll be working with EMR’s way of managing this&lt;/li&gt;
      &lt;li&gt;The changes you’ll be making are on the AWS instances themselves.&lt;/li&gt;
      &lt;li&gt;You will be able to just SSH into the instances and make changes directly (eg. to test)&lt;/li&gt;
      &lt;li&gt;You may only need to change the master node for a variety of things.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dask will be on EC2&lt;/li&gt;
  &lt;li&gt;Should you need to control things to get desired functionality…
    &lt;ul&gt;
      &lt;li&gt;These changes will need to be prepared inside Docker containers&lt;/li&gt;
      &lt;li&gt;You shouldn’t need to make changes in the instances themselves&lt;/li&gt;
      &lt;li&gt;It is &lt;strong&gt;possible&lt;/strong&gt; to get into the individual Docker containers.  But it’s far less easy to do.&lt;/li&gt;
      &lt;li&gt;It’s much more likely you will need to change the Docker container all workers use.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Dask" /><category term="Spark" /><category term="Kubernetes" /><category term="AWS" /><category term="EMR" /><category term="EC2" /><summary type="html">Spark and Dask are somewhat overlapping in what they do. But they are also somewhat complementary.</summary></entry><entry><title type="html">Bootcamp - Blogging about Projects</title><link href="/metis/2018/03/25/Blogging-Projects/" rel="alternate" type="text/html" title="Bootcamp - Blogging about Projects" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/metis/2018/03/25/Blogging-Projects</id><content type="html" xml:base="/metis/2018/03/25/Blogging-Projects/">&lt;p&gt;It’s a challenge to keep up with the blogging!&lt;/p&gt;

&lt;p&gt;We had an interesting and rather lively feedback session Friday.  We’d been talking about
doing one for a bit.  So David finally hosted it.&lt;/p&gt;

&lt;p&gt;One of the more interesting things to come out of it was a consistent call that Dask
should be introduced into the curriculum immediately following Pandas.  This may have been
about the only thing where the cohort at least had no dissenting views.  Most of the feedback
was diverse in the sense that not everyone agreed.&lt;/p&gt;

&lt;p&gt;But Dask?  Not everyone had delved into it.  But many had dabbled a bit.  And Alex did give
a good Investigation presentation on the topic.  Many of us had run into issues throughout
the bootcamp which this could have addressed.  This main problem is what to do when your
local machine runs out of memory?  For most of this there have only been two choices:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Reboot after the crash and try again with a smaller data set.&lt;/li&gt;
  &lt;li&gt;Move it to the cloud somehow.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But moving to the cloud isn’t trivial.  Do you try to create a single EC2 instance with
a ton of RAM?  Or try out something on EMR with the work spread out?  Both of these weren’t
something we could have done early on in the bootcamp.  Sometimes you can restructure your
work so you process a large file one line at a time.  But that’s what Dask does for you!!&lt;/p&gt;

&lt;p&gt;I rolled some swap space.  I’ve now got lots more virtual memory.  So my machine doesn’t
lock up or crash any longer.  But things get slow once I’m chewing through swap.&lt;/p&gt;

&lt;p&gt;Dask, however, would have a been a natural and straightforward solution many of us could
have incorporated rather easily even back in Project One.  And Dask would have provided us
an immediate benefit on a local machine well before taking advantage of its full power on
a cluster.&lt;/p&gt;

&lt;p&gt;One of the more lively topics of discussion related to the Challenges.  I dutifully completed
all the required challenges.  I am actually looking forward to doing some of the optional ones
later… after the conclusion of the bootcamp.  But it seemed pretty clear nobody did the
optional ones.  Indeed, many didn’t actually complete the required ones.  They just turned
them incomplete.  Oh well…&lt;/p&gt;

&lt;p&gt;Another thing that became optional halfway through the bootcamp was blogging about the projects.
This one seems a bit odd.  I mean… that’s pretty much the entire point of the bootcamp - to
develop a portfolio of projects which you display via your blog and/or GitHub.  In this case,
the issue is timing.  You can go back and work on your blogs later.  This seems to be what
a lot of folk do.&lt;/p&gt;

&lt;p&gt;Well… I’ve started to blog to keep a few notes along the way for Project Five.  But I’d not
yet even really started the blogging for Projects Three and Four… tsk… tsk!  So in between
some work on Project Five this afternoon I stopped and fluffed out the structure for the remaining
projects.  Soon I’ll put up the presentations for Project Four.  There’s something I want to go
back and finish for the final presentation of Project Three.  So I may wait on that.  We’ll see…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Blog" /><category term="Dask" /><summary type="html">It’s a challenge to keep up with the blogging!</summary></entry><entry><title type="html">Trouble with Clusters</title><link href="/project/kojak/2018/03/22/Trouble-with-Clusters/" rel="alternate" type="text/html" title="Trouble with Clusters" /><published>2018-03-22T00:00:00+00:00</published><updated>2018-03-22T00:00:00+00:00</updated><id>/project/kojak/2018/03/22/Trouble-with-Clusters</id><content type="html" xml:base="/project/kojak/2018/03/22/Trouble-with-Clusters/">&lt;p&gt;I’m starting to take advantage of clusters on the Cloud for parallel processing power.&lt;/p&gt;

&lt;p&gt;The problem isn’t that there’s not a way to do this.  The problem is that there are
so many ways of doing this.  This is a very dynamic field.&lt;/p&gt;

&lt;p&gt;At a very high level, there’s Amazon and Google.  I’ve tabled Google for the moment.  But I’ll get
back over there eventually.&lt;/p&gt;

&lt;p&gt;But within Amazon, there are many options.&lt;/p&gt;

&lt;p&gt;For general Hadoop, there’s EMR.  But even there we have many options.  I explored the use of
MRjob for a previous project.  I never made it very far.  This was nice in the way it automatically
set up the cluster and permitted a very structured way to slip in Python code at the various stages
of the overall Map-Combine-Reduce flow.  I used it for some small experiments with TF-IDF.  But
I didn’t have to time then to iron out all the issues with setting up instances and containers
appropriately for needs for that project.&lt;/p&gt;

&lt;p&gt;But it seems now that Spark is the way to do Python with a Hadoop structure.  MRJob does actually now work
with Spark.  So I may return to explore that.&lt;/p&gt;

&lt;p&gt;Then we have Zeppelin notebooks which work rather nicely with Spark, either directly in Scala or
via pyspark.  So now we have several layers of how to do things in notebooks.  There’s Jupyter
Notebooks, Jupyter Notebooks in Jupyter Labs and Zeppelin notebooks.&lt;/p&gt;

&lt;p&gt;I tripped over myself yesterday firing up an EMR cluster to play with Zeppelin some more.  This
was a reminder than anything manual will involve problems sooner or later.  So, yesterday I backed
off and slammed in a local install of Spark with Zeppelin in a Docker container.  In all honesty,
I should probably be doing initial prototyping and experimentation locally anyway.  So it was
rather important to get it setup.  But… let’s now try to wrap things up so I can get an EMR
cluster going with Zeppelin/Spark support with as little manual support needed.&lt;/p&gt;

&lt;p&gt;So… my goals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Do everything related to setting up the cluster via the AWS CLI.&lt;/li&gt;
  &lt;li&gt;End up with the command to set up the tunnel and the URL for Zeppelin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the AWS CLI, let’s create a bucket and upload a bootstrap shell script.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws s3 mb s3://aws-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AWSID&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-boots&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;where {AWSID} is the AWS account ID.  S3 bucket names need to be unique.  This ought to be unique.&lt;/p&gt;

&lt;p&gt;Now, let’s create our bootstrap script.  I’ll call it &lt;code class=&quot;highlighter-rouge&quot;&gt;zep_boots.sh&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/anaconda.sh

bash ~/anaconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/anaconda

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\nexport PATH=$HOME/anaconda/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc

conda install &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; seaborn pandas requests

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Upload it to the bucket…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ aws s3 cp zep_boots.sh s3://aws-${AWSID}-boots/

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I had been cloning an existing cluster.  From the AWS website, I took advantage of the “AWS CLI Export”
function to get a CLI sequence to create the cluster.  I saved it to a file and then added to the end
of this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	--bootstrap-actions Path=s3://elasticmapreduce/bootstrap-actions/run-if,Args=[&quot;instance.isMaster=true&quot;,&quot;s3://aws-${AWSID}-boots/zep_boots.sh&quot;]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Because…  that’s what &lt;a href=&quot;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html&quot;&gt;AWS says it should be&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After &lt;strong&gt;many&lt;/strong&gt; attempts and troubleshooting, here’s what you really have to do:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	--bootstrap-actions Path=s3://aws-${AWSID}-boots/zep_boots.sh

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then rather than using the broken run-if script, just incorporate the logic into your code.  I found a good example
to copy on &lt;a href=&quot;https://forums.aws.amazon.com/thread.jspa?threadID=222418&quot;&gt;the forums&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a new version of &lt;code class=&quot;highlighter-rouge&quot;&gt;zep_boots.sh&lt;/code&gt; which includes setup for Graphframes.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#! /bin/bash&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Determine if we are running on the master node.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 0 - running on master&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 1 - running on a task or core node&lt;/span&gt;
check_if_master&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    python - &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;__SCRIPT__&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'
import sys
import json
 
instance_file = &quot;/mnt/var/lib/info/instance.json&quot;
is_master = False
try:
    with open(instance_file) as f:
        props = json.load(f)
 
    is_master = props.get('isMaster', False)
except IOError as ex:
    pass # file will not exist when testing on a non-emr machine
 
if is_master:
    sys.exit(0)
else:
    sys.exit(1)
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;__SCRIPT__
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

check_if_master &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;0

&lt;span class=&quot;c&quot;&gt;# Install Conda and desired modules&lt;/span&gt;

wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/anaconda.sh

bash ~/anaconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/anaconda

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\nexport PATH=$HOME/anaconda/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc

conda install &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; seaborn pandas requests

&lt;span class=&quot;c&quot;&gt;# Install Graphframes&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (gonna need sudo calls here...)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Note... this is sooo kludgey&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mkdir &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /var/lib/zeppelin/local-repo/2ANGGHHMQ
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /var/lib/zeppelin/local-repo/2ANGGHHMQ

&lt;span class=&quot;c&quot;&gt;# Need some dependencies&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar


&lt;span class=&quot;c&quot;&gt;# Now get graphframes&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar

&lt;span class=&quot;c&quot;&gt;# Finally, prepare for pyspark access of graphframes&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mkdir &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /usr/lib/zeppelin/interpreter/lib/python
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/lib/zeppelin/interpreter/lib/python
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;jar xf /var/lib/zeppelin/local-repo/2ANGGHHMQ/graphframes-0.5.0-spark2.1-s_2.11.jar graphframes



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This still requires going into the Interpreters in Zeppelin and changing &lt;code class=&quot;highlighter-rouge&quot;&gt;zeppelin.pyspark.python&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;/home/hadoop/anaconda/bin/python&lt;/code&gt;.  The goofy thing here is the bootstrap script is run &lt;strong&gt;before&lt;/strong&gt; the applications are loaded.  We could get away with creating directories and putting files in (as root… which is good because it prevents the “normal” things removing these files).  But the thing to do to switch to anaconda’s python would require changing a file (interpreter.json).  Hard to do if it’s not there.  And putting it in there may cause the application installation to fail.&lt;/p&gt;

&lt;p&gt;The way AWS EMR has set this up for Zeppelin seems to be… problematic and has caused a number of folk some grief when it comes to loading up additional modules and packages.&lt;/p&gt;

&lt;p&gt;But what about getting the URL for Zeppelin?  Didn’t I want to be able to ignore the AWS website entirely?&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bash create_cluster.sh
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;ClusterId&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;j-38FI39ISFYGTV&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws emr describe-cluster &lt;span class=&quot;nt&quot;&gt;--cluster-id&lt;/span&gt; j-38FI39ISFYGTV | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;State
&lt;span class=&quot;c&quot;&gt;# Wait a bit...  keep checking...&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws emr describe-cluster &lt;span class=&quot;nt&quot;&gt;--cluster-id&lt;/span&gt; j-38FI39ISFYGTV | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Dns
        &lt;span class=&quot;s2&quot;&gt;&quot;MasterPublicDnsName&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;ec2-18-218-45-180.us-east-2.compute.amazonaws.com&quot;&lt;/span&gt;,
&lt;span class=&quot;c&quot;&gt;# Now, to set up the tunnel...&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-ND&lt;/span&gt; 8157 hadoop@ec2-18-218-45-180.us-east-2.compute.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And browse to:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;http://ec2-18-218-45-180.us-east-2.compute.amazonaws.com:8890/&lt;/code&gt;&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Kubernetes" /><category term="AWS" /><category term="EMR" /><category term="Hadoop" /><summary type="html">I’m starting to take advantage of clusters on the Cloud for parallel processing power.</summary></entry></feed>