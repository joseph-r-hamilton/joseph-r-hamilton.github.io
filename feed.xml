<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-01-26T02:57:18+00:00</updated><id>/</id><title type="html">Joseph Hamilton - Solution Oriented Data Scientist</title><subtitle>A website by and for Joseph Hamilton. A place to explore projects and pursuits, to chase curiosities and catch dreams.
</subtitle><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><entry><title type="html">Project Luther Begins</title><link href="/project/luther/2018/01/24/Project-Luther-Begins/" rel="alternate" type="text/html" title="Project Luther Begins" /><published>2018-01-24T00:00:00+00:00</published><updated>2018-01-24T00:00:00+00:00</updated><id>/project/luther/2018/01/24/Project-Luther-Begins</id><content type="html" xml:base="/project/luther/2018/01/24/Project-Luther-Begins/">&lt;p&gt;Project Luther begins.&lt;/p&gt;

&lt;p&gt;Yay!  With this project I get to play with web scraping.&lt;/p&gt;

&lt;p&gt;There are so many aspects of Data Science which I have employed at various points throughout my
career and personal life.  During the lectures this morning we discussed what Machine Learning is
at its most basic definition.  In this definition a lot of the optimization things I did long ago
would qualify as Machine Learning.  Indeed, some of the stuff I’ve done with fitness functions
have quite a lot of resemblance in spirit to Gradient Descent.&lt;/p&gt;

&lt;p&gt;But this project is not just web scraping.  It’s web-scraping plus linear regression.&lt;/p&gt;

&lt;p&gt;It actually proved a tad difficult to choose a topic to investigate.  See… from here on out,
we are only given a few requirements and rough guidelines.  From these we define the project.&lt;/p&gt;

&lt;p&gt;So here, we need something that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Involves some sort of structured data we scrape from the web.  We can use additional data of
any kind from any source.  But the first part of the data needs to be scraped.&lt;/li&gt;
  &lt;li&gt;Has enough data of this structured fashion that we can do some linear regression.&lt;/li&gt;
  &lt;li&gt;Has to be interesting (at least to me).&lt;/li&gt;
  &lt;li&gt;Ought to be somewhat novel.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hmm… tough call.&lt;/p&gt;

&lt;p&gt;I played around last night with a number of ideas.  I settled on something to do with property
tax assessments since I’ve been curious whether there are measurable effects of increasing property
tax.  I don’t think I’ll be able to get enough data longitudinally (in time) to see much.  But I
ought to be able to measure across space or across valuations.&lt;/p&gt;

&lt;p&gt;An even tougher call is HOW to do this…&lt;/p&gt;

&lt;p&gt;I don’t mean how to scrape.  Though I will have to decide for or against inclusion of Scrapy.
I mean Jupyter vs. Python, how much to employ Test-Driven Development, CI, etc.&lt;/p&gt;

&lt;p&gt;MVP is Monday.  Again… the pressure is on.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Luther begins.</summary></entry><entry><title type="html">I broke the blog!</title><link href="/blog/2018/01/24/Blog-Is-Broken/" rel="alternate" type="text/html" title="I broke the blog!" /><published>2018-01-24T00:00:00+00:00</published><updated>2018-01-24T00:00:00+00:00</updated><id>/blog/2018/01/24/Blog-Is-Broken</id><content type="html" xml:base="/blog/2018/01/24/Blog-Is-Broken/">&lt;p&gt;I recently broke the blog!!&lt;/p&gt;

&lt;p&gt;I was indeed able to add in the functionality so I could include slide presentations on the blog!!&lt;/p&gt;

&lt;p&gt;YAY!&lt;/p&gt;

&lt;p&gt;However… as with so much related to the Metis Bootcamp, I was under pressure because of a time crunch.&lt;/p&gt;

&lt;p&gt;There is a lot that I could do with this functionality, but for the purpose of using this within
the project post for Project Benson, I chose to “simply” convert the presentation our tream created
from Powerpoint to Reveal.  I got it!  But… I hadn’t yet pushed any example slide presentation
all the way to the public blog.  I didn’t want this because I didn’t want the public blog with the
Swirply project pointing to a dummy slide.&lt;/p&gt;

&lt;p&gt;But…&lt;/p&gt;

&lt;p&gt;This led to the scenario where I finished the presentation conversion and got done testing things
locally to my heart’s content and then attempted the final push around the time the whole thing
was due.  Actually I was about ten minutes beyond the deadline.  It worked locally, so it should
be fine, right?&lt;/p&gt;

&lt;p&gt;Well…&lt;/p&gt;

&lt;p&gt;I set up a pipeline for the blog.  The push to the public blog goes through a “test” stage.
This is very helpful because not only does it ensure I don’t creating broken links on my own,
it will alert me to cases where external links fail.  This could be because I mistyped it. 
Or, as I’ve learned, it could be because the external site is down that day.  I imagine eventually
I’ll run into cases where the external site has reconfigured or deleted that page.  This testing
lets me catch all that and detemine how to clean it up if I wish or just ignore the problems.&lt;/p&gt;

&lt;p&gt;This testing failed due to an non-trivial number of issues related to things related to the
inclusion of reveal.js.  There was no simple way to ignore all of that in the same way I
can ignore a single URL.&lt;/p&gt;

&lt;p&gt;So… I just had to disable that test phase.&lt;/p&gt;

&lt;p&gt;To get one functionality, I had to break another.&lt;/p&gt;

&lt;p&gt;Sigh…&lt;/p&gt;

&lt;p&gt;EDIT: Blog is fixed.&lt;/p&gt;

&lt;p&gt;I cleaned it up the next day.&lt;/p&gt;

&lt;p&gt;The errors fell into about three categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The testing was checking things in a file within reveal.js that was probably only included there as a sample to use for other things.  The solution here was to mask that directory/module.&lt;/li&gt;
  &lt;li&gt;A sample I’d copied was using data-src in the img tags rather than src.  Testing caught that even though it “worked”.&lt;/li&gt;
  &lt;li&gt;A sample I’d copied had a call to favicon in a different way… fixed it.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="blog" /><category term="Metis" /><summary type="html">I recently broke the blog!!</summary></entry><entry><title type="html">Bootcamp - Week Two</title><link href="/metis/2018/01/22/Bootcamp-Week-Two/" rel="alternate" type="text/html" title="Bootcamp - Week Two" /><published>2018-01-22T00:00:00+00:00</published><updated>2018-01-22T00:00:00+00:00</updated><id>/metis/2018/01/22/Bootcamp-Week-Two</id><content type="html" xml:base="/metis/2018/01/22/Bootcamp-Week-Two/">&lt;p&gt;We’ve segued into Week Two…&lt;/p&gt;

&lt;p&gt;Week Two started with the presentations of Project Benson, the first project.&lt;/p&gt;

&lt;p&gt;These started at 9:30am.  Pair Programming was cancelled.  Alice said we’ll never do
Pair Programming on mornings where we present.&lt;/p&gt;

&lt;p&gt;Well… this was good since we were making last minute changes to the presentation, quite literally.
Dean had changed the them of the presentation which meant the charts and things we’d put into the last
format didn’t quite fit.  And he’d left out one of my charts which I urged him to stuff back in.&lt;/p&gt;

&lt;p&gt;Our team went first.  We magically somehow pulled off a reasonably good presentation despite the
chaos and lack of practice.  And… the new theme got good reviews…&lt;/p&gt;

&lt;p&gt;Our team and the following team both were cut for going too long.  But both teams had made it to
their final slide before the forced ending.  The last team actually ended about half-a-minute early.&lt;/p&gt;

&lt;p&gt;Every team had a slightly different thing that “stuck out”.  Ours was the only team to employ
resampling and weighted averages.  The second to present (Group 1 - Adam, Alex, Chris, Amy) had
created a heat-map (based on tech company locations) with an overlay of the high-traffic stations.
The final team to present (Group 3 - Tiffany, Michael, Nate) was the only group to employ Tableau.
Tiffany apparently has some Tableau experience in her background.  They created a dashboard to
permit the Customer to filter based on desired days and hours.&lt;/p&gt;

&lt;p&gt;Dean had informed me we could get a trail-period license for Tableau.  But Tiffany says there’s another
option which she used for the Bootcamp - the public option.  Apparently you can install Tableau free
but your data must reside on their site.  What an odd requirement.  So this means you should not use
that option for any private or business data.  But it may work great for classes, bootcamps, personal
projects based on public data, etc.  So this is a MUST to try out.&lt;/p&gt;

&lt;p&gt;I’ve got many things to try out.  I’ve pestered Amy to help me later (sigh… FINALLY) switch to vim over vi.
These kids these days must view us old-timers using vi the same way I viewed folk using ed back in the day.&lt;/p&gt;

&lt;p&gt;After the presentation, we’re essentially free for a couple days which is in such stark contrast to
the pressure of Week One.  But we need to do wrapup activities related to the first project.  This
means put the project output on our blog… which many need to create.  And finish and turnin the
challenge (essentially homework).&lt;/p&gt;

&lt;p&gt;In absence of lecture, many of us have escaped the classroom into the large space to plug in extra
monitors.  I’ve finally found the right connectors to be able to hookup two monitors to the laptop
for a total of three…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">We’ve segued into Week Two…</summary></entry><entry><title type="html">Project Benson MVP</title><link href="/project/benson/2018/01/19/Project-Benson-Closing/" rel="alternate" type="text/html" title="Project Benson MVP" /><published>2018-01-19T00:00:00+00:00</published><updated>2018-01-19T00:00:00+00:00</updated><id>/project/benson/2018/01/19/Project-Benson-Closing</id><content type="html" xml:base="/project/benson/2018/01/19/Project-Benson-Closing/">&lt;p&gt;Project Benson is wrapping up.&lt;/p&gt;

&lt;p&gt;We are supposed to be ready to present Monday morning.  We were supposed to have finished up our presentations.
We were supposed to have had a practice presentation.&lt;/p&gt;

&lt;p&gt;We aren’t quite there.  Oh we’re gonna have to present no matter what.  And there was a scheduled social event
to cap out Week-One which really did break the stress and focus, so to speak.&lt;/p&gt;

&lt;p&gt;But I’m not sure any of our groups are truly at a point where they’re not going to be working on this over the
weekend.  And there is a reason they laid out this goal (of having it wrapped up and being free over the weekend)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;they gave us homework… which… ironically would
have been useful &lt;strong&gt;before&lt;/strong&gt; attacking this project.  I’m sure it’ll help reinforce things.  Indeed, I’m
rather certain that’s part of the entire pedagogy here.  We’re getting this training before, during and after
while under immense pressure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If all of the above sounds like doom and gloom, you’re reading it wrong.  I’m thrilled with this.  And I sense
a lot of the others are enjoying this too.  It’s just so fast paced.  And when you force yourself to cut to
the next aspect (for example, from data wrangling to visualization), you have to start an uphill climb all
over again.  You may have gained some confidence by gaining some fluidity in the previous aspect only to feel
like an infant all over again.&lt;/p&gt;

&lt;p&gt;So… enough meta data.  What did we &lt;strong&gt;do&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;Last night I fought with the available geographic data to get a location data for each “Station” in the
MTA data.  The NY data provides geo-data but it is not cleanly aligned with the MTA data.  A bit of patient,
manual sorting and matching got me about 90% there.  Google and Wikipedia got me to about 99%.  For the rest
I just had to make some quasi-intelligent decisions.&lt;/p&gt;

&lt;p&gt;I fought through installation of several libraries and played with plots with maps.  I eventually got a
basic heatmap of all the MTA stations.  It didn’t dawn on me until I could &lt;strong&gt;SEE&lt;/strong&gt; it how odd it is to
be incorporating the non-Manhattan burroughs and simply ignoring Jersey entirely.  The problem statement
didn’t specify Manhattan as the location of the gala, although it seems intuitive.  But the problem statement
clearly lays out the scenario as one where the Customer/Client already had the focus on the MTA.  Too late
now to do anything about it.&lt;/p&gt;

&lt;p&gt;But… this is probably a good example of the issue of putting Design before Data.  Jump right into the
MTA data and you’re locked and biased in that view.  Maybe if you just thought whether it made any sense
to analyze the burroughs the concept of Jersey could have arisen.&lt;/p&gt;

&lt;p&gt;We may have ruled it out.   Indeed, we may have chosen to ignore the burroughs if we could.  But what strikes
me is how we utterly ignored New Jersey entirely.&lt;/p&gt;

&lt;p&gt;Otherwise, the challenge for our team today was to shift from Data Analysis to Visualization.  We simply
did not have the time to properly synchronize or synthesize our various methodologies.  All of our were
doing slightly different things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Me: a Time Series resampling, interpolation, cleaning&lt;/li&gt;
  &lt;li&gt;Derrick: a “min-max” per day approach&lt;/li&gt;
  &lt;li&gt;Kendall: a weighted average rather than “dumb” interpolation&lt;/li&gt;
  &lt;li&gt;Dean: working on code that would take clean data from one of the rest and do further analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the most part, as much as we could test it, all of our approaches were showing the same top sites
in a ranking by traffic.  Derrick had a challenge because it was harder to remove the anomalies with
his method and harder still to limit the impact (those anomalies cost me a 4 hour block and I switched
to interpolating the gap; it cost him a day).  Kendall got her more complicated method mostly working
but before we could really contrast and compare we had to move on.&lt;/p&gt;

&lt;p&gt;Since I was ready with clean data a bit earlier, I’d moved from working with the single per-week dumps
from the MTA and using the yearly dumps from NY.  I grepped (yes… literally using grep) the months
of March, April and May from each of 2015, 2016 and 2017, processed these and one-by-one fed the
data to Dean.  Dean and I then struggled with it.  We fought through issues of Pandas so we could 
do analysis and start making charts.  I found almost no variation in the ranking of the first ten
sites when looking at all three months vs. one week in the 2017 data.&lt;/p&gt;

&lt;p&gt;I made some charts and spent a lot of time beating my head against the arcane details of matplotlib.
I pushed some charts to our group repository and Kendall provided me some feedback to change a few
things to align more with the morning’s lecture on Visualization.&lt;/p&gt;

&lt;p&gt;Eventually Dean drafted a Presentation and we sort of discussed who’d say what.  We will be wrapping
up charts, etc. and will present.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Benson is wrapping up.</summary></entry><entry><title type="html">Bootcamp - First Week Wrapup</title><link href="/metis/2018/01/19/Bootcamp-First-Week-Wrapup/" rel="alternate" type="text/html" title="Bootcamp - First Week Wrapup" /><published>2018-01-19T00:00:00+00:00</published><updated>2018-01-19T00:00:00+00:00</updated><id>/metis/2018/01/19/Bootcamp-First-Week-Wrapup</id><content type="html" xml:base="/metis/2018/01/19/Bootcamp-First-Week-Wrapup/">&lt;p&gt;We have concluded the First Week!  Yay!&lt;/p&gt;

&lt;p&gt;The pace has been brutal.  Many of us are extremely sleep deprived.&lt;/p&gt;

&lt;p&gt;I was up very late last night forcing myself to incorporate the geo-data.
I then fought through library installation (and a bit of library debugging) to be able to follow some
examples I’ve seen on the web.  I got a basic heat map going.  It’s almost meaningless for our purposes.
But it’s good for street cred - it impressed some of my peers.  Several were only barely behind me.  Shortly
after I got there this morning Tiffany asked me about her troubles installing basemap.  I “slacked” her the
command lines I used to install that and half-a-dozen other modules.&lt;/p&gt;

&lt;p&gt;Oh yeah… on that… they weren’t kidding we DO use Slack a ton.  But we use it for &lt;strong&gt;work&lt;/strong&gt; even more
than for the lessons or other messaging.  It doesn’t replace email.  It’s just a &lt;strong&gt;very&lt;/strong&gt; convenient
way to do a quick-and-dirty cut-and-paste of a URL, a code snippet, a file, etc.  For a lot of things, there’s
no point to say “look at this”.  Why waste the other person’s time making them get up and walk around the
table.  They’re busy; you’re busy.  You use half-your brain to cut-and-paste to answer a peer’s question
while they’re wrestling with their problem and you’re struggling with yours.&lt;/p&gt;

&lt;p&gt;At 5pm, we segued into a Social for a couple of hours.  Light snacks, booze and some goofy games.
This stuff is important too.  Many of us are competitive.  Many of us are presumptuous, even precocious.
We’re often criticizing each other’s work and even arguing over methodology.  It is very helpful to
simply chill out and even be silly.&lt;/p&gt;

&lt;p&gt;The first Week concluding aligns more or less with the first Project concluding.  I had to chuckle when
I came across some of the curriculum/guideline that explained when the first project would truly be over.
It’s over when you blog about it.  Chuckle… OK.  It would be silly to claim the fact I had that project
set up on my blog within hours of the beginning of the project means I’ve already met that requirement.
But I’ve &lt;strong&gt;been&lt;/strong&gt; blogging about the project.&lt;/p&gt;

&lt;p&gt;However, we had our lesson on blogging today.&lt;/p&gt;

&lt;p&gt;And I’m horrified.&lt;/p&gt;

&lt;p&gt;Let me explain… if I thought earlier that having gotten the blog out of the way ahead of time would mean
I would have more “free time”, this was slightly incorrect.  Why?  Well… because there wasn’t that much
time in the day where I could have just ignored the lecture.  But that’s not what terrifies me.  No…
what scares me is reflecting that the contrast between the other students getting a short lecture on
blogs despite time required to get a good one going is the same sort of thing I’m going to face shortly
on other aspect such as Stats.  Coupled with the rapid pace on other topics, it’s clear I may have been
right to believe things would get tough after the first couple of weeks.&lt;/p&gt;

&lt;p&gt;Oh well… time to gird my loins.&lt;/p&gt;

&lt;p&gt;And doggone it if I didn’t actually catch a blog bug today anyway.  During the lecture on Visualization, the
declaration of what or Presentation should be made me think.  We are expected to create a slide presentation.
Well… I thought this is exactly what would look good on the blog.  Yeah, sure it would be sufficient
all on its own.  But it would be sweet if I could add that functionality…&lt;/p&gt;

&lt;p&gt;Yup.  So sure enough, I’ll be working on my blog anyway!  Just like everyone else.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">We have concluded the First Week! Yay!</summary></entry><entry><title type="html">More Blog Enhancements</title><link href="/blog/2018/01/19/Blog-More-Enhancements/" rel="alternate" type="text/html" title="More Blog Enhancements" /><published>2018-01-19T00:00:00+00:00</published><updated>2018-01-19T00:00:00+00:00</updated><id>/blog/2018/01/19/Blog-More-Enhancements</id><content type="html" xml:base="/blog/2018/01/19/Blog-More-Enhancements/">&lt;p&gt;I’ve caught a desire to add some functionality to the blog…&lt;/p&gt;

&lt;p&gt;I felt something was… well… missing with the layout for projects that I’ve seen during my
exploratory forays into how folk out in the wild web are using Jekyll sites for these sorts of
things.  The HydeJack Pro theme is in line with most.  And with the addition of a link back
to blog posts about the project, I’m fairly comfortable with it… but…&lt;/p&gt;

&lt;p&gt;Today, I was inspired when we reviewed the expected format for a slide presentation for the
first project of the Metis bootcamp.  I imagined that would help a ton.  I may be going a bit
too far with regards to what others (as in recruiters, employeers, etc.) may want to see vs.
what I want to see.  But I just feel a dense batch of text may not be the best way to present
a project.  I mean, what would separate that from the blogging itself?  Furthermore a quick
hope from the austere front page I’ve set up to a similarly sparse presentation may be nice.
Folk can do a quick get-in/get-out.  If they want, they’ll be more robust stuff to pursue.&lt;/p&gt;

&lt;p&gt;So after a bit of web research, I am going to try to add in some reveal.js magic to the blog.
As before, I’ll likely start with SwirlyPy… my guinea pig project for the purpose of 
having a project on the
blog.  I do intend to get back to that project… eventually… I still feel there’s a place
for that.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Projects" /><summary type="html">I’ve caught a desire to add some functionality to the blog…</summary></entry><entry><title type="html">Project Benson - Data Exploration 02</title><link href="/project/benson/2018/01/18/Project-Benson-X02/" rel="alternate" type="text/html" title="Project Benson - Data Exploration 02" /><published>2018-01-18T00:00:00+00:00</published><updated>2018-01-18T00:00:00+00:00</updated><id>/project/benson/2018/01/18/Project-Benson-X02</id><content type="html" xml:base="/project/benson/2018/01/18/Project-Benson-X02/">&lt;p&gt;Project Benson - More Data Exploration&lt;/p&gt;

&lt;p&gt;More notebooks I developed as I played around with the data…&lt;/p&gt;

&lt;h3 id=&quot;frustration&quot;&gt;Frustration&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/joseph-r-hamilton/Project_Benson/blob/master/Exploration/Project%20Benson%20-%20Exploration%2002.ipynb&quot;&gt;Project Benson - Exploration 02&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I started to work on an approach using Resampling and Interpolation.  I wanted this done with a MultiIndex.
It seemed tricky but I got it working easily enough…  Except that it returned negative numbers.
For the next day, I backed off and worked in an entirely different manner.  When I finally got that working
and &lt;strong&gt;still&lt;/strong&gt; had negative results, I finally went back and explored the source data only to discover that
some of the turnstile are counting backwards.  The negative results were fine.&lt;/p&gt;

&lt;p&gt;I’ve abandoned this alternative approach.  But it was a good exercise nonetheless.&lt;/p&gt;

&lt;h3 id=&quot;cleanup-analysis&quot;&gt;Cleanup Analysis&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/joseph-r-hamilton/Project_Benson/blob/master/Exploration/Project%20Benson%20-%20Exploration%2003b.ipynb&quot;&gt;Project Benson - Exploration 03b&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After returning to the straightforward method, the next step was to try to determine how to clean things up.
Part of this analysis highlighted a need to tweak my methodology.&lt;/p&gt;

&lt;p&gt;Next, however, I had to determine how to handle truly anomalous information.&lt;/p&gt;

&lt;h3 id=&quot;a-clean-notebook&quot;&gt;A Clean Notebook&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/joseph-r-hamilton/Project_Benson/blob/master/Exploration/Project%20Benson%20-%20Exploration%2005.ipynb&quot;&gt;Project Benson - Exploration 05&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After working out methods for Data Cleanup and Aggregation, I created this notebook to serve as a clean way
to show the process, to share with the team and to present to the larger audience.&lt;/p&gt;

&lt;p&gt;This can then serve as a springboard for further analysis and processing.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Benson - More Data Exploration</summary></entry><entry><title type="html">Project Benson MVP</title><link href="/project/benson/2018/01/18/Project-Benson-MVP/" rel="alternate" type="text/html" title="Project Benson MVP" /><published>2018-01-18T00:00:00+00:00</published><updated>2018-01-18T00:00:00+00:00</updated><id>/project/benson/2018/01/18/Project-Benson-MVP</id><content type="html" xml:base="/project/benson/2018/01/18/Project-Benson-MVP/">&lt;p&gt;Project Benson is past MVP.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MVP = Minimal Viable Product&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We presented our MVP early this afternoon.  For us, this meant presenting a Jupyter Notebook
that highlighted our methodology crunching the data for one week and turning that into a ranked
order of stations by traffic.&lt;/p&gt;

&lt;p&gt;There are three teams in the entire cohort.&lt;/p&gt;

&lt;p&gt;This isn’t a competition so much but all the teams are essentially tackling the same problem.
Since the entire cohort is the audience for the presentations, it is &lt;em&gt;very&lt;/em&gt; interesting because
we may all be at different points of the discovery or be using different strategies entirely.&lt;/p&gt;

&lt;p&gt;Each team’s presentations had different strengths, etc.&lt;/p&gt;

&lt;p&gt;Our team was the only one that had incorporated the technique of resampling for time series.&lt;/p&gt;

&lt;p&gt;Another team was the only one who had punched through Tableau for some pretty pictures.&lt;/p&gt;

&lt;p&gt;The third team was the only one which had decided to incorporate the geographical data of tech
companies in the greater New York area.&lt;/p&gt;

&lt;p&gt;There’s precious little time left.  We have to have practiced the presentation by end-of-day tomorrow.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Benson is past MVP.</summary></entry><entry><title type="html">Bootcamp - Pair Programming</title><link href="/metis/2018/01/18/Bootcamp-Pair-Programming/" rel="alternate" type="text/html" title="Bootcamp - Pair Programming" /><published>2018-01-18T00:00:00+00:00</published><updated>2018-01-18T00:00:00+00:00</updated><id>/metis/2018/01/18/Bootcamp-Pair-Programming</id><content type="html" xml:base="/metis/2018/01/18/Bootcamp-Pair-Programming/">&lt;p&gt;Pair Programming is… umm… interesting.&lt;/p&gt;

&lt;p&gt;The structure of the Metis Data is broken down such that we cover different things simultaneously,
concurrently rather than just a deep focus on one thing.&lt;/p&gt;

&lt;p&gt;The projects are the emphasis.  So, it is really easy to fall into a mode (or mood) where you’re
absorbed by or focused on the project of the moment.  The lectures and “other things” serve to
break that.&lt;/p&gt;

&lt;p&gt;One of these “things” is &lt;strong&gt;&lt;em&gt;Pair Programming&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Every day starts with about half-an-hour of Pair Programming.&lt;/p&gt;

&lt;p&gt;Pair Programming serves many purposes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The instructors get a chance to see the students’ code on a regular, daily basis.  It’s one thing
to tell the students about certain practices of coding Python.  It’s another thing to confirm or
chastise students to verify they are incorporating these practices.&lt;/li&gt;
  &lt;li&gt;Working in pairs forces people to work together.  Only person gets the computer.  But either may
have the idea.  Nonetheless, you have to communicate and the other can critique accordingly.&lt;/li&gt;
  &lt;li&gt;These are timed exercises.  So the pressure is on.  This then simulates the experience of code
interviews where in a job interview you have to code or whiteboard.  The types of problems are
also similar to the kind you’d experience in code interviews.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Pair Programming is… umm… interesting.</summary></entry><entry><title type="html">Project Benson Progresses</title><link href="/project/benson/2018/01/17/Project-Benson-Progresses/" rel="alternate" type="text/html" title="Project Benson Progresses" /><published>2018-01-17T00:00:00+00:00</published><updated>2018-01-17T00:00:00+00:00</updated><id>/project/benson/2018/01/17/Project-Benson-Progresses</id><content type="html" xml:base="/project/benson/2018/01/17/Project-Benson-Progresses/">&lt;p&gt;Project Benson is underway.&lt;/p&gt;

&lt;p&gt;OK.  The last 24 hours have been somewhat of a rollercoaster with regards to Project Benson.&lt;/p&gt;

&lt;p&gt;I spent a fair amount of time pursuing the data from a Exploratory Data Analysis point of view.
And I simply loved it.  It was good practice with Pandas.  I learned several new aspects of how
to use Pandas for time series sorts of data.  It was also frustrating when I got stuck trying
to do things that appeared to work but didn’t.&lt;/p&gt;

&lt;p&gt;But in the morning Alice reminded us that good Data Scientists do &lt;strong&gt;NOT&lt;/strong&gt; jump right into the data
searching for things that can be done with the data.  A Data Science project begins with Design.
This was a repeat.  I’m pretty sure she said the same thing yesterday.  But… it was bewildering
then and it was chastening today.  Because… the data is so much FUN.&lt;/p&gt;

&lt;p&gt;There’s a number of traps and pitfalls here.  First of all, we may end up either limited by focusing
too much on the data at hand or simply incredibly biased because of this focus.  Next, we can spend
a lot of time pleasing ourselves or attempting to build monuments which may be overkill or mismatched
compared to what anyone ever really wanted.&lt;/p&gt;

&lt;p&gt;So… how do you do Design before jumping into the Data?  We discussed this…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, &lt;strong&gt;listen&lt;/strong&gt; to your Customers.  Learn what problems they want solved.  Discern what may be
a threshold of satisfaction or a point of diminishing returns.&lt;/li&gt;
  &lt;li&gt;Second, &lt;strong&gt;brainstorm&lt;/strong&gt;.  Think about the problem.  Ponder how you’ll approach the challenge.  Draft your goals.&lt;/li&gt;
  &lt;li&gt;Determine a &lt;strong&gt;MVP - Minimum Viable Product&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach should help to guide the overall effort.  It also sets you up for an iterative method
where you can have &lt;em&gt;something&lt;/em&gt; to show for your work ASAP but then build upon it.&lt;/p&gt;

&lt;p&gt;So… what did we do?  Jump back into the data.&lt;/p&gt;

&lt;p&gt;Well.. that’s not entirely true.  We spent a lot of time debating methodology… about the data.&lt;/p&gt;

&lt;p&gt;Nah… we hastily determined our MVP would be simply a ranked list of the MTA stations based on traffic flow.&lt;/p&gt;

&lt;p&gt;But the trouble here is determining how to assess this.  The “fun” part of the MTA data is this isn’t actually
quite readily availble.  You must massage the data a bit.&lt;/p&gt;

&lt;p&gt;Kendall and I discovered each of us had wrestled with trying to determine the structure behind the data.
We still cannot quite figure out what some of it actually means.  Oh we could figure out what to count.
But… there were hints that you might be able to say more if you undertood more of what it represented.&lt;/p&gt;

&lt;p&gt;At some point I described what I’d been doing with standardizing and interpolating by resampling the data.
Kendall and I debated that a lot because her Math/Stats background made her leery of interpolation, especially
dumb linear interpolation.  Dean jumped in and we explored this a bit.  But it wasn’t clear any other thing
we were discussing would be any easier or better.  Derrick stayed quiet and simply plugged away at another
approach altogether.  When he shared his idea, we pointed out the errors inherent in that approach and guided
him how to continue with it by aggregating up to the STATION level.&lt;/p&gt;

&lt;p&gt;All of our approaches have errors.  Linear interpolation makes assumptions about the rate between the
interpolation.  But so do alternative approaches based on other distributions.  Derrick’s approach bumped
up the granurality to 1 day but with known errors around midnight.&lt;/p&gt;

&lt;p&gt;We’re continuing to work on anything and everything.  But we selected Derrick’s approach because it’s
easier to do quicker.  And… boy is this projecting being rushed!  Dean typed up our Design Statement
and submitted it.  Our MVP is due tomorrow, less than 48 hours after the project started!&lt;/p&gt;

&lt;p&gt;Furthermore, if I get the interpolation working it would permit us to the same 4-hour granularity, or
lower as long as we remember it’s interpolated.  And it would very likley corroborate the per-day
approach.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Benson is underway.</summary></entry></feed>