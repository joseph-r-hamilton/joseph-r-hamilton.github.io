<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-01-31T02:53:58+00:00</updated><id>/</id><title type="html">Joseph Hamilton - Solution Oriented Data Scientist</title><subtitle>A website by and for Joseph Hamilton. A place to explore projects and pursuits, to chase curiosities and catch dreams.
</subtitle><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><entry><title type="html">Project Luther Continues</title><link href="/project/luther/2018/01/30/Project-Luther-Continues/" rel="alternate" type="text/html" title="Project Luther Continues" /><published>2018-01-30T00:00:00+00:00</published><updated>2018-01-30T00:00:00+00:00</updated><id>/project/luther/2018/01/30/Project-Luther-Continues</id><content type="html" xml:base="/project/luther/2018/01/30/Project-Luther-Continues/">&lt;p&gt;Project Luther continues.&lt;/p&gt;

&lt;p&gt;We present on Friday.  However, the Instructors have issued the stricture that we must have our analysis complete
by end-of-day Wednesday.  Now… I may take a bit of issue with what “end-of-day” means.  But they’ve specified
Thursday is to be solely for development of presentations.&lt;/p&gt;

&lt;p&gt;For me, this means that Tuesday was/is for &lt;strong&gt;more&lt;/strong&gt; data scraping.  And Wednesday is primarily going to be devoted
to analysis.  Or to put it another way, I cannot do a good job on the presentation if I’m still fighting with the
analysis.  And I cannot do justice to the analysis, if I’m still scrambling over scraping.&lt;/p&gt;

&lt;p&gt;There’s simply not enough time to do all of what we want on these projects.  I had hoped to incorporate some
basic Unit Testing and CI support.  That’s out.  But I &lt;strong&gt;did&lt;/strong&gt; implement the pipeline to MongoDB as a backend.
My fears may have been unwarranted.  But I was leery of issues of concurrent scrapes dumping to a single
text file.  It was likely not really an issue.  I had though the concurrency setting was set to “1”.  But that
was what I had seen as I reviewed the project from last year.  In any case, I’m happy to have incorporated
MongoDB.&lt;/p&gt;

&lt;p&gt;I did implement a scraping approach that could end up pulling all the property tax data from Will County…
if it’s left to run long enough.  It’s still a bit slow for some odd reason.  Maybe, I can tweak it a bit…
who knows.&lt;/p&gt;

&lt;p&gt;But I’ve already shifted to analysis.  The pair-plots seemed to show me that there may yet be some pattern or
signal to capture but it’s being swamped by outliers.  So I started doing some box-plots and histograms to get
a sense for things… And oh boy!  I have outliers.&lt;/p&gt;

&lt;p&gt;I still have work to do to incorporate a good number of my features.  And I may be able to create some
more features based on aggregated data (assuming I can get reasonably close to a complete scrape).  But even
with the initial set of records, it’s clear there are things I could do.  I may end up jettisoning some records
to get rid of outliers.   Or I may switch those features to logarithms.  I’ll have to experiment.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="MongoDB" /><summary type="html">Project Luther continues.</summary></entry><entry><title type="html">Bootcamp - Transit Woes</title><link href="/metis/2018/01/30/Bootcamp-Transit-Woes/" rel="alternate" type="text/html" title="Bootcamp - Transit Woes" /><published>2018-01-30T00:00:00+00:00</published><updated>2018-01-30T00:00:00+00:00</updated><id>/metis/2018/01/30/Bootcamp-Transit-Woes</id><content type="html" xml:base="/metis/2018/01/30/Bootcamp-Transit-Woes/">&lt;p&gt;I’m stuck on a train!!  :cry:&lt;/p&gt;

&lt;p&gt;We have folk who take Uber.  We have those who navigate the CTA, bus and rail.  Few drive on a
regular basis.&lt;/p&gt;

&lt;p&gt;I’ve driven a couple of times so far.  It may make more sense for me to take the train.  But there are times,
I need to do things in the afternoon or evening where I need the car.  From a raw time perspectice, so far,
it seems driving gets me there slightly earlier.  Rumor has it there might be some places to park for free.
I just use the parking lot next to the Metis facility.  They have an “early bird” special if you get in before
a certain time.  But… the first time I drove, something really weird happened.  When I tried to exit, I ended
up in a slow line of cars.  The car in front of me seemed to be stuck for some odd reason.  And I kept hearing
noises like they were talking to someone over the intercom.  I’d never seen anything like this in that garage.
I had always parked there when coming to Metis for weekend or evening stints.  Strange.  It was like being
behind someone counting out pennies at a toll booth.  My turn came… and I had the pleasure of discovering
the cause for the backup - their scanner wouldn’t work… at all.  So it became my chance to push the intercom
and begin the goofy discussion trying to explain to the person on the other end what was wrong.  They tried
to take my information (info from the silly ticket).  They determined my rate and attempted to set that
manually on their end.  Even that didn’t work.  Eventually they just raised the bar remotely and let me out.
I guess I should be thankful that part of the system still worked.&lt;/p&gt;

&lt;p&gt;The next day?  Back to normal.  Weird.&lt;/p&gt;

&lt;p&gt;Well, I’ve experienced slight delays on the Metra.  But tonight is my first opportunity to be trapped on
a broken train.  Oh what fun.  It could be worse, of course.  If I were stuck in gridlock traffic on the
Eisenhower, for example, it wouldn’t be practical to whip out the laptop and fire off a few blog posts.
But I must say it’s adding insult to injury to watch the later trains zoom by on the alternate track.&lt;/p&gt;

&lt;p&gt;At the moment, Metra has another train behind us pushing us.  Not sure what the outcome of that will be.
It seems the plan is huff and puff and crawl to the nearest station.  And then we get off and get on
another train?&lt;/p&gt;

&lt;p&gt;Yup… that was the ticket.&lt;/p&gt;

&lt;p&gt;I’m back home now.  We were pushed to the station where we all had to get off and cross to the other
side where another train was waiting for us.  It seems that one was one of the later trains that had been
commandeered to assist us.  I wondered when I’d eventually have to ride the Metra standing.  That ended 
being today.  While balancing in the aisle on a bouncing, lurching train, my son called me.  I had to
hang up on him quickly because it was a tad difficult to manage it all.&lt;/p&gt;

&lt;p&gt;Whew.  Well… I hope that doesn’t happen again anytime soon.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">I’m stuck on a train!! :cry:</summary></entry><entry><title type="html">Project Luther - MVP</title><link href="/project/luther/2018/01/29/Project-Luther-MVP/" rel="alternate" type="text/html" title="Project Luther - MVP" /><published>2018-01-29T00:00:00+00:00</published><updated>2018-01-29T00:00:00+00:00</updated><id>/project/luther/2018/01/29/Project-Luther-MVP</id><content type="html" xml:base="/project/luther/2018/01/29/Project-Luther-MVP/">&lt;p&gt;Project Luther passes MVP.&lt;/p&gt;

&lt;p&gt;I gave the MVP for Luther earlier today.&lt;/p&gt;

&lt;p&gt;I was actually quite bewildered with what to do given the sense that there was nothing really to show.&lt;/p&gt;

&lt;p&gt;This is a bit of a trap.  There are so many interesting little traps we can fall into.
There’s the problem of perfectionism… where we may want to keep improving or tweaking or correcting
or whatever well beyond a reasonable timeframe.  There are lots of pitfalls with regards to misunderstanding
the data or not fully appreciating the algorithms or tools being used.&lt;/p&gt;

&lt;p&gt;Then, there is the entire issue of what IS an MVP.&lt;/p&gt;

&lt;p&gt;The curriculum today actually included a few pointers and references on this topic.  It seems as much as
is possible the idea is to have progressed fairly far through the goals of the project with a small sample
of the data.  This irks me somewhat because for a large class of projects, once you’re able to do something
with a small bit of data, there’s not much to do related to scaling.  In short, if you can crunch a few
records, you’re already almost completely done.&lt;/p&gt;

&lt;p&gt;In my case, the goal was to have completed data scraping and have done some regression.  I had reached that
point.  I hadn’t done much beyond that.&lt;/p&gt;

&lt;p&gt;Since some of my peers had indeed developed slide presentations, I followed suit.  I chose to develop a
presentation in reveal.js from the beginning.  This made it straightforward to put it on the blog immediately,
since quite literally it was being served from a local instance of the blog.&lt;/p&gt;

&lt;p&gt;I was able to switch immediately to the underlying Jupyter notebook to show the bit of regression I’d done…&lt;/p&gt;

&lt;p&gt;which seemed to show almost no coorelation at all between my target and the various features.&lt;/p&gt;

&lt;p&gt;The other aspect of this round of presenting MVPs was the time allotted.  It was to be about two minutes.&lt;/p&gt;

&lt;p&gt;For this particular project, the final presentations need to similarly terse.  We get a whopping five minutes.&lt;/p&gt;

&lt;p&gt;For MVP, I scraped about 5,000 records.  After some cleaning and such, I had a bit more than 1,500 records.
The greatest feature-target correlation was about 0.1.&lt;/p&gt;

&lt;p&gt;Nonetheless, it was easy to chart out the “next steps”: more data and more analysis.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Luther passes MVP.</summary></entry><entry><title type="html">Bootcamp - Week Two Down</title><link href="/metis/2018/01/27/Bootcamp-Week-Two-Down/" rel="alternate" type="text/html" title="Bootcamp - Week Two Down" /><published>2018-01-27T00:00:00+00:00</published><updated>2018-01-27T00:00:00+00:00</updated><id>/metis/2018/01/27/Bootcamp-Week-Two-Down</id><content type="html" xml:base="/metis/2018/01/27/Bootcamp-Week-Two-Down/">&lt;p&gt;I have completed Week Two of the Metis Bootcamp!&lt;/p&gt;

&lt;p&gt;I vaguely remember the concept of sleep.  It’s like an old friend I’ll have to visit again once the Bootcamp concludes.&lt;/p&gt;

&lt;p&gt;It’s one thing to look at a curriculum and see Web Scraping and say, “Oh yeah… I know how to do that”.  It’s a
completely different thing to rush through a project where you first have to dream up &lt;strong&gt;WHAT&lt;/strong&gt; to explore, then &lt;strong&gt;HOW&lt;/strong&gt;
to find the related data on the Web and finally to pull together some mechanism to do so in short order.&lt;/p&gt;

&lt;p&gt;In essence, there’s a significant difference between any project you may have done in the past which spanned several
weeks and one where you’re under terrible pressure to do it all very quickly.&lt;/p&gt;

&lt;p&gt;Project One is pretty much the only project where you are even barely spoon fed.  The instructors don’t tell you exactly
where to go and what to do (well… that’s not entirely true… they do… but only you’ve after done it… which is
hilarious).  But the requirements for Project One fairly strongly push you in a certain direction and you end up working
from a small set of sources for your data.  None of that is true for the rest of the Bootcamp.  You are given structure
only in the sense of what methods need to be employed.  But dreaming up what to study is on you.  This is entirely
on purpose since this is a rather valuable facet of being a Data Scientist.  Can you determine what to study in order
to drive towards a desired goal?  Or do you have to be spoon fed what to do?&lt;/p&gt;

&lt;p&gt;But knowing this doesn’t make it any easier.  It’s rather difficult.&lt;/p&gt;

&lt;p&gt;I had a lot of fun this past week with Pair Programming.  I think part of it is that we’re all getting to know one
another better.  So there’s less of the typical sense of guardedness or competition or fear of looking foolish.
Another piece of the puzzle may be that we’re all so overwhelmed or exhausted that if anyone can drive things towards
the goal, we’re happy no matter who drives.&lt;/p&gt;

&lt;p&gt;Project Two is actually entirely solo.  But it does fall in line with my earlier assessment that the projects
overall trend from group to solo.  So, Project Two is solo… but… we’ve all been paired with a “buddy”.  The
instructors waited till folk had chosen their focus of study in order to pair people up based on topic.  I have no
idea how well this functions for most cohorts, but for our cohort it sort of magically worked out because we 
really did end up with many people chasing similar topics.  We’re supposed to be an aid or a cheerleader to our
buddy.  We’ve been given explicit instruction that whenever we hit a roadblock we go to our buddy first before
hitting up the instructors.&lt;/p&gt;

&lt;p&gt;This helps.  But I’ve actually been learning lots of little things from everyone.  I &lt;em&gt;did&lt;/em&gt; follow my buddy (Nate)
in dumping data to a text file first before anything else.  Easy to see.  Easy to pull into pandas in a Jupyter
notebook for more experimentation.  Sure I want a more robust mechanism.  But, do that later.  I have also been
intrigued by those who chose topics I discarded because I couldn’t quickly determine how to look up data to scrape.  One
trick appears to be just to tack on “database” after the topic in a typical web search.&lt;/p&gt;

&lt;p&gt;In week two, we actually got a decent amount of instruction related to how to do Project Two before or during the
execution of the project.  For so much of this, there still remains a lot more we have to figure out beyond that.
Furthermore, the various challenges we face depends a lot of what we’re actually tackling.&lt;/p&gt;

&lt;p&gt;Also, in week two we started with workshops and meetings with Career Services.  For many of us, our first contacts
with Metis involved the Program Manager, Nathan.  When you start doing “things” with Metis, you deal with Nathan
for pretty much any particular event.  And, as you might imagine, the &lt;strong&gt;Program&lt;/strong&gt; Manager is our interface to
all sorts of logistics, such as when anyone gets stuck in the elevator or the heating in the building doesn’t quite 
seem to be working.  But I must say even if Ashley never gave a lecture or met with me personally, it only takes
couple of weeks of being in the same work space to realize that woman works hard.  Though things may change in the
future Nathan and Ashley have a sort of open office arrangement.  And due to the arrangement of rooms, their desks
are in between the lecture room and the restrooms.  So we can hear Ashley hard at work.  And we can easily imagine
that’ll be us on the other end of that phone in a few months.&lt;/p&gt;

&lt;p&gt;A lot of what Career Services is providing is stuff I’ve heard before since this is sort of my third round through
these sorts of agencies.  Nonetheless, it’s good to get it tailored a bit towards this specific industry.&lt;/p&gt;

&lt;p&gt;Oh… another thought I’ll toss in here before I wrap up this week’s summary.  Our TA Ibrahim was in on Friday.
We had some interesting discussions with him.  A lot of us really don’t yet have a good feel for this industry.
Ask me the major telecom companies and I can rattle them off.  I cannot yet do that for Data Science.  We queried
Ibrahim for stuff like that.  But even more interesting was when someone pressed him to contrast his experience
in his Masters Porgram with the Bootcamp.  See… Ibrahim is a relatively recent grad of the Metis Bootcamp himself.
He snuck the Bootcamp in between the years of his Masters program in, I believe, Statistics.  Ibrahim stated he
wanted to do the Bootcamp to help him build his portfolio because he couldn’t easily use his projects at school
to do the same (due to restrictions, NDAs, etc.).  I found this fascinating given how hard it is to assess
the benefit of having a portfolio.&lt;/p&gt;

&lt;p&gt;I realize one can never really be sure of everything… you know… Plato’s cave and all that.  But, if I needed
little tidbits to help me feel better about my reasons for jumping into the Bootcamp (as expensive as it is), I have several:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ibrahim stepping “down” from a Masters in Statistics into the Metis Bootcamp to build up his portfolio.&lt;/li&gt;
  &lt;li&gt;Adam, who as a Data Science Recruiter decided he didn’t just want to help Data Science folk get a job.  He wanted to
go and become a Data Scientist.&lt;/li&gt;
  &lt;li&gt;Dean, whose wife is a recent graduate of the Metis Bootcamp (and who deleted her Project One work so he couldn’t copy it…
chuckle…).  I mean, as a couple, they decided after paying for one of them to pay for the other.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">I have completed Week Two of the Metis Bootcamp!</summary></entry><entry><title type="html">Project Luther - Discovery</title><link href="/project/luther/2018/01/25/Project-Luther-Discovery/" rel="alternate" type="text/html" title="Project Luther - Discovery" /><published>2018-01-25T00:00:00+00:00</published><updated>2018-01-25T00:00:00+00:00</updated><id>/project/luther/2018/01/25/Project-Luther-Discovery</id><content type="html" xml:base="/project/luther/2018/01/25/Project-Luther-Discovery/">&lt;p&gt;Project Luther is in motion.&lt;/p&gt;

&lt;p&gt;For Project Luther, I need to move quickly towards a MVP (Minimal Vialble
Product).  This has been broken into two parts.  I believe the formal MVP
presentation is Monday.  But Friday I must be able to show I’ve already begun
to collect data.  This projects has two key aspects: Web Scraping; Linear
Regression.  So to a degree the MVP for the Scraping is tomorrow.&lt;/p&gt;

&lt;p&gt;I’ve performed a bit of Exploration and Discovery.  I played with Selenium
and the websites for four of the counties of the Chicagoland area:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cook&lt;/li&gt;
  &lt;li&gt;DuPage&lt;/li&gt;
  &lt;li&gt;Lake&lt;/li&gt;
  &lt;li&gt;Will&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I evaluated these for ease of access as well as for the amount and type of
data I could procure.  For the purpose of the MVP I have settled on Will
countly alone.&lt;/p&gt;

&lt;p&gt;Next, due to the need to show data tomorrow, I cannot do start as I’d hoped.
I do want to start to switch to CI.  But I dare not waste time on getting that
part working.&lt;/p&gt;

&lt;p&gt;Hilarously, I already have a template project to pull from for the web
scraping.  I will likely create a writeup for that project in the near future.&lt;/p&gt;

&lt;p&gt;PLACEHOLDER (I’ll come back here later and create a link to it).&lt;/p&gt;

&lt;p&gt;But just because I cannot immediately integrate CI doesn’t mean I shouldn’t go
ahead and start the repository.  So here’s the short-term plan…&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create the repository for GitHub&lt;/li&gt;
  &lt;li&gt;Use Jupyter, Selenium and BeautifulSoup for prototyping&lt;/li&gt;
  &lt;li&gt;Begin development with Scrapy&lt;/li&gt;
  &lt;li&gt;Incorporate Mongo for storage&lt;/li&gt;
  &lt;li&gt;Pull some data&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Luther is in motion.</summary></entry><entry><title type="html">Project Luther Begins</title><link href="/project/luther/2018/01/24/Project-Luther-Begins/" rel="alternate" type="text/html" title="Project Luther Begins" /><published>2018-01-24T00:00:00+00:00</published><updated>2018-01-24T00:00:00+00:00</updated><id>/project/luther/2018/01/24/Project-Luther-Begins</id><content type="html" xml:base="/project/luther/2018/01/24/Project-Luther-Begins/">&lt;p&gt;Project Luther begins.&lt;/p&gt;

&lt;p&gt;Yay!  With this project I get to play with web scraping.&lt;/p&gt;

&lt;p&gt;There are so many aspects of Data Science which I have employed at various points throughout my
career and personal life.  During the lectures this morning we discussed what Machine Learning is
at its most basic definition.  In this definition a lot of the optimization things I did long ago
would qualify as Machine Learning.  Indeed, some of the stuff I’ve done with fitness functions
have quite a lot of resemblance in spirit to Gradient Descent.&lt;/p&gt;

&lt;p&gt;But this project is not just web scraping.  It’s web-scraping plus linear regression.&lt;/p&gt;

&lt;p&gt;It actually proved a tad difficult to choose a topic to investigate.  See… from here on out,
we are only given a few requirements and rough guidelines.  From these we define the project.&lt;/p&gt;

&lt;p&gt;So here, we need something that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Involves some sort of structured data we scrape from the web.  We can use additional data of
any kind from any source.  But the first part of the data needs to be scraped.&lt;/li&gt;
  &lt;li&gt;Has enough data of this structured fashion that we can do some linear regression.&lt;/li&gt;
  &lt;li&gt;Has to be interesting (at least to me).&lt;/li&gt;
  &lt;li&gt;Ought to be somewhat novel.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hmm… tough call.&lt;/p&gt;

&lt;p&gt;I played around last night with a number of ideas.  I settled on something to do with property
tax assessments since I’ve been curious whether there are measurable effects of increasing property
tax.  I don’t think I’ll be able to get enough data longitudinally (in time) to see much.  But I
ought to be able to measure across space or across valuations.&lt;/p&gt;

&lt;p&gt;An even tougher call is HOW to do this…&lt;/p&gt;

&lt;p&gt;I don’t mean how to scrape.  Though I will have to decide for or against inclusion of Scrapy.
I mean Jupyter vs. Python, how much to employ Test-Driven Development, CI, etc.&lt;/p&gt;

&lt;p&gt;MVP is Monday.  Again… the pressure is on.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Luther begins.</summary></entry><entry><title type="html">I broke the blog!</title><link href="/blog/2018/01/24/Blog-Is-Broken/" rel="alternate" type="text/html" title="I broke the blog!" /><published>2018-01-24T00:00:00+00:00</published><updated>2018-01-24T00:00:00+00:00</updated><id>/blog/2018/01/24/Blog-Is-Broken</id><content type="html" xml:base="/blog/2018/01/24/Blog-Is-Broken/">&lt;p&gt;I recently broke the blog!!&lt;/p&gt;

&lt;p&gt;I was indeed able to add in the functionality so I could include slide presentations on the blog!!&lt;/p&gt;

&lt;p&gt;YAY!&lt;/p&gt;

&lt;p&gt;However… as with so much related to the Metis Bootcamp, I was under pressure because of a time crunch.&lt;/p&gt;

&lt;p&gt;There is a lot that I could do with this functionality, but for the purpose of using this within
the project post for Project Benson, I chose to “simply” convert the presentation our tream created
from Powerpoint to Reveal.  I got it!  But… I hadn’t yet pushed any example slide presentation
all the way to the public blog.  I didn’t want this because I didn’t want the public blog with the
Swirply project pointing to a dummy slide.&lt;/p&gt;

&lt;p&gt;But…&lt;/p&gt;

&lt;p&gt;This led to the scenario where I finished the presentation conversion and got done testing things
locally to my heart’s content and then attempted the final push around the time the whole thing
was due.  Actually I was about ten minutes beyond the deadline.  It worked locally, so it should
be fine, right?&lt;/p&gt;

&lt;p&gt;Well…&lt;/p&gt;

&lt;p&gt;I set up a pipeline for the blog.  The push to the public blog goes through a “test” stage.
This is very helpful because not only does it ensure I don’t creating broken links on my own,
it will alert me to cases where external links fail.  This could be because I mistyped it. 
Or, as I’ve learned, it could be because the external site is down that day.  I imagine eventually
I’ll run into cases where the external site has reconfigured or deleted that page.  This testing
lets me catch all that and detemine how to clean it up if I wish or just ignore the problems.&lt;/p&gt;

&lt;p&gt;This testing failed due to an non-trivial number of issues related to things related to the
inclusion of reveal.js.  There was no simple way to ignore all of that in the same way I
can ignore a single URL.&lt;/p&gt;

&lt;p&gt;So… I just had to disable that test phase.&lt;/p&gt;

&lt;p&gt;To get one functionality, I had to break another.&lt;/p&gt;

&lt;p&gt;Sigh…&lt;/p&gt;

&lt;p&gt;EDIT: Blog is fixed.&lt;/p&gt;

&lt;p&gt;I cleaned it up the next day.&lt;/p&gt;

&lt;p&gt;The errors fell into about three categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The testing was checking things in a file within reveal.js that was probably only included there as a sample to use for other things.  The solution here was to mask that directory/module.&lt;/li&gt;
  &lt;li&gt;A sample I’d copied was using data-src in the img tags rather than src.  Testing caught that even though it “worked”.&lt;/li&gt;
  &lt;li&gt;A sample I’d copied had a call to favicon in a different way… fixed it.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="blog" /><category term="Metis" /><summary type="html">I recently broke the blog!!</summary></entry><entry><title type="html">Bootcamp - Week Two</title><link href="/metis/2018/01/22/Bootcamp-Week-Two/" rel="alternate" type="text/html" title="Bootcamp - Week Two" /><published>2018-01-22T00:00:00+00:00</published><updated>2018-01-22T00:00:00+00:00</updated><id>/metis/2018/01/22/Bootcamp-Week-Two</id><content type="html" xml:base="/metis/2018/01/22/Bootcamp-Week-Two/">&lt;p&gt;We’ve segued into Week Two…&lt;/p&gt;

&lt;p&gt;Week Two started with the presentations of Project Benson, the first project.&lt;/p&gt;

&lt;p&gt;These started at 9:30am.  Pair Programming was cancelled.  Alice said we’ll never do
Pair Programming on mornings where we present.&lt;/p&gt;

&lt;p&gt;Well… this was good since we were making last minute changes to the presentation, quite literally.
Dean had changed the them of the presentation which meant the charts and things we’d put into the last
format didn’t quite fit.  And he’d left out one of my charts which I urged him to stuff back in.&lt;/p&gt;

&lt;p&gt;Our team went first.  We magically somehow pulled off a reasonably good presentation despite the
chaos and lack of practice.  And… the new theme got good reviews…&lt;/p&gt;

&lt;p&gt;Our team and the following team both were cut for going too long.  But both teams had made it to
their final slide before the forced ending.  The last team actually ended about half-a-minute early.&lt;/p&gt;

&lt;p&gt;Every team had a slightly different thing that “stuck out”.  Ours was the only team to employ
resampling and weighted averages.  The second to present (Group 1 - Adam, Alex, Chris, Amy) had
created a heat-map (based on tech company locations) with an overlay of the high-traffic stations.
The final team to present (Group 3 - Tiffany, Michael, Nate) was the only group to employ Tableau.
Tiffany apparently has some Tableau experience in her background.  They created a dashboard to
permit the Customer to filter based on desired days and hours.&lt;/p&gt;

&lt;p&gt;Dean had informed me we could get a trail-period license for Tableau.  But Tiffany says there’s another
option which she used for the Bootcamp - the public option.  Apparently you can install Tableau free
but your data must reside on their site.  What an odd requirement.  So this means you should not use
that option for any private or business data.  But it may work great for classes, bootcamps, personal
projects based on public data, etc.  So this is a MUST to try out.&lt;/p&gt;

&lt;p&gt;I’ve got many things to try out.  I’ve pestered Amy to help me later (sigh… FINALLY) switch to vim over vi.
These kids these days must view us old-timers using vi the same way I viewed folk using ed back in the day.&lt;/p&gt;

&lt;p&gt;After the presentation, we’re essentially free for a couple days which is in such stark contrast to
the pressure of Week One.  But we need to do wrapup activities related to the first project.  This
means put the project output on our blog… which many need to create.  And finish and turnin the
challenge (essentially homework).&lt;/p&gt;

&lt;p&gt;In absence of lecture, many of us have escaped the classroom into the large space to plug in extra
monitors.  I’ve finally found the right connectors to be able to hookup two monitors to the laptop
for a total of three…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">We’ve segued into Week Two…</summary></entry><entry><title type="html">Project Benson MVP</title><link href="/project/benson/2018/01/19/Project-Benson-Closing/" rel="alternate" type="text/html" title="Project Benson MVP" /><published>2018-01-19T00:00:00+00:00</published><updated>2018-01-19T00:00:00+00:00</updated><id>/project/benson/2018/01/19/Project-Benson-Closing</id><content type="html" xml:base="/project/benson/2018/01/19/Project-Benson-Closing/">&lt;p&gt;Project Benson is wrapping up.&lt;/p&gt;

&lt;p&gt;We are supposed to be ready to present Monday morning.  We were supposed to have finished up our presentations.
We were supposed to have had a practice presentation.&lt;/p&gt;

&lt;p&gt;We aren’t quite there.  Oh we’re gonna have to present no matter what.  And there was a scheduled social event
to cap out Week-One which really did break the stress and focus, so to speak.&lt;/p&gt;

&lt;p&gt;But I’m not sure any of our groups are truly at a point where they’re not going to be working on this over the
weekend.  And there is a reason they laid out this goal (of having it wrapped up and being free over the weekend)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;they gave us homework… which… ironically would
have been useful &lt;strong&gt;before&lt;/strong&gt; attacking this project.  I’m sure it’ll help reinforce things.  Indeed, I’m
rather certain that’s part of the entire pedagogy here.  We’re getting this training before, during and after
while under immense pressure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If all of the above sounds like doom and gloom, you’re reading it wrong.  I’m thrilled with this.  And I sense
a lot of the others are enjoying this too.  It’s just so fast paced.  And when you force yourself to cut to
the next aspect (for example, from data wrangling to visualization), you have to start an uphill climb all
over again.  You may have gained some confidence by gaining some fluidity in the previous aspect only to feel
like an infant all over again.&lt;/p&gt;

&lt;p&gt;So… enough meta data.  What did we &lt;strong&gt;do&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;Last night I fought with the available geographic data to get a location data for each “Station” in the
MTA data.  The NY data provides geo-data but it is not cleanly aligned with the MTA data.  A bit of patient,
manual sorting and matching got me about 90% there.  Google and Wikipedia got me to about 99%.  For the rest
I just had to make some quasi-intelligent decisions.&lt;/p&gt;

&lt;p&gt;I fought through installation of several libraries and played with plots with maps.  I eventually got a
basic heatmap of all the MTA stations.  It didn’t dawn on me until I could &lt;strong&gt;SEE&lt;/strong&gt; it how odd it is to
be incorporating the non-Manhattan burroughs and simply ignoring Jersey entirely.  The problem statement
didn’t specify Manhattan as the location of the gala, although it seems intuitive.  But the problem statement
clearly lays out the scenario as one where the Customer/Client already had the focus on the MTA.  Too late
now to do anything about it.&lt;/p&gt;

&lt;p&gt;But… this is probably a good example of the issue of putting Design before Data.  Jump right into the
MTA data and you’re locked and biased in that view.  Maybe if you just thought whether it made any sense
to analyze the burroughs the concept of Jersey could have arisen.&lt;/p&gt;

&lt;p&gt;We may have ruled it out.   Indeed, we may have chosen to ignore the burroughs if we could.  But what strikes
me is how we utterly ignored New Jersey entirely.&lt;/p&gt;

&lt;p&gt;Otherwise, the challenge for our team today was to shift from Data Analysis to Visualization.  We simply
did not have the time to properly synchronize or synthesize our various methodologies.  All of our were
doing slightly different things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Me: a Time Series resampling, interpolation, cleaning&lt;/li&gt;
  &lt;li&gt;Derrick: a “min-max” per day approach&lt;/li&gt;
  &lt;li&gt;Kendall: a weighted average rather than “dumb” interpolation&lt;/li&gt;
  &lt;li&gt;Dean: working on code that would take clean data from one of the rest and do further analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the most part, as much as we could test it, all of our approaches were showing the same top sites
in a ranking by traffic.  Derrick had a challenge because it was harder to remove the anomalies with
his method and harder still to limit the impact (those anomalies cost me a 4 hour block and I switched
to interpolating the gap; it cost him a day).  Kendall got her more complicated method mostly working
but before we could really contrast and compare we had to move on.&lt;/p&gt;

&lt;p&gt;Since I was ready with clean data a bit earlier, I’d moved from working with the single per-week dumps
from the MTA and using the yearly dumps from NY.  I grepped (yes… literally using grep) the months
of March, April and May from each of 2015, 2016 and 2017, processed these and one-by-one fed the
data to Dean.  Dean and I then struggled with it.  We fought through issues of Pandas so we could 
do analysis and start making charts.  I found almost no variation in the ranking of the first ten
sites when looking at all three months vs. one week in the 2017 data.&lt;/p&gt;

&lt;p&gt;I made some charts and spent a lot of time beating my head against the arcane details of matplotlib.
I pushed some charts to our group repository and Kendall provided me some feedback to change a few
things to align more with the morning’s lecture on Visualization.&lt;/p&gt;

&lt;p&gt;Eventually Dean drafted a Presentation and we sort of discussed who’d say what.  We will be wrapping
up charts, etc. and will present.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Benson is wrapping up.</summary></entry><entry><title type="html">Bootcamp - First Week Wrapup</title><link href="/metis/2018/01/19/Bootcamp-First-Week-Wrapup/" rel="alternate" type="text/html" title="Bootcamp - First Week Wrapup" /><published>2018-01-19T00:00:00+00:00</published><updated>2018-01-19T00:00:00+00:00</updated><id>/metis/2018/01/19/Bootcamp-First-Week-Wrapup</id><content type="html" xml:base="/metis/2018/01/19/Bootcamp-First-Week-Wrapup/">&lt;p&gt;We have concluded the First Week!  Yay!&lt;/p&gt;

&lt;p&gt;The pace has been brutal.  Many of us are extremely sleep deprived.&lt;/p&gt;

&lt;p&gt;I was up very late last night forcing myself to incorporate the geo-data.
I then fought through library installation (and a bit of library debugging) to be able to follow some
examples I’ve seen on the web.  I got a basic heat map going.  It’s almost meaningless for our purposes.
But it’s good for street cred - it impressed some of my peers.  Several were only barely behind me.  Shortly
after I got there this morning Tiffany asked me about her troubles installing basemap.  I “slacked” her the
command lines I used to install that and half-a-dozen other modules.&lt;/p&gt;

&lt;p&gt;Oh yeah… on that… they weren’t kidding we DO use Slack a ton.  But we use it for &lt;strong&gt;work&lt;/strong&gt; even more
than for the lessons or other messaging.  It doesn’t replace email.  It’s just a &lt;strong&gt;very&lt;/strong&gt; convenient
way to do a quick-and-dirty cut-and-paste of a URL, a code snippet, a file, etc.  For a lot of things, there’s
no point to say “look at this”.  Why waste the other person’s time making them get up and walk around the
table.  They’re busy; you’re busy.  You use half-your brain to cut-and-paste to answer a peer’s question
while they’re wrestling with their problem and you’re struggling with yours.&lt;/p&gt;

&lt;p&gt;At 5pm, we segued into a Social for a couple of hours.  Light snacks, booze and some goofy games.
This stuff is important too.  Many of us are competitive.  Many of us are presumptuous, even precocious.
We’re often criticizing each other’s work and even arguing over methodology.  It is very helpful to
simply chill out and even be silly.&lt;/p&gt;

&lt;p&gt;The first Week concluding aligns more or less with the first Project concluding.  I had to chuckle when
I came across some of the curriculum/guideline that explained when the first project would truly be over.
It’s over when you blog about it.  Chuckle… OK.  It would be silly to claim the fact I had that project
set up on my blog within hours of the beginning of the project means I’ve already met that requirement.
But I’ve &lt;strong&gt;been&lt;/strong&gt; blogging about the project.&lt;/p&gt;

&lt;p&gt;However, we had our lesson on blogging today.&lt;/p&gt;

&lt;p&gt;And I’m horrified.&lt;/p&gt;

&lt;p&gt;Let me explain… if I thought earlier that having gotten the blog out of the way ahead of time would mean
I would have more “free time”, this was slightly incorrect.  Why?  Well… because there wasn’t that much
time in the day where I could have just ignored the lecture.  But that’s not what terrifies me.  No…
what scares me is reflecting that the contrast between the other students getting a short lecture on
blogs despite time required to get a good one going is the same sort of thing I’m going to face shortly
on other aspect such as Stats.  Coupled with the rapid pace on other topics, it’s clear I may have been
right to believe things would get tough after the first couple of weeks.&lt;/p&gt;

&lt;p&gt;Oh well… time to gird my loins.&lt;/p&gt;

&lt;p&gt;And doggone it if I didn’t actually catch a blog bug today anyway.  During the lecture on Visualization, the
declaration of what or Presentation should be made me think.  We are expected to create a slide presentation.
Well… I thought this is exactly what would look good on the blog.  Yeah, sure it would be sufficient
all on its own.  But it would be sweet if I could add that functionality…&lt;/p&gt;

&lt;p&gt;Yup.  So sure enough, I’ll be working on my blog anyway!  Just like everyone else.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">We have concluded the First Week! Yay!</summary></entry></feed>