<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-05-08T16:21:36+00:00</updated><id>/</id><title type="html">Joseph Hamilton - Solution Oriented Data Scientist</title><subtitle>A website by and for Joseph Hamilton. A place to explore projects and pursuits, to chase curiosities and catch dreams.
</subtitle><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><entry><title type="html">Status Update</title><link href="/metis/2018/03/25/Status-Update/" rel="alternate" type="text/html" title="Status Update" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/metis/2018/03/25/Status-Update</id><content type="html" xml:base="/metis/2018/03/25/Status-Update/">&lt;p&gt;Whew!  The first month post-bootcamp was busy!&lt;/p&gt;

&lt;p&gt;Woah.  I got away from blogging once things got really busy towards the end of bootcamp.&lt;/p&gt;

&lt;p&gt;I have some unfinished draft posts and I may go in and backcreate some posts I’ve been meaning
to write.  So at some point in the future, it may not seem so.  But this has been a six week hiatus.&lt;/p&gt;

&lt;p&gt;Because I may go into more depth with some of those posts on what happened in those last couple of weeks
of the bootcamp, I’m only going to summarize that here.  In essence, things got busy.  The last project
was very challenging in several ways.  But, for the most part, I actually adhered to the idea of segmenting
things per the guidance of the instructors… moreso than any of the other projects.  For some queer reason,
however, I had the idea it’d be &lt;em&gt;easier&lt;/em&gt; once I halted work on the project and focused solely on the
presentation itself.  I needed a bit of free time because I had to start shifting my focus to some
post-bootcamp stuff.  But this wasn’t the case at all.  In any case, we made it.&lt;/p&gt;

&lt;p&gt;All of us had the same need to recover from the bootcamp… while aggressively working our contacts from
the Career Day, etc.  But I had one additional area to pursue - lining up more training.  And this proved
to be nerve-wracking.&lt;/p&gt;

&lt;p&gt;What’s up here?&lt;/p&gt;

&lt;p&gt;The nature of the Reduction-in-Force that affected me makes me eligible for some specific governmental
benefits.  There are actually a couple levels here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.illinoisworknet.com/WIOA&quot;&gt;WIOA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.doleta.gov/tradeact/&quot;&gt;TAA&lt;/a&gt;/&lt;a href=&quot;https://www.doleta.gov/tradeact/2014_amend_att1.cfm&quot;&gt;TRA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A hasty, rough summary of these and how they apply to many like me would be to say that the former
is to help people in industries which have been designated in decline.  But the latter is for those
whose jobs had been moved to another country.&lt;/p&gt;

&lt;p&gt;Pursuing the first is easy because pretty much all you need to do is document your previous employer.
It’s pretty clear whether that is or is not covered.  But how to you prove your job went overseas?
The trick here is… &lt;em&gt;you&lt;/em&gt; don’t… your &lt;em&gt;former company&lt;/em&gt; has to.  This is just one part of the
bureaucratic difficulties I’ve been enduring.  I was very certain my job was shipped out.  Why?
Because I attempted to chase it.  I had already assisted with some of the training of the group in
Mexico that was taking over the work.  It took a bit of legwork but I found the hiring manager who
was going to oversee the work as they group that team.  They responded promptly when I sent my resume.
It was a brief conversation:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You’re perfect for this role.  You’ve been doing what we’ll be doing.
But the job is in Mexico City.  No telecommuting.  No relocation support.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And… though I didn’t ask, I bet salary would be adjust to Mexico City norms.  So… yeah.  I knew
definitively where my job went.  But… it took between three to four months for the paperwork to progress
from my former company to the appropriate governmental agencies to me.  This was a problem because with
TAA/TRA there are some specific deadlines/timeframes involved.  Specifically for my situation, I was
supposed to be actively in training before six months after I first filed for unemployment.&lt;/p&gt;

&lt;p&gt;I was able to foresee this bit of nonsense because of the people at the placement agency.  Indeed,
I never would have learned about any of these benefits were it not for these people.  But they helped me
to see the timeframes I should expect.  This is why I jumped into the bootcamp when I did.&lt;/p&gt;

&lt;p&gt;Nonetheless, I found myself with the challenge of determining &lt;strong&gt;what&lt;/strong&gt; training to pursue.  There is a bit
of an oddity in that you have to have been accepted and enrolled with the &lt;em&gt;complete&lt;/em&gt; training plan documented
before the government will approve and issue the grant to pay for it.  This puts a bit of a burden on the
training agency.  Some know how to roll with this.  Some do not.  I’d already been informed the most likely
options for Data Science or Big Data were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Illinois Institute of Technology&lt;/li&gt;
  &lt;li&gt;Depaul University&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I had to agree.  But, as I compiled supporting documentation for the government, I also researched bona
fide Masters of Science in Data Science programs.  Most of these are well outside the approved price range.
However, I found one that seemed &lt;strong&gt;GREAT&lt;/strong&gt; - University of Illinois.  Their program is entirely online via
Coursera.  Sadly, though, they proved to be far too slow.  I could not get their support to work fast enough
to meet governmental deadlines.&lt;/p&gt;

&lt;p&gt;Depaul has great programs.  But… the problem is that they’ve packaged up individual programs.  It didn’t
seem likely that I could do several of them.  And… sadly… though they do have some experience with these 
governmental programs, they also couldn’t work fast enough.&lt;/p&gt;

&lt;p&gt;IIT?  As everyone had told me, the woman overseeing the professional stuff knows &lt;em&gt;very well&lt;/em&gt; how to work
with this stuff.  The trouble I had was that their coursework seemed too low level.  This was balanced
by how flexible they are.  I am now essentially pursuing a custom plan.  For example, the first course
I’ll take there is on Recommender Systems.  Sure, that was covered in the bootcamp.  But this will be a
chance to go into greater depth and get some practice.&lt;/p&gt;

&lt;p&gt;I am now, once again, a college student.  I year from now, I’ll have a professional certificate from IIT.
And who knows… though I’ve not yet heard back from Illinois, they may accept me too.  I’d have to string
them along, likely starting later.  In any case, it’s clear enough to me that I’ll be doing lots of training
continually in this field, even if I land a job in the near term.&lt;/p&gt;

&lt;p&gt;Now that this stuff is for the most part resolved, I can shift my focus back to other things… like this blog.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Blog" /><summary type="html">Whew! The first month post-bootcamp was busy!</summary></entry><entry><title type="html">Dask and Spark</title><link href="/project/kojak/2018/03/25/Dask-on-AWS/" rel="alternate" type="text/html" title="Dask and Spark" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/project/kojak/2018/03/25/Dask-on-AWS</id><content type="html" xml:base="/project/kojak/2018/03/25/Dask-on-AWS/">&lt;p&gt;Spark and Dask are somewhat overlapping in what they do.  But they are also somewhat complementary.&lt;/p&gt;

&lt;p&gt;To be terribly honest, for a lot of production level things it seems the “Big” of Big Data really has to be
pretty signficant or upon analysis just building a large SQL database will serve just fine.&lt;/p&gt;

&lt;p&gt;For my current project, there are things I want to take advantage of at both ends of a pipeline.  For the
initial feature engineering I may benefit from using Graphframes on Spark.  There may be algorithms
avaialble in Dask (eg. SpectralClustering) that aren’t yet available in Spark.&lt;/p&gt;

&lt;p&gt;Dask is relatively new.  And AWS keeps providing alternatives to doing things.  So this post may
get outdated realtively quickly.  But, at the moment, this is what an individual would experience:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dask is far easier to install than Spark on a single, local machine.&lt;/li&gt;
  &lt;li&gt;Spark is much more straightforward to get working in a cluster on AWS.&lt;/li&gt;
  &lt;li&gt;Dask is vastly easier to learn for anyone used to Python and Pandas.&lt;/li&gt;
  &lt;li&gt;Spark is great for anyone already adept at SQL.&lt;/li&gt;
  &lt;li&gt;Dask is likely more capable of providing someone &lt;em&gt;immediate&lt;/em&gt; relief if they’re running into
memory issues trying to pull all their data into a large Pandas dataframe.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, again, this is &lt;strong&gt;at the moment&lt;/strong&gt;.  If I had to roll my own Hadoop setup to lay Spark atop of, I
very likely would be singing a different tune.&lt;/p&gt;

&lt;p&gt;So what’s the issue with Dask?&lt;/p&gt;

&lt;p&gt;First, let’s try to separate out things that pertain to any effort of taking advantage of a cluster.
Not every problem can be parallelized.  And many things that can require a fair amount of care to
change the general approach.  You cannot always just take your algorithms and functions and throw
them to a parallel virtual machine, cluster or whatnot.  This becomes evident even on a single
machine.  There are several sklearn models that incorpriate an “njobs” parameter to manage
parallel processing across CPUs/cores.  For the most part with Dask, if you can use this on a
single machine, you can immediately use it on a cluster.  You may still end up with issues where
the data isn’t where you need it to be.&lt;/p&gt;

&lt;p&gt;No, that’s not really different.  You have to manage or at least be aware of that issue on Dask,
Spark or whatever.&lt;/p&gt;

&lt;p&gt;What’s different at the moment is that for a Dask cluster, you have to set up the cluster.  For
Spark with EMR this is qausi-magically managed for you.&lt;/p&gt;

&lt;p&gt;And the more important thing.  Dask is bleeding edge.  The various software solutions to manage
this cluster creation have come and gone rapidly in the relatively recent past.&lt;/p&gt;

&lt;p&gt;But I have a feeling that things have somewhat settled on the suggested solution.  So, I’ll describe
that here.  It may help to lay out one guiding principle to guide your thinking.  If you try to
look at this from the perspective of Dask or your python code using Dask as being “in control” and
setting up the cluster as needed, you need to flip your thinking.  Get comfortable with the idea
that you need a system to manage a cluster to stick Dask (along with anything else) into.&lt;/p&gt;

&lt;p&gt;So.. what is that system?&lt;/p&gt;

&lt;p&gt;At the moment it’s Kubernetes for the cluster and Helm for the “sticking Dask in “ function.&lt;/p&gt;

&lt;p&gt;This keeps things modular.  It really doesn’t matter whether your Kubernetes cluster is on Google,
Amazon, local network or whatever.  Once you have it, you let Helm do the driving.&lt;/p&gt;

&lt;p&gt;For AWS, this is how it breaks down:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark is on EMR.&lt;/li&gt;
  &lt;li&gt;If you need to boostrap stuff to build in required functionality…
    &lt;ul&gt;
      &lt;li&gt;You’ll be working with EMR’s way of managing this&lt;/li&gt;
      &lt;li&gt;The changes you’ll be making are on the AWS instances themselves.&lt;/li&gt;
      &lt;li&gt;You will be able to just SSH into the instances and make changes directly (eg. to test)&lt;/li&gt;
      &lt;li&gt;You may only need to change the master node for a variety of things.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dask will be on EC2&lt;/li&gt;
  &lt;li&gt;Should you need to control things to get desired functionality…
    &lt;ul&gt;
      &lt;li&gt;These changes will need to be prepared inside Docker containers&lt;/li&gt;
      &lt;li&gt;You shouldn’t need to make changes in the instances themselves&lt;/li&gt;
      &lt;li&gt;It is &lt;strong&gt;possible&lt;/strong&gt; to get into the individual Docker containers.  But it’s far less easy to do.&lt;/li&gt;
      &lt;li&gt;It’s much more likely you will need to change the Docker container all workers use.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Dask" /><category term="Spark" /><category term="Kubernetes" /><category term="AWS" /><category term="EMR" /><category term="EC2" /><summary type="html">Spark and Dask are somewhat overlapping in what they do. But they are also somewhat complementary.</summary></entry><entry><title type="html">Bootcamp - Blogging about Projects</title><link href="/metis/2018/03/25/Blogging-Projects/" rel="alternate" type="text/html" title="Bootcamp - Blogging about Projects" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/metis/2018/03/25/Blogging-Projects</id><content type="html" xml:base="/metis/2018/03/25/Blogging-Projects/">&lt;p&gt;It’s a challenge to keep up with the blogging!&lt;/p&gt;

&lt;p&gt;We had an interesting and rather lively feedback session Friday.  We’d been talking about
doing one for a bit.  So David finally hosted it.&lt;/p&gt;

&lt;p&gt;One of the more interesting things to come out of it was a consistent call that Dask
should be introduced into the curriculum immediately following Pandas.  This may have been
about the only thing where the cohort at least had no dissenting views.  Most of the feedback
was diverse in the sense that not everyone agreed.&lt;/p&gt;

&lt;p&gt;But Dask?  Not everyone had delved into it.  But many had dabbled a bit.  And Alex did give
a good Investigation presentation on the topic.  Many of us had run into issues throughout
the bootcamp which this could have addressed.  This main problem is what to do when your
local machine runs out of memory?  For most of this there have only been two choices:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Reboot after the crash and try again with a smaller data set.&lt;/li&gt;
  &lt;li&gt;Move it to the cloud somehow.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But moving to the cloud isn’t trivial.  Do you try to create a single EC2 instance with
a ton of RAM?  Or try out something on EMR with the work spread out?  Both of these weren’t
something we could have done early on in the bootcamp.  Sometimes you can restructure your
work so you process a large file one line at a time.  But that’s what Dask does for you!!&lt;/p&gt;

&lt;p&gt;I rolled some swap space.  I’ve now got lots more virtual memory.  So my machine doesn’t
lock up or crash any longer.  But things get slow once I’m chewing through swap.&lt;/p&gt;

&lt;p&gt;Dask, however, would have a been a natural and straightforward solution many of us could
have incorporated rather easily even back in Project One.  And Dask would have provided us
an immediate benefit on a local machine well before taking advantage of its full power on
a cluster.&lt;/p&gt;

&lt;p&gt;One of the more lively topics of discussion related to the Challenges.  I dutifully completed
all the required challenges.  I am actually looking forward to doing some of the optional ones
later… after the conclusion of the bootcamp.  But it seemed pretty clear nobody did the
optional ones.  Indeed, many didn’t actually complete the required ones.  They just turned
them incomplete.  Oh well…&lt;/p&gt;

&lt;p&gt;Another thing that became optional halfway through the bootcamp was blogging about the projects.
This one seems a bit odd.  I mean… that’s pretty much the entire point of the bootcamp - to
develop a portfolio of projects which you display via your blog and/or GitHub.  In this case,
the issue is timing.  You can go back and work on your blogs later.  This seems to be what
a lot of folk do.&lt;/p&gt;

&lt;p&gt;Well… I’ve started to blog to keep a few notes along the way for Project Five.  But I’d not
yet even really started the blogging for Projects Three and Four… tsk… tsk!  So in between
some work on Project Five this afternoon I stopped and fluffed out the structure for the remaining
projects.  Soon I’ll put up the presentations for Project Four.  There’s something I want to go
back and finish for the final presentation of Project Three.  So I may wait on that.  We’ll see…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Blog" /><category term="Dask" /><summary type="html">It’s a challenge to keep up with the blogging!</summary></entry><entry><title type="html">Trouble with Clusters</title><link href="/project/kojak/2018/03/22/Trouble-with-Clusters/" rel="alternate" type="text/html" title="Trouble with Clusters" /><published>2018-03-22T00:00:00+00:00</published><updated>2018-03-22T00:00:00+00:00</updated><id>/project/kojak/2018/03/22/Trouble-with-Clusters</id><content type="html" xml:base="/project/kojak/2018/03/22/Trouble-with-Clusters/">&lt;p&gt;I’m starting to take advantage of clusters on the Cloud for parallel processing power.&lt;/p&gt;

&lt;p&gt;The problem isn’t that there’s not a way to do this.  The problem is that there are
so many ways of doing this.  This is a very dynamic field.&lt;/p&gt;

&lt;p&gt;At a very high level, there’s Amazon and Google.  I’ve tabled Google for the moment.  But I’ll get
back over there eventually.&lt;/p&gt;

&lt;p&gt;But within Amazon, there are many options.&lt;/p&gt;

&lt;p&gt;For general Hadoop, there’s EMR.  But even there we have many options.  I explored the use of
MRjob for a previous project.  I never made it very far.  This was nice in the way it automatically
set up the cluster and permitted a very structured way to slip in Python code at the various stages
of the overall Map-Combine-Reduce flow.  I used it for some small experiments with TF-IDF.  But
I didn’t have to time then to iron out all the issues with setting up instances and containers
appropriately for needs for that project.&lt;/p&gt;

&lt;p&gt;But it seems now that Spark is the way to do Python with a Hadoop structure.  MRJob does actually now work
with Spark.  So I may return to explore that.&lt;/p&gt;

&lt;p&gt;Then we have Zeppelin notebooks which work rather nicely with Spark, either directly in Scala or
via pyspark.  So now we have several layers of how to do things in notebooks.  There’s Jupyter
Notebooks, Jupyter Notebooks in Jupyter Labs and Zeppelin notebooks.&lt;/p&gt;

&lt;p&gt;I tripped over myself yesterday firing up an EMR cluster to play with Zeppelin some more.  This
was a reminder than anything manual will involve problems sooner or later.  So, yesterday I backed
off and slammed in a local install of Spark with Zeppelin in a Docker container.  In all honesty,
I should probably be doing initial prototyping and experimentation locally anyway.  So it was
rather important to get it setup.  But… let’s now try to wrap things up so I can get an EMR
cluster going with Zeppelin/Spark support with as little manual support needed.&lt;/p&gt;

&lt;p&gt;So… my goals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Do everything related to setting up the cluster via the AWS CLI.&lt;/li&gt;
  &lt;li&gt;End up with the command to set up the tunnel and the URL for Zeppelin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the AWS CLI, let’s create a bucket and upload a bootstrap shell script.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws s3 mb s3://aws-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AWSID&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-boots&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;where {AWSID} is the AWS account ID.  S3 bucket names need to be unique.  This ought to be unique.&lt;/p&gt;

&lt;p&gt;Now, let’s create our bootstrap script.  I’ll call it &lt;code class=&quot;highlighter-rouge&quot;&gt;zep_boots.sh&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/anaconda.sh

bash ~/anaconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/anaconda

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\nexport PATH=$HOME/anaconda/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc

conda install &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; seaborn pandas requests

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Upload it to the bucket…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ aws s3 cp zep_boots.sh s3://aws-${AWSID}-boots/

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I had been cloning an existing cluster.  From the AWS website, I took advantage of the “AWS CLI Export”
function to get a CLI sequence to create the cluster.  I saved it to a file and then added to the end
of this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	--bootstrap-actions Path=s3://elasticmapreduce/bootstrap-actions/run-if,Args=[&quot;instance.isMaster=true&quot;,&quot;s3://aws-${AWSID}-boots/zep_boots.sh&quot;]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Because…  that’s what &lt;a href=&quot;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html&quot;&gt;AWS says it should be&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After &lt;strong&gt;many&lt;/strong&gt; attempts and troubleshooting, here’s what you really have to do:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	--bootstrap-actions Path=s3://aws-${AWSID}-boots/zep_boots.sh

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then rather than using the broken run-if script, just incorporate the logic into your code.  I found a good example
to copy on &lt;a href=&quot;https://forums.aws.amazon.com/thread.jspa?threadID=222418&quot;&gt;the forums&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a new version of &lt;code class=&quot;highlighter-rouge&quot;&gt;zep_boots.sh&lt;/code&gt; which includes setup for Graphframes.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#! /bin/bash&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Determine if we are running on the master node.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 0 - running on master&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 1 - running on a task or core node&lt;/span&gt;
check_if_master&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    python - &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;__SCRIPT__&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'
import sys
import json
 
instance_file = &quot;/mnt/var/lib/info/instance.json&quot;
is_master = False
try:
    with open(instance_file) as f:
        props = json.load(f)
 
    is_master = props.get('isMaster', False)
except IOError as ex:
    pass # file will not exist when testing on a non-emr machine
 
if is_master:
    sys.exit(0)
else:
    sys.exit(1)
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;__SCRIPT__
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

check_if_master &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;0

&lt;span class=&quot;c&quot;&gt;# Install Conda and desired modules&lt;/span&gt;

wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/anaconda.sh

bash ~/anaconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/anaconda

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\nexport PATH=$HOME/anaconda/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc

conda install &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; seaborn pandas requests

&lt;span class=&quot;c&quot;&gt;# Install Graphframes&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (gonna need sudo calls here...)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Note... this is sooo kludgey&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mkdir &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /var/lib/zeppelin/local-repo/2ANGGHHMQ
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /var/lib/zeppelin/local-repo/2ANGGHHMQ

&lt;span class=&quot;c&quot;&gt;# Need some dependencies&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar


&lt;span class=&quot;c&quot;&gt;# Now get graphframes&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar

&lt;span class=&quot;c&quot;&gt;# Finally, prepare for pyspark access of graphframes&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mkdir &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /usr/lib/zeppelin/interpreter/lib/python
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/lib/zeppelin/interpreter/lib/python
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;jar xf /var/lib/zeppelin/local-repo/2ANGGHHMQ/graphframes-0.5.0-spark2.1-s_2.11.jar graphframes



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This still requires going into the Interpreters in Zeppelin and changing &lt;code class=&quot;highlighter-rouge&quot;&gt;zeppelin.pyspark.python&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;/home/hadoop/anaconda/bin/python&lt;/code&gt;.  The goofy thing here is the bootstrap script is run &lt;strong&gt;before&lt;/strong&gt; the applications are loaded.  We could get away with creating directories and putting files in (as root… which is good because it prevents the “normal” things removing these files).  But the thing to do to switch to anaconda’s python would require changing a file (interpreter.json).  Hard to do if it’s not there.  And putting it in there may cause the application installation to fail.&lt;/p&gt;

&lt;p&gt;The way AWS EMR has set this up for Zeppelin seems to be… problematic and has caused a number of folk some grief when it comes to loading up additional modules and packages.&lt;/p&gt;

&lt;p&gt;But what about getting the URL for Zeppelin?  Didn’t I want to be able to ignore the AWS website entirely?&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bash create_cluster.sh
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;ClusterId&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;j-38FI39ISFYGTV&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws emr describe-cluster &lt;span class=&quot;nt&quot;&gt;--cluster-id&lt;/span&gt; j-38FI39ISFYGTV | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;State
&lt;span class=&quot;c&quot;&gt;# Wait a bit...  keep checking...&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws emr describe-cluster &lt;span class=&quot;nt&quot;&gt;--cluster-id&lt;/span&gt; j-38FI39ISFYGTV | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Dns
        &lt;span class=&quot;s2&quot;&gt;&quot;MasterPublicDnsName&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;ec2-18-218-45-180.us-east-2.compute.amazonaws.com&quot;&lt;/span&gt;,
&lt;span class=&quot;c&quot;&gt;# Now, to set up the tunnel...&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-ND&lt;/span&gt; 8157 hadoop@ec2-18-218-45-180.us-east-2.compute.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And browse to:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;http://ec2-18-218-45-180.us-east-2.compute.amazonaws.com:8890/&lt;/code&gt;&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Kubernetes" /><category term="AWS" /><category term="EMR" /><category term="Hadoop" /><summary type="html">I’m starting to take advantage of clusters on the Cloud for parallel processing power.</summary></entry><entry><title type="html">Bootcamp - Investigations Conclude</title><link href="/metis/2018/03/21/Investigations-Conclude/" rel="alternate" type="text/html" title="Bootcamp - Investigations Conclude" /><published>2018-03-21T00:00:00+00:00</published><updated>2018-03-21T00:00:00+00:00</updated><id>/metis/2018/03/21/Investigations-Conclude</id><content type="html" xml:base="/metis/2018/03/21/Investigations-Conclude/">&lt;p&gt;We’re all done with Investigations!&lt;/p&gt;

&lt;p&gt;Alas.  No more investigations.  No more competing fancies to vie for our time.&lt;/p&gt;

&lt;p&gt;True to form, the instructors held our feet to the fire and we did indeed wrap up all our Investigations
before Lunch on Monday.&lt;/p&gt;

&lt;p&gt;And, I was still the last one to present for whatever that’s worth.&lt;/p&gt;

&lt;p&gt;This ended up being like a project presentation day because so many were presenting back-to-back.
But it was a lot more relaxed and fun because we weren’t being evaluated.&lt;/p&gt;

&lt;p&gt;Tiffany had given one on GraphX Friday.  And Michael explored Time Series.  Monday Alex did Dask. 
Amy did Spacey.  Adam did… something
I cannot quite describe in one word.  It was a method of using a CNN to apply artistic styles
to images.  And I gave a &lt;a href=&quot;/slides/reinforcement/&quot; target=&quot;_blank&quot;&gt; presentation on reinforcement learning &lt;/a&gt;
 with a focus on bots playing games.&lt;/p&gt;

&lt;p&gt;Adam and I both had sort of transitioned an idea or hope for the Passion Project into an investigation
instead.  I started hearing an odd phrase recently… “Business Applicability”.  It has become expected
that we take ideas to the instructors (David and Alice) and have them shot down for one reason or another,
often due to things like too complex, too simple, inapplicable use of modeling assumptions, etc.  But lately
there seemed to be folk incorporating the Career Advisor (Ashley) into this discussion.  I guess the
discussions were someting like “yeah… that’s cool… but… what’s the &lt;strong&gt;business applicaton&lt;/strong&gt;”.  There
are often ways to answer that question.  Sometimes there is value in the method of approach overall even
if the end goal has less direct applicability.  But… things like weird picture generation or better AI bots
for games… well… better to have fun with it as an Investgiation.&lt;/p&gt;

&lt;p&gt;The really good thing about having everyone do investigations like this is that we all get more exposure
to what’s out there.  You can grab the work another student did whenever you need it.  This saves you a bit
of time you’d have spent wandering around the web exploring the topic.&lt;/p&gt;

&lt;p&gt;For example, I could see myself using any of these investigations during Project Five:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dask&lt;/li&gt;
  &lt;li&gt;GraphX&lt;/li&gt;
  &lt;li&gt;Spacey&lt;/li&gt;
  &lt;li&gt;Docker&lt;/li&gt;
  &lt;li&gt;Isolation Forest&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Reinforcement_Learning" /><summary type="html">We’re all done with Investigations!</summary></entry><entry><title type="html">Project Kojak Begins</title><link href="/project/kojak/2018/03/15/Project-Kojak-Begins/" rel="alternate" type="text/html" title="Project Kojak Begins" /><published>2018-03-15T00:00:00+00:00</published><updated>2018-03-15T00:00:00+00:00</updated><id>/project/kojak/2018/03/15/Project-Kojak-Begins</id><content type="html" xml:base="/project/kojak/2018/03/15/Project-Kojak-Begins/">&lt;p&gt;We’re finally here… we’ve crossed into Passion Project territory.&lt;/p&gt;

&lt;p&gt;And… in many ways, it’s just like the previous projects.  We’ve got till Friday to post our
two project ideas on projects slack channel.  We need to try to “fail fast” and switch to the
alternative idea if the first one proves not to be viable.&lt;/p&gt;

&lt;p&gt;But there are many rather significant differences this time.  First of all, there’s next to
no guidance or restrictions.  Unlike the previous projects, there’s no focus on Supervised vs.
Unsupervised.  There’s no requirement to use SQL or NoSQL.  No goal to incorporate visualizations.
Instead, it’s all of the above in whatever way we want.&lt;/p&gt;

&lt;p&gt;Except…&lt;/p&gt;

&lt;p&gt;The instructors are very actively involved in helping us choose projects.  This is in two primary
forms.  First, they would &lt;strong&gt;like&lt;/strong&gt; to create some sort of balance across the cohort.  For Project
Four, David made the comment that he’d very rarely, if ever, seen word clouds in project presentations.
However for some odd reason, a large percentage of us included word clouds.  They were artfully done.
But as a whole cohort it was essentially overdone.  They don’t want half of us chasing the same topic,
area or toolset for Project 5.  So, they listed about a dozen general areas and suggested to us
those which might appear more to employers.&lt;/p&gt;

&lt;p&gt;Ah yes… employers.  For Project 4 we had to hold a microphone while presenting.  It was weird.
This was to prepare us because Project 5 presentations will be livestreamed.  The primary audience
for Project 5 presentations will be employers and recruiters, both present and remote.&lt;/p&gt;

&lt;p&gt;So… if feels the instructors are even more rigorous in helping us avoid “bad” projects.  Our ideas
are getting shot down quickly.  If our ideas involve circular logic, weak assumptions, untenable
goals, etc., we’re redirected accordingly.&lt;/p&gt;

&lt;p&gt;Finally, it would &lt;strong&gt;seem&lt;/strong&gt; we have more time for this final project.  But that’s all somewhat of an
illusion.  This first week is devoted to helping us &lt;strong&gt;choose&lt;/strong&gt; a project (two projects, actually -
we need a viable alternative as well).  And the last week is earmarked for presentation refinement.
Oh yes.  There absolutely &lt;strong&gt;WILL NOT&lt;/strong&gt; be any presentations given on Career Day that were actually
tossed together or wrapped up within twelve hours of the presentation.  Oh no no no.  We’ll be
presenting internally a full week ahead of time and going through &lt;em&gt;several&lt;/em&gt; iterations for refinement.&lt;/p&gt;

&lt;p&gt;That puts much more pressure on us overall.&lt;/p&gt;

&lt;p&gt;I finally selected an alternative yesterday afternoon and posted my two ideas.  I’ve been moving large
amounts of data into an AWS S3 bucket for the purpose of EDA.  So, I’ve begun.  But my focus will be
split for a few days as I prepare the final Investigation.&lt;/p&gt;

&lt;p&gt;It seems I wasn’t the only one hoping to postpone delivery/presentation of the final Investigation.
I thought I was “safe” in this since I had the last slot on the official schedule.  But that means
little when about half the cohort was trying to push this off as well.  Alice was quite lenient in
granting these postponement requests.  But it now appears there was something missed in interpretation.
The students meant postponements on the order of a week (I’d’ve preferred a month).  She probably meant
something like a day tops.  So Alice laid down the law and declared &lt;strong&gt;ALL&lt;/strong&gt; Investigation presentations
must be completed by Monday.  This is creating a traffic jam and we’ll have something like a Project
Presentation Day because so many of us will be giving these Investigations back-to-back.&lt;/p&gt;

&lt;p&gt;Her motive is clear.  She wants us focused on Project Five.  And I see why.  I’m having lots of fun
with my Investigation.  It really is now a competing focus.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">We’re finally here… we’ve crossed into Passion Project territory.</summary></entry><entry><title type="html">HackerRank Targets: RegEx and NLP</title><link href="/hackerrank/2018/03/15/NLP-on-HackerRank/" rel="alternate" type="text/html" title="HackerRank Targets: RegEx and NLP" /><published>2018-03-15T00:00:00+00:00</published><updated>2018-03-15T00:00:00+00:00</updated><id>/hackerrank/2018/03/15/NLP-on-HackerRank</id><content type="html" xml:base="/hackerrank/2018/03/15/NLP-on-HackerRank/">&lt;p&gt;I’ve identified tracks in HackerRank to coincide with Project 4.&lt;/p&gt;

&lt;p&gt;Similar to Project Three, I thought it would be useful to progress through the related material
on HackerRank.  And as before, it was indeed uScores seful.  You don’t get the whizbang toolsets to
help you.  No SciKit Learn… No Gensim… No NLTK.  Nope.  It really does help to learn things
if you are forced to fall back and essentially code things from scratch.  And, you may learn
a lot more along the way.&lt;/p&gt;

&lt;p&gt;For example, as I coded TF-IDF from scratch, I read the Wikipedia page on TF-IDF.  Nothing
previous had underscored that there really isn’t just one TF-IDF algorithm.  There are many
variations.
Scores 
So, I chose the Regular Expression (Regex) track and the Natural Language Processing (NLP) section
withing the Machine Learning track.&lt;/p&gt;

&lt;p&gt;Alas… although it was and is helpful, it was also robust enough I couldn’t finish in the
time frame of Project 4.  I plowed all through the “learning” challenge sections for RegEx.  But
the last section is full of near-real-world applications.  I just couldn’t devote the time.
And the NLP section… sheesh.  That’s really going to take some time.&lt;/p&gt;

&lt;p&gt;I’m accumulating no small amount of curiosities and goals for things to pursue after the Metis
bootcamp.&lt;/p&gt;

&lt;p&gt;For Project 5, I’m starting to use Spark which depends on Scala.  So, in the same spirit, I
should jump on the Functional Programming track on Hacker Rank.  But… it’s going to have to wait.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Regex" /><category term="NLP" /><summary type="html">I’ve identified tracks in HackerRank to coincide with Project 4.</summary></entry><entry><title type="html">Project Fletcher Closing</title><link href="/project/fletcher/2018/03/09/Project-Fletcher-Closing/" rel="alternate" type="text/html" title="Project Fletcher Closing" /><published>2018-03-09T00:00:00+00:00</published><updated>2018-03-09T00:00:00+00:00</updated><id>/project/fletcher/2018/03/09/Project-Fletcher-Closing</id><content type="html" xml:base="/project/fletcher/2018/03/09/Project-Fletcher-Closing/">&lt;p&gt;Project Fletcher is coming to a close…&lt;/p&gt;

&lt;p&gt;I’m on the train commuting down to Project Presentation Day.&lt;/p&gt;

&lt;p&gt;And I have just been reminded of an significant risk I’ve been running - I’m incredibly dependent upon
access to the Internet to give my presentations.  It’s not just something like developing a presentation
in Google Slides or Slides.com and never downloading an offline copy.  My presentations are live.
They are run from a local web server (on the laptop).  Although I keep trying to ensure all javascript
libraries are served locally, I’m not catching everything.  In a more perfect world, I would/should
have a completely static copy in some format or another as a backup or failsafe.&lt;/p&gt;

&lt;p&gt;Things seem to work… more or less… eventually.  But it sometimes requires reloading, kicking it, waiting.
I CAN reload the presentation now on the train with no internet access.  Not quite what I’d want while
under a strict time constraint.  Next, I need to make a note to check into something else.  I’ve now got
half a dozen slides or more with iframes pulling other pages.  Somehow, for some reason, these seem to be
falling out of cache.  Sigh…&lt;/p&gt;

&lt;p&gt;The trouble with these projects is that you are SUPPOSED to stop your exploration, your analysis, etc.,
and then have time focused just on creation and refining the presentation.  But most of us keep at it.
Sometimes, you’re really close to finishing something and once you’ve got it then that opens up a floodgate
of further analysis or insight… sometimes invalidating or overtuning previous ideas or observations
you’d baked into the presentation.  It’s a very weird thing.&lt;/p&gt;

&lt;p&gt;Nonetheless, this is a one of the most key benefits of this bootcamp.  We’re learning time management
and prioritization skills relative to projects overall.  We’re being forced to turning things around in
an iterative fashion where we can present things at various stages along the way.  This is invaluable.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Fletcher is coming to a close…</summary></entry><entry><title type="html">Bootcamp - Live Programming</title><link href="/metis/2018/03/08/Bootcamp-Live-Programming/" rel="alternate" type="text/html" title="Bootcamp - Live Programming" /><published>2018-03-08T00:00:00+00:00</published><updated>2018-03-08T00:00:00+00:00</updated><id>/metis/2018/03/08/Bootcamp-Live-Programming</id><content type="html" xml:base="/metis/2018/03/08/Bootcamp-Live-Programming/">&lt;p&gt;Pair Programming has given way to Live Programming.&lt;/p&gt;

&lt;p&gt;Pair Programming is a key part of the daily routine in the Metis Data Science Bootcamp.  Almost every day begins with
approximately half-an-hour devoted to it.  About the only exception to this is any day devoted to Project
presentations.&lt;/p&gt;

&lt;p&gt;What we &lt;strong&gt;DO&lt;/strong&gt; for Pair Programming varies quite a bit.&lt;/p&gt;

&lt;p&gt;It &lt;strong&gt;FEELS&lt;/strong&gt; almost as if they set things up to catch you off guard.  We’ll go days with what are essentially
your standard programming tutorial challenges.  Little coding challenges you can find on any website you could
use to help you learn programming.  The thing here is that once you’ve seen a good number of these, they sort
of stick in your memory and you can just draw upon that to make things work.  And since you’re in a &lt;strong&gt;pair&lt;/strong&gt;,
there’s a reasonably good chance one of you can remember what to do.  Then all of a sudden, out of the blue,
we get a challenge to code one of the algorithms we use for regression or modeling entirely from scratch.&lt;/p&gt;

&lt;p&gt;Another common variant is challenges which are really just walkthroughs.  These are actually harder than they
seem because of the time constraint.  I quite often come nowhere near finishing those.&lt;/p&gt;

&lt;p&gt;Well, we’ve finally switched` to “Live Programming”.  Rarely Pair Programming has been Solo.  Sometimes they do
want to ensure you can do it on your own.  Well, Live Programming is entirely Solo… sort of.&lt;/p&gt;

&lt;p&gt;What happens is someone is picked (supposedly) at random.  I say supposedly because it seemed very coincidental
I got selected immediately after asking whether the sampling was with replacement.  Kendall, who was selected
yesterday responded with a vehement NO.  This person who was selected must go up front and hook up their laptop
to the projector.  Between that and the white board, they respond to a challenge given by the instructors.
Everyone else must work on the same challenge… solo and in silence.  The person on the hot seat, can ask
question, of course.  But nobody else can and everyone else cannot comment on the work of the one in the
hot seat, neither to criticize, correct nor congratulate.&lt;/p&gt;

&lt;p&gt;I daresay, if the last two days are anything to go on, we do much better when we’re not the one up front.
My solution when Kendall was up front was good.  My solution when I went up front was OK.  But Nate had a
better solution.  He was somewhat shy about it but Alice pushed him to go up and show off his solution.&lt;/p&gt;

&lt;p&gt;We’re all used to going up and presenting after Pair Programming.  That’s usually how Pair Programming
sessions end.  Times up and anyone who has a working solution can go up and present.  Then anyone else
who had a solution significantly different can go up and also present.  So we’re used to presenting
&lt;strong&gt;AFTER&lt;/strong&gt; coding.  It’s presenting &lt;strong&gt;WHILE&lt;/strong&gt; coding that’s the new thing.&lt;/p&gt;

&lt;p&gt;This is all supposed to work to prepare us for all kinds of coding interview experiences.  We get feedback
from the instructors afterwards.  And sometimes, the instructors change the request or objective of the
challenge on the fly while you’re in progress.  They’re much more focused on your behavior than they
are the specific techniques.  For example, I got a little guidance on legible variable names (actually
sliding constants into variables named descriptively).  But I got called out specifically and majorly
for two behavioral issues:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I didn’t make use of the white board despite Alice suggesting I could.&lt;/li&gt;
  &lt;li&gt;I didn’t pseudo-code first.  A bit later on I started using comments in a pseudo-code like way.
But I should have started with pseudo-code first.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The feedback isn’t all negative.  Overall they were pleased with my performance.  I did talk a fair
amount during the process (though I got quiet when wrestling with something in my head - something
that should have been whiteboarded).  They liked that I tested my functions quickly and repeatedly
while developing them.&lt;/p&gt;

&lt;p&gt;Being on the hot seat adds a bit of pressure and discomfort for a lot of us.  That’s why, of course,
we’re getting the practice.  It clouds your mind a bit.  For example, yesterday Kendall chose an approach
based on strings.  I used math instead.  Many of these could be done a number of ways.  But Kendall has
a strong Math background.  So I was surprised she chose strings.  When you’re up front, you tend to get
tunnel vision.  That’s part of why we need the practice.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Pair Programming has given way to Live Programming.</summary></entry><entry><title type="html">Project Fletcher Began</title><link href="/project/fletcher/2018/03/01/Project-Fletcher-Began/" rel="alternate" type="text/html" title="Project Fletcher Began" /><published>2018-03-01T00:00:00+00:00</published><updated>2018-03-01T00:00:00+00:00</updated><id>/project/fletcher/2018/03/01/Project-Fletcher-Began</id><content type="html" xml:base="/project/fletcher/2018/03/01/Project-Fletcher-Began/">&lt;p&gt;Project Fletcher began a while ago…&lt;/p&gt;

&lt;p&gt;I guess you might say as far as blogging goes, the cat caught my tongue.&lt;/p&gt;

&lt;p&gt;I have not yet finished blogging about Project McNulty.&lt;/p&gt;

&lt;p&gt;Which is hilarious because the storytelling part about that project was that some
nefarious group imprisoned my blog.  Lo and behold if that didn’t happen in effect.&lt;/p&gt;

&lt;p&gt;I often see graduates from previous cohorts milling around the Metis campus.
It seems very worthwhile to see what they’re up to or to ask them about their
experiences.  A fair amount of the time, they’re working with Ashley.  Some times,
they’re just coming in on a regular, if infrequent, basis and they spend their
time working on some skill or another.  But one high runner for activities is
polishing up their past projects either in their blog or in their GitHub repositories.&lt;/p&gt;

&lt;p&gt;Heavens, if I’ve not created the same problem for myself.&lt;/p&gt;

&lt;p&gt;I was behind on the last project due to planned and unplanned absences.  The instructors
gave me a grace day and I presented a day after everyone else.  They were also understanding
in the sense that they didn’t expect or require me to submit or publish my material on the
same schedule as everyone else.  But I didn’t realize they weren’t going to wait for that
to grade my work.  I’m not surprised.  They make the rounds almost every day to check up
on our progress.  Between that and us pestering them, they have a good sense of what we’re
doing.  That and the presentation itself is a good metric.&lt;/p&gt;

&lt;p&gt;In any case, the trouble was that my lag on ending Project Three meant I was already bleeding
into the time where everyone else was working on Project Four.  And if that wasn’t enough
I have started my research and work leading to my next Investigation.&lt;/p&gt;

&lt;p&gt;My focus is wildly split now.  And Project Three is the only thing where there’s no real pressure.
The others have deadlines.  Nonetheless, there’s one thing I wish to do before I publish Project
Three… so it’s gonna linger for a bit…&lt;/p&gt;

&lt;p&gt;In any case, Project Four is NLP and Unsupervised Learning, both of which can be… umm… a
tad weird.  I chose two optional scopes for the project.  We each had a private meeting with
the instructors to go over these.  Yet again, I found myself with an incorrect assessment of
which would be harder.  I knew which would be more direct and in line with the lectures, etc.
But it’s not just that the other would be a bit harder… apparently it would have been much
more difficult in the time frame available.  In my scope review meeting with the instructors, I
asked about how we would measure success for this project.  The previous two projects had various
metrics.  The response was interesting…&lt;/p&gt;

&lt;p&gt;“When you have something that makes sense.”&lt;/p&gt;

&lt;p&gt;Well… a bit of experimentation with Topic Modeling, and I have a much deeper appreciation for
that statement.&lt;/p&gt;

&lt;p&gt;Another intriguing aspect of this project is that I may actually take advantage of the AWS servers.
I may not truly need such if I limit my scope.  But to scale up, it seems very likely I’ll actually
benefit.  I don’t need an EC2 instance for MongoDB.  I just dumped stuff to my local MongoDB server
I had running since Project Two.  But, I may need some extra horsepower.  We’ll have to see what I
can accomplish…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Fletcher began a while ago…</summary></entry></feed>