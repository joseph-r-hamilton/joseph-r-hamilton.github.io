<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-05-25T16:37:03+00:00</updated><id>/</id><title type="html">Joseph Hamilton - Solution Oriented Data Scientist</title><subtitle>A website by and for Joseph Hamilton. A place to explore projects and pursuits, to chase curiosities and catch dreams.
</subtitle><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><entry><title type="html">IIT Beginnings</title><link href="/iit/2018/05/25/IIT-Beginnings/" rel="alternate" type="text/html" title="IIT Beginnings" /><published>2018-05-25T00:00:00+00:00</published><updated>2018-05-25T00:00:00+00:00</updated><id>/iit/2018/05/25/IIT-Beginnings</id><content type="html" xml:base="/iit/2018/05/25/IIT-Beginnings/">&lt;p&gt;I am a student once again!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Joseph_Hamilton_Juji_Profile.png&quot; alt=&quot;Juji Profile &quot; /&gt;&lt;/p&gt;

&lt;p&gt;I met an IIT grad last night at a meetup.  I told her I was an IIT student.  She thought I meant an alum.&lt;/p&gt;

&lt;p&gt;Nope.  I’m a bona fide IIT &lt;em&gt;student&lt;/em&gt;.  My first course on Recommender Systems starts in early July.&lt;/p&gt;

&lt;p&gt;It is rather clear to me that I will be in mode of some rather serious self-training for quite a while.
Indeed, in the Data Science arena, constant learning is pretty much required.&lt;/p&gt;

&lt;p&gt;My TRA/TAA program is in an odd state of limbo.  Things are so backwards working with the government.
I was supposed to have multiple options for comparison.  But for each option I needed to have been
accepted, registered and have the complete curriculum mapped out such that the institution could send
the government a letter testifying to such.  Only IIT was nimble enough to make this work for the
time constraints I faced.&lt;/p&gt;

&lt;p&gt;Call out to &lt;a href=&quot;https://appliedtech.iit.edu/people/nuala-power&quot;&gt;Nuala Power&lt;/a&gt;.  This woman knows how
to make things happen quickly.&lt;/p&gt;

&lt;p&gt;Depaul was just too slow to get back to me.  Their program would have been better to help me towards
the Cloudera certs.  But ehh… I’ll do that on my own in time.&lt;/p&gt;

&lt;p&gt;And University of Illinois… chuckle… sheesh those wheels are turning slowly.  But they are turning.
They’ve sent me an email that they received my application… weeks after they received it.  I think
this means it bumped from one stage in the process to another.  If I do get accepted, I’ll have to
determine what to do at that point.  I’ll probably beg for a deferral.  I &lt;strong&gt;will not&lt;/strong&gt; be able to
do it while under the oversight of TRA.  The government is very clear on this point.  If they’re
involved and pay for something they approved, thou shalt not pay for anything else, anywhere else
on your own.&lt;/p&gt;

&lt;p&gt;But, the wrinkle is what happens when I get a job.  I’m getting mixed signals.  But it seems that
I &lt;em&gt;can&lt;/em&gt; drop everything at my whim.  Or I can complete any courses underway.  However, support for
completing the previously approved program may end.  Furthermore, because my first IIT course has
an odd schedule, we’re going to apply for another waiver and then apply for final approval.  This
is what I mean about things being in limbo.  I have not yet actually been approved.&lt;/p&gt;

&lt;p&gt;So… in theory… if I get a job in the very near future this IIT program may simply evaporate.&lt;/p&gt;

&lt;p&gt;Oh well.&lt;/p&gt;

&lt;p&gt;None of that matters so much to IIT.  They’re comfortable with the odd aspects of these governmental
things.  So I’m fully registered with my own new IIT email and such.&lt;/p&gt;

&lt;p&gt;The course instructor sent me a request to help him get to know me.  Silly me.  I thought this meant
send him an email.  No.  He provided a link to a Juji chatbot session.  This was quite a bizarre
experience.  I essentially had an interview with an AI.  At the end of it, &lt;a href=&quot;https://juji.ai/post-chat/5b080a1e-38a3-43c2-b196-8bcb2e5c3305/share&quot;&gt;Juji provided me an
assessment of my strengths&lt;/a&gt;. 
This would be cute on its own.  But it actually dovetails with some of the more rigorous assessments
I’ve done recently.  So much so that it was creepy if it pulled all that from a short interview.&lt;/p&gt;

&lt;p&gt;For example, here’s a snippet from a Birkman Assessment.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Your Usual Style is the proactive, positive, and adaptable behavior you have learned to
use to achieve successful outcomes. These strength behaviors help you work
productively and are often seen as your strengths by others. Strength behaviors are
comfortable and easy for you to use.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Your Usual Style is:&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Friendly, decisive and energetic&lt;/li&gt;
    &lt;li&gt;Socially sensitive&lt;/li&gt;
    &lt;li&gt;Fairly methodical in your approach&lt;/li&gt;
    &lt;li&gt;Fairly assertive without being domineering&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;In describing yourself to others indicate that:&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;You are responsive to a variety of demands&lt;/li&gt;
    &lt;li&gt;You encourage healthy competition&lt;/li&gt;
    &lt;li&gt;You pay attention to detail and follow through&lt;/li&gt;
    &lt;li&gt;You spend enough time on decisions that mistakes are not made due to hasty action&lt;/li&gt;
  &lt;/ul&gt;

&lt;/blockquote&gt;

&lt;p&gt;Hmm…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Training" /><summary type="html">I am a student once again!</summary></entry><entry><title type="html">DataCamp - a Beginning</title><link href="/datacamp/2018/05/11/DataCamp/" rel="alternate" type="text/html" title="DataCamp - a Beginning" /><published>2018-05-11T00:00:00+00:00</published><updated>2018-05-11T00:00:00+00:00</updated><id>/datacamp/2018/05/11/DataCamp</id><content type="html" xml:base="/datacamp/2018/05/11/DataCamp/">&lt;p&gt;I’ve started to pursue lessons via DataCamp.&lt;/p&gt;

&lt;p&gt;Training in all sorts of things related to Data Science and Data Engineering is going to keep me relatively
busy for quite a while.&lt;/p&gt;

&lt;p&gt;I was introduced to HackerRank by Metis.  Metis grads recommended it as a mechanism to prepare for the
Metis Data Science Bootcamp.  I can now echo that.  And Metis used it for some things.&lt;/p&gt;

&lt;p&gt;But now that I’m past the bootcamp, I’m exploring lots of mechanisms and options for self-training.
One thing that caught my eye was DataCamp.  It seems to be rather defined.  It’s not necesarily more
comprehensive than HackerRank.  But it seems more structured in a couple of ways.  First, now that I have
the experience of the bootcamp, I’m much better able to know what I want to pursue, learn or practice.  In
a sense, I know how to “read” things.  As such, I can see the structure of Datacamp’s material.  If I want
to practice algorithm X, I know exactly where to go for it.  HackerRank is more open ended where they give
problems and you can use anything to solve it.  Next, DataCamp has organized their coursework in career
tracks.&lt;/p&gt;

&lt;p&gt;Honestly, I’m not sure I’ll prioritize a single career track.  I plan on completing &lt;em&gt;everything&lt;/em&gt; on their site.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Python" /><category term="R" /><category term="SQL" /><summary type="html">I’ve started to pursue lessons via DataCamp.</summary></entry><entry><title type="html">Status Update</title><link href="/metis/2018/03/25/Status-Update/" rel="alternate" type="text/html" title="Status Update" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/metis/2018/03/25/Status-Update</id><content type="html" xml:base="/metis/2018/03/25/Status-Update/">&lt;p&gt;Whew!  The first month post-bootcamp was busy!&lt;/p&gt;

&lt;p&gt;Woah.  I got away from blogging once things got really busy towards the end of bootcamp.&lt;/p&gt;

&lt;p&gt;I have some unfinished draft posts and I may go in and backcreate some posts I’ve been meaning
to write.  So at some point in the future, it may not seem so.  But this has been a six week hiatus.&lt;/p&gt;

&lt;p&gt;Because I may go into more depth with some of those posts on what happened in those last couple of weeks
of the bootcamp, I’m only going to summarize that here.  In essence, things got busy.  The last project
was very challenging in several ways.  But, for the most part, I actually adhered to the idea of segmenting
things per the guidance of the instructors… moreso than any of the other projects.  For some queer reason,
however, I had the idea it’d be &lt;em&gt;easier&lt;/em&gt; once I halted work on the project and focused solely on the
presentation itself.  I needed a bit of free time because I had to start shifting my focus to some
post-bootcamp stuff.  But this wasn’t the case at all.  In any case, we made it.&lt;/p&gt;

&lt;p&gt;All of us had the same need to recover from the bootcamp… while aggressively working our contacts from
the Career Day, etc.  But I had one additional area to pursue - lining up more training.  And this proved
to be nerve-wracking.&lt;/p&gt;

&lt;p&gt;What’s up here?&lt;/p&gt;

&lt;p&gt;The nature of the Reduction-in-Force that affected me makes me eligible for some specific governmental
benefits.  There are actually a couple levels here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.illinoisworknet.com/WIOA&quot;&gt;WIOA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.doleta.gov/tradeact/&quot;&gt;TAA&lt;/a&gt;/&lt;a href=&quot;https://www.doleta.gov/tradeact/2014_amend_att1.cfm&quot;&gt;TRA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A hasty, rough summary of these and how they apply to many like me would be to say that the former
is to help people in industries which have been designated in decline.  But the latter is for those
whose jobs had been moved to another country.&lt;/p&gt;

&lt;p&gt;Pursuing the first is easy because pretty much all you need to do is document your previous employer.
It’s pretty clear whether that is or is not covered.  But how to you prove your job went overseas?
The trick here is… &lt;em&gt;you&lt;/em&gt; don’t… your &lt;em&gt;former company&lt;/em&gt; has to.  This is just one part of the
bureaucratic difficulties I’ve been enduring.  I was very certain my job was shipped out.  Why?
Because I attempted to chase it.  I had already assisted with some of the training of the group in
Mexico that was taking over the work.  It took a bit of legwork but I found the hiring manager who
was going to oversee the work as they group that team.  They responded promptly when I sent my resume.
It was a brief conversation:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You’re perfect for this role.  You’ve been doing what we’ll be doing.
But the job is in Mexico City.  No telecommuting.  No relocation support.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And… though I didn’t ask, I bet salary would be adjust to Mexico City norms.  So… yeah.  I knew
definitively where my job went.  But… it took between three to four months for the paperwork to progress
from my former company to the appropriate governmental agencies to me.  This was a problem because with
TAA/TRA there are some specific deadlines/timeframes involved.  Specifically for my situation, I was
supposed to be actively in training before six months after I first filed for unemployment.&lt;/p&gt;

&lt;p&gt;I was able to foresee this bit of nonsense because of the people at the placement agency.  Indeed,
I never would have learned about any of these benefits were it not for these people.  But they helped me
to see the timeframes I should expect.  This is why I jumped into the bootcamp when I did.&lt;/p&gt;

&lt;p&gt;Nonetheless, I found myself with the challenge of determining &lt;strong&gt;what&lt;/strong&gt; training to pursue.  There is a bit
of an oddity in that you have to have been accepted and enrolled with the &lt;em&gt;complete&lt;/em&gt; training plan documented
before the government will approve and issue the grant to pay for it.  This puts a bit of a burden on the
training agency.  Some know how to roll with this.  Some do not.  I’d already been informed the most likely
options for Data Science or Big Data were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Illinois Institute of Technology&lt;/li&gt;
  &lt;li&gt;Depaul University&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I had to agree.  But, as I compiled supporting documentation for the government, I also researched bona
fide Masters of Science in Data Science programs.  Most of these are well outside the approved price range.
However, I found one that seemed &lt;strong&gt;GREAT&lt;/strong&gt; - University of Illinois.  Their program is entirely online via
Coursera.  Sadly, though, they proved to be far too slow.  I could not get their support to work fast enough
to meet governmental deadlines.&lt;/p&gt;

&lt;p&gt;Depaul has great programs.  But… the problem is that they’ve packaged up individual programs.  It didn’t
seem likely that I could do several of them.  And… sadly… though they do have some experience with these 
governmental programs, they also couldn’t work fast enough.&lt;/p&gt;

&lt;p&gt;IIT?  As everyone had told me, the woman overseeing the professional stuff knows &lt;em&gt;very well&lt;/em&gt; how to work
with this stuff.  The trouble I had was that their coursework seemed too low level.  This was balanced
by how flexible they are.  I am now essentially pursuing a custom plan.  For example, the first course
I’ll take there is on Recommender Systems.  Sure, that was covered in the bootcamp.  But this will be a
chance to go into greater depth and get some practice.&lt;/p&gt;

&lt;p&gt;I am now, once again, a college student.  I year from now, I’ll have a professional certificate from IIT.
And who knows… though I’ve not yet heard back from Illinois, they may accept me too.  I’d have to string
them along, likely starting later.  In any case, it’s clear enough to me that I’ll be doing lots of training
continually in this field, even if I land a job in the near term.&lt;/p&gt;

&lt;p&gt;Now that this stuff is for the most part resolved, I can shift my focus back to other things… like this blog.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Blog" /><summary type="html">Whew! The first month post-bootcamp was busy!</summary></entry><entry><title type="html">Dask and Spark</title><link href="/project/kojak/2018/03/25/Dask-on-AWS/" rel="alternate" type="text/html" title="Dask and Spark" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/project/kojak/2018/03/25/Dask-on-AWS</id><content type="html" xml:base="/project/kojak/2018/03/25/Dask-on-AWS/">&lt;p&gt;Spark and Dask are somewhat overlapping in what they do.  But they are also somewhat complementary.&lt;/p&gt;

&lt;p&gt;To be terribly honest, for a lot of production level things it seems the “Big” of Big Data really has to be
pretty signficant or upon analysis just building a large SQL database will serve just fine.&lt;/p&gt;

&lt;p&gt;For my current project, there are things I want to take advantage of at both ends of a pipeline.  For the
initial feature engineering I may benefit from using Graphframes on Spark.  There may be algorithms
avaialble in Dask (eg. SpectralClustering) that aren’t yet available in Spark.&lt;/p&gt;

&lt;p&gt;Dask is relatively new.  And AWS keeps providing alternatives to doing things.  So this post may
get outdated realtively quickly.  But, at the moment, this is what an individual would experience:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dask is far easier to install than Spark on a single, local machine.&lt;/li&gt;
  &lt;li&gt;Spark is much more straightforward to get working in a cluster on AWS.&lt;/li&gt;
  &lt;li&gt;Dask is vastly easier to learn for anyone used to Python and Pandas.&lt;/li&gt;
  &lt;li&gt;Spark is great for anyone already adept at SQL.&lt;/li&gt;
  &lt;li&gt;Dask is likely more capable of providing someone &lt;em&gt;immediate&lt;/em&gt; relief if they’re running into
memory issues trying to pull all their data into a large Pandas dataframe.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, again, this is &lt;strong&gt;at the moment&lt;/strong&gt;.  If I had to roll my own Hadoop setup to lay Spark atop of, I
very likely would be singing a different tune.&lt;/p&gt;

&lt;p&gt;So what’s the issue with Dask?&lt;/p&gt;

&lt;p&gt;First, let’s try to separate out things that pertain to any effort of taking advantage of a cluster.
Not every problem can be parallelized.  And many things that can require a fair amount of care to
change the general approach.  You cannot always just take your algorithms and functions and throw
them to a parallel virtual machine, cluster or whatnot.  This becomes evident even on a single
machine.  There are several sklearn models that incorpriate an “njobs” parameter to manage
parallel processing across CPUs/cores.  For the most part with Dask, if you can use this on a
single machine, you can immediately use it on a cluster.  You may still end up with issues where
the data isn’t where you need it to be.&lt;/p&gt;

&lt;p&gt;No, that’s not really different.  You have to manage or at least be aware of that issue on Dask,
Spark or whatever.&lt;/p&gt;

&lt;p&gt;What’s different at the moment is that for a Dask cluster, you have to set up the cluster.  For
Spark with EMR this is qausi-magically managed for you.&lt;/p&gt;

&lt;p&gt;And the more important thing.  Dask is bleeding edge.  The various software solutions to manage
this cluster creation have come and gone rapidly in the relatively recent past.&lt;/p&gt;

&lt;p&gt;But I have a feeling that things have somewhat settled on the suggested solution.  So, I’ll describe
that here.  It may help to lay out one guiding principle to guide your thinking.  If you try to
look at this from the perspective of Dask or your python code using Dask as being “in control” and
setting up the cluster as needed, you need to flip your thinking.  Get comfortable with the idea
that you need a system to manage a cluster to stick Dask (along with anything else) into.&lt;/p&gt;

&lt;p&gt;So.. what is that system?&lt;/p&gt;

&lt;p&gt;At the moment it’s Kubernetes for the cluster and Helm for the “sticking Dask in “ function.&lt;/p&gt;

&lt;p&gt;This keeps things modular.  It really doesn’t matter whether your Kubernetes cluster is on Google,
Amazon, local network or whatever.  Once you have it, you let Helm do the driving.&lt;/p&gt;

&lt;p&gt;For AWS, this is how it breaks down:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark is on EMR.&lt;/li&gt;
  &lt;li&gt;If you need to boostrap stuff to build in required functionality…
    &lt;ul&gt;
      &lt;li&gt;You’ll be working with EMR’s way of managing this&lt;/li&gt;
      &lt;li&gt;The changes you’ll be making are on the AWS instances themselves.&lt;/li&gt;
      &lt;li&gt;You will be able to just SSH into the instances and make changes directly (eg. to test)&lt;/li&gt;
      &lt;li&gt;You may only need to change the master node for a variety of things.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dask will be on EC2&lt;/li&gt;
  &lt;li&gt;Should you need to control things to get desired functionality…
    &lt;ul&gt;
      &lt;li&gt;These changes will need to be prepared inside Docker containers&lt;/li&gt;
      &lt;li&gt;You shouldn’t need to make changes in the instances themselves&lt;/li&gt;
      &lt;li&gt;It is &lt;strong&gt;possible&lt;/strong&gt; to get into the individual Docker containers.  But it’s far less easy to do.&lt;/li&gt;
      &lt;li&gt;It’s much more likely you will need to change the Docker container all workers use.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Dask" /><category term="Spark" /><category term="Kubernetes" /><category term="AWS" /><category term="EMR" /><category term="EC2" /><summary type="html">Spark and Dask are somewhat overlapping in what they do. But they are also somewhat complementary.</summary></entry><entry><title type="html">Bootcamp - Blogging about Projects</title><link href="/metis/2018/03/25/Blogging-Projects/" rel="alternate" type="text/html" title="Bootcamp - Blogging about Projects" /><published>2018-03-25T00:00:00+00:00</published><updated>2018-03-25T00:00:00+00:00</updated><id>/metis/2018/03/25/Blogging-Projects</id><content type="html" xml:base="/metis/2018/03/25/Blogging-Projects/">&lt;p&gt;It’s a challenge to keep up with the blogging!&lt;/p&gt;

&lt;p&gt;We had an interesting and rather lively feedback session Friday.  We’d been talking about
doing one for a bit.  So David finally hosted it.&lt;/p&gt;

&lt;p&gt;One of the more interesting things to come out of it was a consistent call that Dask
should be introduced into the curriculum immediately following Pandas.  This may have been
about the only thing where the cohort at least had no dissenting views.  Most of the feedback
was diverse in the sense that not everyone agreed.&lt;/p&gt;

&lt;p&gt;But Dask?  Not everyone had delved into it.  But many had dabbled a bit.  And Alex did give
a good Investigation presentation on the topic.  Many of us had run into issues throughout
the bootcamp which this could have addressed.  This main problem is what to do when your
local machine runs out of memory?  For most of this there have only been two choices:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Reboot after the crash and try again with a smaller data set.&lt;/li&gt;
  &lt;li&gt;Move it to the cloud somehow.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But moving to the cloud isn’t trivial.  Do you try to create a single EC2 instance with
a ton of RAM?  Or try out something on EMR with the work spread out?  Both of these weren’t
something we could have done early on in the bootcamp.  Sometimes you can restructure your
work so you process a large file one line at a time.  But that’s what Dask does for you!!&lt;/p&gt;

&lt;p&gt;I rolled some swap space.  I’ve now got lots more virtual memory.  So my machine doesn’t
lock up or crash any longer.  But things get slow once I’m chewing through swap.&lt;/p&gt;

&lt;p&gt;Dask, however, would have a been a natural and straightforward solution many of us could
have incorporated rather easily even back in Project One.  And Dask would have provided us
an immediate benefit on a local machine well before taking advantage of its full power on
a cluster.&lt;/p&gt;

&lt;p&gt;One of the more lively topics of discussion related to the Challenges.  I dutifully completed
all the required challenges.  I am actually looking forward to doing some of the optional ones
later… after the conclusion of the bootcamp.  But it seemed pretty clear nobody did the
optional ones.  Indeed, many didn’t actually complete the required ones.  They just turned
them incomplete.  Oh well…&lt;/p&gt;

&lt;p&gt;Another thing that became optional halfway through the bootcamp was blogging about the projects.
This one seems a bit odd.  I mean… that’s pretty much the entire point of the bootcamp - to
develop a portfolio of projects which you display via your blog and/or GitHub.  In this case,
the issue is timing.  You can go back and work on your blogs later.  This seems to be what
a lot of folk do.&lt;/p&gt;

&lt;p&gt;Well… I’ve started to blog to keep a few notes along the way for Project Five.  But I’d not
yet even really started the blogging for Projects Three and Four… tsk… tsk!  So in between
some work on Project Five this afternoon I stopped and fluffed out the structure for the remaining
projects.  Soon I’ll put up the presentations for Project Four.  There’s something I want to go
back and finish for the final presentation of Project Three.  So I may wait on that.  We’ll see…&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Blog" /><category term="Dask" /><summary type="html">It’s a challenge to keep up with the blogging!</summary></entry><entry><title type="html">Trouble with Clusters</title><link href="/project/kojak/2018/03/22/Trouble-with-Clusters/" rel="alternate" type="text/html" title="Trouble with Clusters" /><published>2018-03-22T00:00:00+00:00</published><updated>2018-03-22T00:00:00+00:00</updated><id>/project/kojak/2018/03/22/Trouble-with-Clusters</id><content type="html" xml:base="/project/kojak/2018/03/22/Trouble-with-Clusters/">&lt;p&gt;I’m starting to take advantage of clusters on the Cloud for parallel processing power.&lt;/p&gt;

&lt;p&gt;The problem isn’t that there’s not a way to do this.  The problem is that there are
so many ways of doing this.  This is a very dynamic field.&lt;/p&gt;

&lt;p&gt;At a very high level, there’s Amazon and Google.  I’ve tabled Google for the moment.  But I’ll get
back over there eventually.&lt;/p&gt;

&lt;p&gt;But within Amazon, there are many options.&lt;/p&gt;

&lt;p&gt;For general Hadoop, there’s EMR.  But even there we have many options.  I explored the use of
MRjob for a previous project.  I never made it very far.  This was nice in the way it automatically
set up the cluster and permitted a very structured way to slip in Python code at the various stages
of the overall Map-Combine-Reduce flow.  I used it for some small experiments with TF-IDF.  But
I didn’t have to time then to iron out all the issues with setting up instances and containers
appropriately for needs for that project.&lt;/p&gt;

&lt;p&gt;But it seems now that Spark is the way to do Python with a Hadoop structure.  MRJob does actually now work
with Spark.  So I may return to explore that.&lt;/p&gt;

&lt;p&gt;Then we have Zeppelin notebooks which work rather nicely with Spark, either directly in Scala or
via pyspark.  So now we have several layers of how to do things in notebooks.  There’s Jupyter
Notebooks, Jupyter Notebooks in Jupyter Labs and Zeppelin notebooks.&lt;/p&gt;

&lt;p&gt;I tripped over myself yesterday firing up an EMR cluster to play with Zeppelin some more.  This
was a reminder than anything manual will involve problems sooner or later.  So, yesterday I backed
off and slammed in a local install of Spark with Zeppelin in a Docker container.  In all honesty,
I should probably be doing initial prototyping and experimentation locally anyway.  So it was
rather important to get it setup.  But… let’s now try to wrap things up so I can get an EMR
cluster going with Zeppelin/Spark support with as little manual support needed.&lt;/p&gt;

&lt;p&gt;So… my goals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Do everything related to setting up the cluster via the AWS CLI.&lt;/li&gt;
  &lt;li&gt;End up with the command to set up the tunnel and the URL for Zeppelin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the AWS CLI, let’s create a bucket and upload a bootstrap shell script.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws s3 mb s3://aws-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AWSID&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-boots&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;where {AWSID} is the AWS account ID.  S3 bucket names need to be unique.  This ought to be unique.&lt;/p&gt;

&lt;p&gt;Now, let’s create our bootstrap script.  I’ll call it &lt;code class=&quot;highlighter-rouge&quot;&gt;zep_boots.sh&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/anaconda.sh

bash ~/anaconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/anaconda

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\nexport PATH=$HOME/anaconda/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc

conda install &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; seaborn pandas requests

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Upload it to the bucket…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ aws s3 cp zep_boots.sh s3://aws-${AWSID}-boots/

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I had been cloning an existing cluster.  From the AWS website, I took advantage of the “AWS CLI Export”
function to get a CLI sequence to create the cluster.  I saved it to a file and then added to the end
of this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	--bootstrap-actions Path=s3://elasticmapreduce/bootstrap-actions/run-if,Args=[&quot;instance.isMaster=true&quot;,&quot;s3://aws-${AWSID}-boots/zep_boots.sh&quot;]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Because…  that’s what &lt;a href=&quot;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html&quot;&gt;AWS says it should be&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After &lt;strong&gt;many&lt;/strong&gt; attempts and troubleshooting, here’s what you really have to do:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	--bootstrap-actions Path=s3://aws-${AWSID}-boots/zep_boots.sh

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then rather than using the broken run-if script, just incorporate the logic into your code.  I found a good example
to copy on &lt;a href=&quot;https://forums.aws.amazon.com/thread.jspa?threadID=222418&quot;&gt;the forums&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a new version of &lt;code class=&quot;highlighter-rouge&quot;&gt;zep_boots.sh&lt;/code&gt; which includes setup for Graphframes.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#! /bin/bash&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Determine if we are running on the master node.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 0 - running on master&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 1 - running on a task or core node&lt;/span&gt;
check_if_master&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    python - &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;__SCRIPT__&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'
import sys
import json
 
instance_file = &quot;/mnt/var/lib/info/instance.json&quot;
is_master = False
try:
    with open(instance_file) as f:
        props = json.load(f)
 
    is_master = props.get('isMaster', False)
except IOError as ex:
    pass # file will not exist when testing on a non-emr machine
 
if is_master:
    sys.exit(0)
else:
    sys.exit(1)
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;__SCRIPT__
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

check_if_master &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;0

&lt;span class=&quot;c&quot;&gt;# Install Conda and desired modules&lt;/span&gt;

wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; ~/anaconda.sh

bash ~/anaconda.sh &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/anaconda

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\nexport PATH=$HOME/anaconda/bin:$PATH'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bashrc

conda install &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; seaborn pandas requests

&lt;span class=&quot;c&quot;&gt;# Install Graphframes&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# (gonna need sudo calls here...)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Note... this is sooo kludgey&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mkdir &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /var/lib/zeppelin/local-repo/2ANGGHHMQ
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /var/lib/zeppelin/local-repo/2ANGGHHMQ

&lt;span class=&quot;c&quot;&gt;# Need some dependencies&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://central.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar


&lt;span class=&quot;c&quot;&gt;# Now get graphframes&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;wget http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar

&lt;span class=&quot;c&quot;&gt;# Finally, prepare for pyspark access of graphframes&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mkdir &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /usr/lib/zeppelin/interpreter/lib/python
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/lib/zeppelin/interpreter/lib/python
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;jar xf /var/lib/zeppelin/local-repo/2ANGGHHMQ/graphframes-0.5.0-spark2.1-s_2.11.jar graphframes



&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This still requires going into the Interpreters in Zeppelin and changing &lt;code class=&quot;highlighter-rouge&quot;&gt;zeppelin.pyspark.python&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;/home/hadoop/anaconda/bin/python&lt;/code&gt;.  The goofy thing here is the bootstrap script is run &lt;strong&gt;before&lt;/strong&gt; the applications are loaded.  We could get away with creating directories and putting files in (as root… which is good because it prevents the “normal” things removing these files).  But the thing to do to switch to anaconda’s python would require changing a file (interpreter.json).  Hard to do if it’s not there.  And putting it in there may cause the application installation to fail.&lt;/p&gt;

&lt;p&gt;The way AWS EMR has set this up for Zeppelin seems to be… problematic and has caused a number of folk some grief when it comes to loading up additional modules and packages.&lt;/p&gt;

&lt;p&gt;But what about getting the URL for Zeppelin?  Didn’t I want to be able to ignore the AWS website entirely?&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bash create_cluster.sh
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;ClusterId&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;j-38FI39ISFYGTV&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws emr describe-cluster &lt;span class=&quot;nt&quot;&gt;--cluster-id&lt;/span&gt; j-38FI39ISFYGTV | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;State
&lt;span class=&quot;c&quot;&gt;# Wait a bit...  keep checking...&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;aws emr describe-cluster &lt;span class=&quot;nt&quot;&gt;--cluster-id&lt;/span&gt; j-38FI39ISFYGTV | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Dns
        &lt;span class=&quot;s2&quot;&gt;&quot;MasterPublicDnsName&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;ec2-18-218-45-180.us-east-2.compute.amazonaws.com&quot;&lt;/span&gt;,
&lt;span class=&quot;c&quot;&gt;# Now, to set up the tunnel...&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-ND&lt;/span&gt; 8157 hadoop@ec2-18-218-45-180.us-east-2.compute.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And browse to:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;http://ec2-18-218-45-180.us-east-2.compute.amazonaws.com:8890/&lt;/code&gt;&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Kubernetes" /><category term="AWS" /><category term="EMR" /><category term="Hadoop" /><summary type="html">I’m starting to take advantage of clusters on the Cloud for parallel processing power.</summary></entry><entry><title type="html">Bootcamp - Investigations Conclude</title><link href="/metis/2018/03/21/Investigations-Conclude/" rel="alternate" type="text/html" title="Bootcamp - Investigations Conclude" /><published>2018-03-21T00:00:00+00:00</published><updated>2018-03-21T00:00:00+00:00</updated><id>/metis/2018/03/21/Investigations-Conclude</id><content type="html" xml:base="/metis/2018/03/21/Investigations-Conclude/">&lt;p&gt;We’re all done with Investigations!&lt;/p&gt;

&lt;p&gt;Alas.  No more investigations.  No more competing fancies to vie for our time.&lt;/p&gt;

&lt;p&gt;True to form, the instructors held our feet to the fire and we did indeed wrap up all our Investigations
before Lunch on Monday.&lt;/p&gt;

&lt;p&gt;And, I was still the last one to present for whatever that’s worth.&lt;/p&gt;

&lt;p&gt;This ended up being like a project presentation day because so many were presenting back-to-back.
But it was a lot more relaxed and fun because we weren’t being evaluated.&lt;/p&gt;

&lt;p&gt;Tiffany had given one on GraphX Friday.  And Michael explored Time Series.  Monday Alex did Dask. 
Amy did Spacey.  Adam did… something
I cannot quite describe in one word.  It was a method of using a CNN to apply artistic styles
to images.  And I gave a &lt;a href=&quot;/slides/reinforcement/&quot; target=&quot;_blank&quot;&gt; presentation on reinforcement learning &lt;/a&gt;
 with a focus on bots playing games.&lt;/p&gt;

&lt;p&gt;Adam and I both had sort of transitioned an idea or hope for the Passion Project into an investigation
instead.  I started hearing an odd phrase recently… “Business Applicability”.  It has become expected
that we take ideas to the instructors (David and Alice) and have them shot down for one reason or another,
often due to things like too complex, too simple, inapplicable use of modeling assumptions, etc.  But lately
there seemed to be folk incorporating the Career Advisor (Ashley) into this discussion.  I guess the
discussions were someting like “yeah… that’s cool… but… what’s the &lt;strong&gt;business applicaton&lt;/strong&gt;”.  There
are often ways to answer that question.  Sometimes there is value in the method of approach overall even
if the end goal has less direct applicability.  But… things like weird picture generation or better AI bots
for games… well… better to have fun with it as an Investgiation.&lt;/p&gt;

&lt;p&gt;The really good thing about having everyone do investigations like this is that we all get more exposure
to what’s out there.  You can grab the work another student did whenever you need it.  This saves you a bit
of time you’d have spent wandering around the web exploring the topic.&lt;/p&gt;

&lt;p&gt;For example, I could see myself using any of these investigations during Project Five:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dask&lt;/li&gt;
  &lt;li&gt;GraphX&lt;/li&gt;
  &lt;li&gt;Spacey&lt;/li&gt;
  &lt;li&gt;Docker&lt;/li&gt;
  &lt;li&gt;Isolation Forest&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Reinforcement_Learning" /><summary type="html">We’re all done with Investigations!</summary></entry><entry><title type="html">Project Kojak Begins</title><link href="/project/kojak/2018/03/15/Project-Kojak-Begins/" rel="alternate" type="text/html" title="Project Kojak Begins" /><published>2018-03-15T00:00:00+00:00</published><updated>2018-03-15T00:00:00+00:00</updated><id>/project/kojak/2018/03/15/Project-Kojak-Begins</id><content type="html" xml:base="/project/kojak/2018/03/15/Project-Kojak-Begins/">&lt;p&gt;We’re finally here… we’ve crossed into Passion Project territory.&lt;/p&gt;

&lt;p&gt;And… in many ways, it’s just like the previous projects.  We’ve got till Friday to post our
two project ideas on projects slack channel.  We need to try to “fail fast” and switch to the
alternative idea if the first one proves not to be viable.&lt;/p&gt;

&lt;p&gt;But there are many rather significant differences this time.  First of all, there’s next to
no guidance or restrictions.  Unlike the previous projects, there’s no focus on Supervised vs.
Unsupervised.  There’s no requirement to use SQL or NoSQL.  No goal to incorporate visualizations.
Instead, it’s all of the above in whatever way we want.&lt;/p&gt;

&lt;p&gt;Except…&lt;/p&gt;

&lt;p&gt;The instructors are very actively involved in helping us choose projects.  This is in two primary
forms.  First, they would &lt;strong&gt;like&lt;/strong&gt; to create some sort of balance across the cohort.  For Project
Four, David made the comment that he’d very rarely, if ever, seen word clouds in project presentations.
However for some odd reason, a large percentage of us included word clouds.  They were artfully done.
But as a whole cohort it was essentially overdone.  They don’t want half of us chasing the same topic,
area or toolset for Project 5.  So, they listed about a dozen general areas and suggested to us
those which might appear more to employers.&lt;/p&gt;

&lt;p&gt;Ah yes… employers.  For Project 4 we had to hold a microphone while presenting.  It was weird.
This was to prepare us because Project 5 presentations will be livestreamed.  The primary audience
for Project 5 presentations will be employers and recruiters, both present and remote.&lt;/p&gt;

&lt;p&gt;So… if feels the instructors are even more rigorous in helping us avoid “bad” projects.  Our ideas
are getting shot down quickly.  If our ideas involve circular logic, weak assumptions, untenable
goals, etc., we’re redirected accordingly.&lt;/p&gt;

&lt;p&gt;Finally, it would &lt;strong&gt;seem&lt;/strong&gt; we have more time for this final project.  But that’s all somewhat of an
illusion.  This first week is devoted to helping us &lt;strong&gt;choose&lt;/strong&gt; a project (two projects, actually -
we need a viable alternative as well).  And the last week is earmarked for presentation refinement.
Oh yes.  There absolutely &lt;strong&gt;WILL NOT&lt;/strong&gt; be any presentations given on Career Day that were actually
tossed together or wrapped up within twelve hours of the presentation.  Oh no no no.  We’ll be
presenting internally a full week ahead of time and going through &lt;em&gt;several&lt;/em&gt; iterations for refinement.&lt;/p&gt;

&lt;p&gt;That puts much more pressure on us overall.&lt;/p&gt;

&lt;p&gt;I finally selected an alternative yesterday afternoon and posted my two ideas.  I’ve been moving large
amounts of data into an AWS S3 bucket for the purpose of EDA.  So, I’ve begun.  But my focus will be
split for a few days as I prepare the final Investigation.&lt;/p&gt;

&lt;p&gt;It seems I wasn’t the only one hoping to postpone delivery/presentation of the final Investigation.
I thought I was “safe” in this since I had the last slot on the official schedule.  But that means
little when about half the cohort was trying to push this off as well.  Alice was quite lenient in
granting these postponement requests.  But it now appears there was something missed in interpretation.
The students meant postponements on the order of a week (I’d’ve preferred a month).  She probably meant
something like a day tops.  So Alice laid down the law and declared &lt;strong&gt;ALL&lt;/strong&gt; Investigation presentations
must be completed by Monday.  This is creating a traffic jam and we’ll have something like a Project
Presentation Day because so many of us will be giving these Investigations back-to-back.&lt;/p&gt;

&lt;p&gt;Her motive is clear.  She wants us focused on Project Five.  And I see why.  I’m having lots of fun
with my Investigation.  It really is now a competing focus.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">We’re finally here… we’ve crossed into Passion Project territory.</summary></entry><entry><title type="html">HackerRank Targets: RegEx and NLP</title><link href="/hackerrank/2018/03/15/NLP-on-HackerRank/" rel="alternate" type="text/html" title="HackerRank Targets: RegEx and NLP" /><published>2018-03-15T00:00:00+00:00</published><updated>2018-03-15T00:00:00+00:00</updated><id>/hackerrank/2018/03/15/NLP-on-HackerRank</id><content type="html" xml:base="/hackerrank/2018/03/15/NLP-on-HackerRank/">&lt;p&gt;I’ve identified tracks in HackerRank to coincide with Project 4.&lt;/p&gt;

&lt;p&gt;Similar to Project Three, I thought it would be useful to progress through the related material
on HackerRank.  And as before, it was indeed uScores seful.  You don’t get the whizbang toolsets to
help you.  No SciKit Learn… No Gensim… No NLTK.  Nope.  It really does help to learn things
if you are forced to fall back and essentially code things from scratch.  And, you may learn
a lot more along the way.&lt;/p&gt;

&lt;p&gt;For example, as I coded TF-IDF from scratch, I read the Wikipedia page on TF-IDF.  Nothing
previous had underscored that there really isn’t just one TF-IDF algorithm.  There are many
variations.
Scores 
So, I chose the Regular Expression (Regex) track and the Natural Language Processing (NLP) section
withing the Machine Learning track.&lt;/p&gt;

&lt;p&gt;Alas… although it was and is helpful, it was also robust enough I couldn’t finish in the
time frame of Project 4.  I plowed all through the “learning” challenge sections for RegEx.  But
the last section is full of near-real-world applications.  I just couldn’t devote the time.
And the NLP section… sheesh.  That’s really going to take some time.&lt;/p&gt;

&lt;p&gt;I’m accumulating no small amount of curiosities and goals for things to pursue after the Metis
bootcamp.&lt;/p&gt;

&lt;p&gt;For Project 5, I’m starting to use Spark which depends on Scala.  So, in the same spirit, I
should jump on the Functional Programming track on Hacker Rank.  But… it’s going to have to wait.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><category term="Regex" /><category term="NLP" /><summary type="html">I’ve identified tracks in HackerRank to coincide with Project 4.</summary></entry><entry><title type="html">Project Fletcher Closing</title><link href="/project/fletcher/2018/03/09/Project-Fletcher-Closing/" rel="alternate" type="text/html" title="Project Fletcher Closing" /><published>2018-03-09T00:00:00+00:00</published><updated>2018-03-09T00:00:00+00:00</updated><id>/project/fletcher/2018/03/09/Project-Fletcher-Closing</id><content type="html" xml:base="/project/fletcher/2018/03/09/Project-Fletcher-Closing/">&lt;p&gt;Project Fletcher is coming to a close…&lt;/p&gt;

&lt;p&gt;I’m on the train commuting down to Project Presentation Day.&lt;/p&gt;

&lt;p&gt;And I have just been reminded of an significant risk I’ve been running - I’m incredibly dependent upon
access to the Internet to give my presentations.  It’s not just something like developing a presentation
in Google Slides or Slides.com and never downloading an offline copy.  My presentations are live.
They are run from a local web server (on the laptop).  Although I keep trying to ensure all javascript
libraries are served locally, I’m not catching everything.  In a more perfect world, I would/should
have a completely static copy in some format or another as a backup or failsafe.&lt;/p&gt;

&lt;p&gt;Things seem to work… more or less… eventually.  But it sometimes requires reloading, kicking it, waiting.
I CAN reload the presentation now on the train with no internet access.  Not quite what I’d want while
under a strict time constraint.  Next, I need to make a note to check into something else.  I’ve now got
half a dozen slides or more with iframes pulling other pages.  Somehow, for some reason, these seem to be
falling out of cache.  Sigh…&lt;/p&gt;

&lt;p&gt;The trouble with these projects is that you are SUPPOSED to stop your exploration, your analysis, etc.,
and then have time focused just on creation and refining the presentation.  But most of us keep at it.
Sometimes, you’re really close to finishing something and once you’ve got it then that opens up a floodgate
of further analysis or insight… sometimes invalidating or overtuning previous ideas or observations
you’d baked into the presentation.  It’s a very weird thing.&lt;/p&gt;

&lt;p&gt;Nonetheless, this is a one of the most key benefits of this bootcamp.  We’re learning time management
and prioritization skills relative to projects overall.  We’re being forced to turning things around in
an iterative fashion where we can present things at various stages along the way.  This is invaluable.&lt;/p&gt;</content><author><name>Joseph Hamilton</name><email>hamilton.joseph.r@gmail.com</email></author><summary type="html">Project Fletcher is coming to a close…</summary></entry></feed>